<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"junyiha.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="摘要 onnxruntime库相关学习笔记  onnxruntime库 是什么ONNX Runtime 是一个高性能推理引擎，专门用于执行基于 ONNX（Open Neural Network Exchange） 格式的机器学习模型。它由 微软（Microsoft） 开发，旨在提供跨平台、高效、灵活的推理支持，支持从云到边缘设备的多种部署场景。  核心特点 跨平台支持：  可在 Windows、L">
<meta property="og:type" content="article">
<meta property="og:title" content="onnxruntime">
<meta property="og:url" content="https://junyiha.github.io/2024/12/30/notebook/C++/C++_04_%E5%B8%B8%E7%94%A8%E5%BA%93/2024-12-30-onnxruntime/index.html">
<meta property="og:site_name" content="junyi&#39;s blog">
<meta property="og:description" content="摘要 onnxruntime库相关学习笔记  onnxruntime库 是什么ONNX Runtime 是一个高性能推理引擎，专门用于执行基于 ONNX（Open Neural Network Exchange） 格式的机器学习模型。它由 微软（Microsoft） 开发，旨在提供跨平台、高效、灵活的推理支持，支持从云到边缘设备的多种部署场景。  核心特点 跨平台支持：  可在 Windows、L">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-30T01:00:00.000Z">
<meta property="article:modified_time" content="2025-04-28T08:08:39.160Z">
<meta property="article:author" content="zhang junyi">
<meta property="article:tag" content="CppLibrary">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://junyiha.github.io/2024/12/30/notebook/C++/C++_04_%E5%B8%B8%E7%94%A8%E5%BA%93/2024-12-30-onnxruntime/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>onnxruntime | junyi's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">junyi's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">hahahahaha</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/30/notebook/C++/C++_04_%E5%B8%B8%E7%94%A8%E5%BA%93/2024-12-30-onnxruntime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          onnxruntime
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-30 09:00:00" itemprop="dateCreated datePublished" datetime="2024-12-30T09:00:00+08:00">2024-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cpp/" itemprop="url" rel="index"><span itemprop="name">Cpp</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>onnxruntime库相关学习笔记</li>
</ul>
<h2 id="onnxruntime库-是什么"><a href="#onnxruntime库-是什么" class="headerlink" title="onnxruntime库 是什么"></a>onnxruntime库 是什么</h2><p><strong><code>ONNX Runtime</code></strong> 是一个高性能推理引擎，专门用于执行基于 ONNX（Open Neural Network Exchange） 格式的机器学习模型。它由 <strong>微软（Microsoft）</strong> 开发，旨在提供跨平台、高效、灵活的推理支持，支持从云到边缘设备的多种部署场景。</p>
<hr>
<h3 id="核心特点"><a href="#核心特点" class="headerlink" title="核心特点"></a><strong>核心特点</strong></h3><ol>
<li><p><strong>跨平台支持</strong>：</p>
<ul>
<li>可在 Windows、Linux、MacOS 上运行，也支持移动设备（iOS 和 Android）和嵌入式系统。</li>
</ul>
</li>
<li><p><strong>高性能</strong>：</p>
<ul>
<li>提供高效的硬件加速，支持 CPU、GPU，以及特殊硬件（如 Intel OpenVINO、NVIDIA TensorRT）。</li>
</ul>
</li>
<li><p><strong>多语言支持</strong>：</p>
<ul>
<li>提供 Python、C++、C# 等语言绑定，方便开发者集成到不同语言的项目中。</li>
</ul>
</li>
<li><p><strong>灵活性</strong>：</p>
<ul>
<li>支持从多种框架（如 PyTorch、TensorFlow、Keras、MXNet）导出的模型。</li>
<li>提供优化工具（如 <code>onnxruntime-tools</code>），帮助提升模型推理性能。</li>
</ul>
</li>
<li><p><strong>云与边缘兼容</strong>：</p>
<ul>
<li>适用于云端服务和边缘设备（如 IoT 和移动端），满足多种场景的部署需求。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a><strong>主要功能</strong></h3><ol>
<li><p><strong>模型推理（Inference）</strong>：</p>
<ul>
<li>通过加载 ONNX 格式的模型，快速进行预测或分类。</li>
<li>示例（Python）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">session = ort.InferenceSession(<span class="string">&quot;model.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">input_data = &#123;<span class="string">&quot;input&quot;</span>: some_numpy_array&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理</span></span><br><span class="line">output = session.run(<span class="literal">None</span>, input_data)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>硬件加速支持</strong>：</p>
<ul>
<li>自动选择最优的硬件执行提供程序（Execution Provider），如：<ul>
<li>CPU（默认）</li>
<li>GPU（CUDA、DirectML）</li>
<li>专用加速（TensorRT、OpenVINO 等）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>动态与静态模型支持</strong>：</p>
<ul>
<li>兼容静态计算图和动态计算图模型。</li>
</ul>
</li>
<li><p><strong>优化和量化</strong>：</p>
<ul>
<li>提供模型优化工具（如模型裁剪、量化）以减少模型大小，提高推理速度。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h3><ol>
<li><strong>Web 服务</strong>：部署深度学习模型进行实时推理（如聊天机器人、图像分类服务）。</li>
<li><strong>移动应用</strong>：将经过优化的 ONNX 模型嵌入到移动端以进行离线预测。</li>
<li><strong>边缘设备</strong>：在嵌入式系统中运行 ONNX 模型（如工业物联网传感器）。</li>
<li><strong>高性能计算</strong>：利用 GPU 或其他加速器大规模推理。</li>
</ol>
<hr>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>使用 pip 安装 ONNX Runtime 的 Python 包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnxruntime</span><br></pre></td></tr></table></figure>

<p>如果需要 GPU 支持，可以安装 GPU 版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnxruntime-gpu</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="ONNX-的来源"><a href="#ONNX-的来源" class="headerlink" title="ONNX 的来源"></a><strong>ONNX 的来源</strong></h3><p>ONNX 是一种开放的深度学习模型格式，支持不同框架间的模型互操作。通过将模型转换为 ONNX 格式，ONNX Runtime 能够高效加载并执行这些模型。</p>
<hr>
<h3 id="ONNX-Runtime-的优势"><a href="#ONNX-Runtime-的优势" class="headerlink" title="ONNX Runtime 的优势"></a><strong>ONNX Runtime 的优势</strong></h3><ul>
<li><strong>简化开发和部署流程</strong>：无需为每种硬件配置单独编写代码。</li>
<li><strong>兼容多框架模型</strong>：同一推理引擎支持从多个框架导出的模型。</li>
<li><strong>高效运行</strong>：内置优化能力，可在多种硬件上实现卓越性能。</li>
</ul>
<hr>
<p>如果你有更具体的需求（如如何优化模型或特定硬件部署），可以告诉我，我会详细解答！</p>
<h2 id="onnxruntime-C-接口-详解"><a href="#onnxruntime-C-接口-详解" class="headerlink" title="onnxruntime C++接口 详解"></a>onnxruntime C++接口 详解</h2><p><code>ONNX Runtime</code> 提供了功能强大的 C++ 接口，用于高性能地加载和推理 ONNX 模型。以下是对其 C++ 接口的详细解析，包括核心概念、关键类、常用方法及完整示例。</p>
<hr>
<h3 id="核心概念与类"><a href="#核心概念与类" class="headerlink" title="核心概念与类"></a><strong>核心概念与类</strong></h3><p>ONNX Runtime 的 C++ API 基于以下核心组件：</p>
<ol>
<li><p>**<code>Ort::Env</code>**：</p>
<ul>
<li>表示 ONNX Runtime 的环境，所有会话共享一个环境。</li>
<li>主要负责初始化 ONNX Runtime，设置日志等。</li>
</ul>
</li>
<li><p>**<code>Ort::Session</code>**：</p>
<ul>
<li>表示一个模型会话，用于加载和执行 ONNX 模型。</li>
<li>会话加载模型后可以多次推理。</li>
</ul>
</li>
<li><p>**<code>Ort::MemoryInfo</code>**：</p>
<ul>
<li>描述用于输入&#x2F;输出张量的内存分配信息。</li>
</ul>
</li>
<li><p>**<code>Ort::Allocator</code>**：</p>
<ul>
<li>管理内存分配，用于推理中获取分配的结果张量。</li>
</ul>
</li>
<li><p>**<code>Ort::Value</code>**：</p>
<ul>
<li>表示 ONNX 模型的输入和输出张量。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键步骤与方法"><a href="#关键步骤与方法" class="headerlink" title="关键步骤与方法"></a><strong>关键步骤与方法</strong></h3><ol>
<li><p><strong>初始化环境</strong>：</p>
<ul>
<li>使用 <code>Ort::Env</code> 创建 ONNX Runtime 环境：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>加载模型</strong>：</p>
<ul>
<li>使用 <code>Ort::Session</code> 加载 <code>.onnx</code> 模型文件：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line"><span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>准备输入和输出</strong>：</p>
<ul>
<li>创建输入张量并指定维度和数据类型：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;; <span class="comment">// Example shape for an image</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>; <span class="comment">// Initialize with dummy data</span></span><br><span class="line">Ort::MemoryInfo memory_info = Ort::MemoryInfo::<span class="built_in">CreateCpu</span>(OrtArenaAllocator, OrtMemTypeDefault);</span><br><span class="line">Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">    memory_info, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>获取模型输入&#x2F;输出名称</strong>：</p>
<ul>
<li>用于绑定输入和输出：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> num_input_nodes = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>执行推理</strong>：</p>
<ul>
<li>调用 <code>Run</code> 方法执行推理：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">    Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,    <span class="comment">// 默认运行选项</span></span><br><span class="line">    &amp;input_name,                <span class="comment">// 输入名称</span></span><br><span class="line">    &amp;input_tensor,              <span class="comment">// 输入张量</span></span><br><span class="line">    <span class="number">1</span>,                          <span class="comment">// 输入数量</span></span><br><span class="line">    &amp;output_name,               <span class="comment">// 输出名称</span></span><br><span class="line">    <span class="number">1</span>                           <span class="comment">// 输出数量</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>处理输出</strong>：</p>
<ul>
<li>输出通常是一个张量，可以通过 <code>GetTensorMutableData</code> 获取数据指针：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<hr>
<h3 id="完整示例代码"><a href="#完整示例代码" class="headerlink" title="完整示例代码"></a><strong>完整示例代码</strong></h3><p>下面是一个完整的 C++ 程序，演示如何使用 ONNX Runtime 推理一个 ONNX 模型：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载 ONNX 模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型输入信息</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>; <span class="comment">// Dummy input</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入张量</span></span><br><span class="line">    Ort::MemoryInfo memory_info = Ort::MemoryInfo::<span class="built_in">CreateCpu</span>(OrtArenaAllocator, OrtMemTypeDefault);</span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        memory_info, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型输出信息</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="keyword">auto</span> output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">        Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,</span><br><span class="line">        &amp;input_name, &amp;input_tensor, <span class="number">1</span>,</span><br><span class="line">        &amp;output_name, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出张量数据</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出结果（示例）</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output value: &quot;</span> &lt;&lt; output_data[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优化与扩展"><a href="#优化与扩展" class="headerlink" title="优化与扩展"></a><strong>优化与扩展</strong></h3><ol>
<li><p><strong>多线程优化</strong>：</p>
<ul>
<li>使用 <code>session_options.SetIntraOpNumThreads()</code> 和 <code>SetInterOpNumThreads()</code> 配置多线程。</li>
</ul>
</li>
<li><p><strong>GPU 支持</strong>：</p>
<ul>
<li>安装 ONNX Runtime 的 GPU 版本，并通过 <code>session_options.AppendExecutionProvider_CUDA()</code> 启用 GPU。</li>
</ul>
</li>
<li><p><strong>动态输入支持</strong>：</p>
<ul>
<li>对于不确定输入形状的模型，可在运行时动态设置张量维度。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a><strong>常见问题</strong></h3><ol>
<li><p><strong>模型加载失败</strong>：</p>
<ul>
<li>确保模型是有效的 ONNX 格式。</li>
<li>检查模型是否包含动态维度（需要正确处理）。</li>
</ul>
</li>
<li><p><strong>输出数据为空</strong>：</p>
<ul>
<li>确认输入张量的数据和维度与模型的预期一致。</li>
</ul>
</li>
<li><p><strong>硬件加速问题</strong>：</p>
<ul>
<li>检查是否正确安装了支持 GPU 的 ONNX Runtime 版本。</li>
</ul>
</li>
</ol>
<hr>
<p>如果有具体需求或问题，比如如何处理动态输入、支持特定硬件等，可以进一步讨论！</p>
<h2 id="C-onnxruntime-Ort-Env-详解"><a href="#C-onnxruntime-Ort-Env-详解" class="headerlink" title="C++ onnxruntime Ort::Env 详解"></a>C++ onnxruntime Ort::Env 详解</h2><p><code>Ort::Env</code> 是 ONNX Runtime 的 C++ API 中的一个核心类，用于初始化 ONNX Runtime 环境并设置相关的全局配置。ONNX Runtime 是一个高性能的推理引擎，<code>Ort::Env</code> 是使用 C++ API 的入口点之一。</p>
<p>以下是关于 <code>Ort::Env</code> 的详细解析：</p>
<hr>
<h3 id="Ort-Env-的功能"><a href="#Ort-Env-的功能" class="headerlink" title="Ort::Env 的功能"></a><strong><code>Ort::Env</code> 的功能</strong></h3><ol>
<li><p><strong>初始化 ONNX Runtime</strong>：</p>
<ul>
<li><code>Ort::Env</code> 是一个 RAII（资源获取即初始化）类，在创建时初始化 ONNX Runtime，销毁时释放相关资源。</li>
<li>在一个应用中，只需创建一个 <code>Ort::Env</code> 实例即可。</li>
</ul>
</li>
<li><p><strong>配置日志和线程池</strong>：</p>
<ul>
<li>允许设置日志级别、日志输出路径等全局配置。</li>
<li>配置线程池的行为（如并行度）。</li>
</ul>
</li>
<li><p><strong>管理会话的上下文环境</strong>：</p>
<ul>
<li>所有的推理会话 (<code>Ort::Session</code>) 都依赖于 <code>Ort::Env</code> 的存在。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Ort-Env-的构造函数"><a href="#Ort-Env-的构造函数" class="headerlink" title="Ort::Env 的构造函数"></a><strong><code>Ort::Env</code> 的构造函数</strong></h3><p><code>Ort::Env</code> 提供多种构造方式，允许用户灵活设置日志和多线程相关的参数。</p>
<h4 id="1-简单构造"><a href="#1-简单构造" class="headerlink" title="1. 简单构造"></a>1. <strong>简单构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>参数说明</strong>：</p>
<ul>
<li><code>ORT_LOGGING_LEVEL_WARNING</code>：日志级别，ONNX Runtime 只记录警告及更高级别的日志。</li>
<li><code>&quot;ONNXRuntime&quot;</code>：日志的默认记录器名称，用于区分日志源。</li>
</ul>
</li>
<li><p><strong>日志级别选项</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ORT_LOGGING_LEVEL_VERBOSE   <span class="comment">// 记录所有日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_INFO      <span class="comment">// 记录信息级别及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_WARNING   <span class="comment">// 记录警告及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_ERROR     <span class="comment">// 记录错误及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_FATAL     <span class="comment">// 记录致命错误日志</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-高级构造"><a href="#2-高级构造" class="headerlink" title="2. 高级构造"></a>2. <strong>高级构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_VERBOSE, <span class="string">&quot;ONNXRuntime&quot;</span>, OrtThreadingOptions&#123;&#125;)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用 <code>OrtThreadingOptions</code> 配置线程池行为。</li>
<li><strong><code>OrtThreadingOptions</code> 示例</strong>：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OrtThreadingOptions* options = Ort::<span class="built_in">GetApi</span>().<span class="built_in">CreateThreadingOptions</span>();</span><br><span class="line">Ort::<span class="built_in">GetApi</span>().<span class="built_in">SetGlobalIntraOpThreadAffinity</span>(options, <span class="literal">true</span>);</span><br><span class="line">Ort::<span class="built_in">GetApi</span>().<span class="built_in">ReleaseThreadingOptions</span>(options);</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a><strong>常见用法</strong></h3><h4 id="初始化-ONNX-Runtime-环境"><a href="#初始化-ONNX-Runtime-环境" class="headerlink" title="初始化 ONNX Runtime 环境"></a><strong>初始化 ONNX Runtime 环境</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 其他操作，如创建会话等</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="结合推理会话"><a href="#结合推理会话" class="headerlink" title="结合推理会话"></a><strong>结合推理会话</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建推理会话</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>); <span class="comment">// 设置线程数</span></span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Ort-Env-的主要接口"><a href="#Ort-Env-的主要接口" class="headerlink" title="Ort::Env 的主要接口"></a><strong><code>Ort::Env</code> 的主要接口</strong></h3><h4 id="设置日志级别"><a href="#设置日志级别" class="headerlink" title="设置日志级别"></a><strong>设置日志级别</strong></h4><p>设置日志的输出级别。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.<span class="built_in">SetLogLevel</span>(ORT_LOGGING_LEVEL_INFO);</span><br></pre></td></tr></table></figure>

<h4 id="设置线程池选项"><a href="#设置线程池选项" class="headerlink" title="设置线程池选项"></a><strong>设置线程池选项</strong></h4><p>可以通过 <code>OrtThreadingOptions</code> 配置线程池行为，如线程数、线程亲和性等。</p>
<hr>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>单实例原则</strong>：</p>
<ul>
<li>一个应用中通常只需要一个 <code>Ort::Env</code> 实例。</li>
<li>多个实例可能导致资源冲突或性能问题。</li>
</ul>
</li>
<li><p><strong>线程安全性</strong>：</p>
<ul>
<li><code>Ort::Env</code> 本身是线程安全的，可以在多线程环境中安全使用。</li>
</ul>
</li>
<li><p><strong>销毁时机</strong>：</p>
<ul>
<li>当 <code>Ort::Env</code> 实例销毁时，ONNX Runtime 的资源会被自动释放。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="完整示例"><a href="#完整示例" class="headerlink" title="完整示例"></a><strong>完整示例</strong></h3><p>以下是一个完整的推理流程，展示了如何使用 <code>Ort::Env</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">2</span>);</span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型输入输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_input_nodes = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    <span class="type">size_t</span> num_output_nodes = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_input_nodes &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of outputs: &quot;</span> &lt;&lt; num_output_nodes &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>如果需要进一步了解某些特性（如日志配置、线程选项等）或其他问题，可以继续提问！</p>
<h2 id="Ort-SessionOptions-详解"><a href="#Ort-SessionOptions-详解" class="headerlink" title="Ort::SessionOptions 详解"></a>Ort::SessionOptions 详解</h2><p><code>Ort::SessionOptions</code> 是 ONNX Runtime C++ API 中的重要类，用于配置推理会话 (<code>Ort::Session</code>) 的运行行为。通过该类，可以设置线程数、优化级别、自定义执行提供程序等。</p>
<p>以下是对 <code>Ort::SessionOptions</code> 的详细解析：</p>
<hr>
<h3 id="Ort-SessionOptions-的功能"><a href="#Ort-SessionOptions-的功能" class="headerlink" title="Ort::SessionOptions 的功能"></a><strong><code>Ort::SessionOptions</code> 的功能</strong></h3><ol>
<li><p><strong>线程配置</strong>：</p>
<ul>
<li>控制并行线程数。</li>
<li>配置线程间的交互模式（单线程或多线程）。</li>
</ul>
</li>
<li><p><strong>图优化</strong>：</p>
<ul>
<li>优化计算图以提升推理性能。</li>
</ul>
</li>
<li><p><strong>执行提供程序</strong>：</p>
<ul>
<li>指定使用的计算设备（如 CPU、GPU）。</li>
</ul>
</li>
<li><p><strong>自定义选项</strong>：</p>
<ul>
<li>支持自定义分配器、上下文、会话行为等。</li>
</ul>
</li>
<li><p><strong>内存分配和 IO 绑定</strong>：</p>
<ul>
<li>允许用户自定义内存分配器和 IO 绑定。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="构造和基本用法"><a href="#构造和基本用法" class="headerlink" title="构造和基本用法"></a><strong>构造和基本用法</strong></h3><h4 id="默认构造函数"><a href="#默认构造函数" class="headerlink" title="默认构造函数"></a><strong>默认构造函数</strong></h4><p>创建一个默认的 <code>Ort::SessionOptions</code> 对象：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ort::SessionOptions session_options;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="主要成员函数"><a href="#主要成员函数" class="headerlink" title="主要成员函数"></a><strong>主要成员函数</strong></h3><h4 id="1-设置线程数"><a href="#1-设置线程数" class="headerlink" title="1. 设置线程数"></a><strong>1. 设置线程数</strong></h4><h5 id="SetIntraOpNumThreads"><a href="#SetIntraOpNumThreads" class="headerlink" title="SetIntraOpNumThreads"></a><strong><code>SetIntraOpNumThreads</code></strong></h5><ul>
<li>设置单个运算操作（如矩阵乘法）的最大线程数。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">4</span>);</span><br></pre></td></tr></table></figure></li>
<li>默认值为 0，表示由 ONNX Runtime 自动决定线程数。</li>
</ul>
<h5 id="SetInterOpNumThreads"><a href="#SetInterOpNumThreads" class="headerlink" title="SetInterOpNumThreads"></a><strong><code>SetInterOpNumThreads</code></strong></h5><ul>
<li>设置多个操作之间的最大线程数。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetInterOpNumThreads</span>(<span class="number">2</span>);</span><br></pre></td></tr></table></figure></li>
<li>默认值为 0，表示自动决定线程数。</li>
</ul>
<hr>
<h4 id="2-设置优化级别"><a href="#2-设置优化级别" class="headerlink" title="2. 设置优化级别"></a><strong>2. 设置优化级别</strong></h4><h5 id="SetGraphOptimizationLevel"><a href="#SetGraphOptimizationLevel" class="headerlink" title="SetGraphOptimizationLevel"></a><strong><code>SetGraphOptimizationLevel</code></strong></h5><ul>
<li>设置 ONNX 模型图的优化级别。</li>
<li>可用优化级别：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ORT_DISABLE_ALL            <span class="comment">// 禁用所有优化</span></span><br><span class="line">ORT_ENABLE_BASIC           <span class="comment">// 启用基本优化（默认值）</span></span><br><span class="line">ORT_ENABLE_EXTENDED        <span class="comment">// 启用扩展优化（包括内存优化）</span></span><br><span class="line">ORT_ENABLE_ALL             <span class="comment">// 启用所有优化</span></span><br></pre></td></tr></table></figure></li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="3-配置执行提供程序"><a href="#3-配置执行提供程序" class="headerlink" title="3. 配置执行提供程序"></a><strong>3. 配置执行提供程序</strong></h4><h5 id="CPU-提供程序"><a href="#CPU-提供程序" class="headerlink" title="CPU 提供程序"></a><strong>CPU 提供程序</strong></h5><ul>
<li>默认情况下，ONNX Runtime 使用 CPU 提供程序。</li>
<li>如果明确指定：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">AppendExecutionProvider_CPU</span>(<span class="number">0</span>); <span class="comment">// 0 表示使用默认配置</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="GPU-提供程序（CUDA）"><a href="#GPU-提供程序（CUDA）" class="headerlink" title="GPU 提供程序（CUDA）"></a><strong>GPU 提供程序（CUDA）</strong></h5><ul>
<li>需要安装 ONNX Runtime 的 CUDA 支持版本。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cuda/cuda_provider_factory.h&gt;</span></span></span><br><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">AppendExecutionProvider_CUDA</span>(<span class="number">0</span>); <span class="comment">// 0 表示 GPU ID</span></span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="4-配置内存分配"><a href="#4-配置内存分配" class="headerlink" title="4. 配置内存分配"></a><strong>4. 配置内存分配</strong></h4><h5 id="EnableMemPattern"><a href="#EnableMemPattern" class="headerlink" title="EnableMemPattern"></a><strong><code>EnableMemPattern</code></strong></h5><ul>
<li>启用&#x2F;禁用内存模式优化（默认启用）。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">EnableMemPattern</span>(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="DisableMemPattern"><a href="#DisableMemPattern" class="headerlink" title="DisableMemPattern"></a><strong><code>DisableMemPattern</code></strong></h5><ul>
<li>禁用内存模式优化。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">DisableMemPattern</span>();</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="5-使用分配器"><a href="#5-使用分配器" class="headerlink" title="5. 使用分配器"></a><strong>5. 使用分配器</strong></h4><h5 id="设置分配器（可选）"><a href="#设置分配器（可选）" class="headerlink" title="设置分配器（可选）"></a><strong>设置分配器（可选）</strong></h5><p>用户可以指定自定义的内存分配器：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line">OrtAllocator* custom_allocator = ...; <span class="comment">// 用户自定义分配器</span></span><br><span class="line">session_options.<span class="built_in">AddConfigEntry</span>(<span class="string">&quot;memory.custom_allocator&quot;</span>, custom_allocator);</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="6-打开和关闭调试选项"><a href="#6-打开和关闭调试选项" class="headerlink" title="6. 打开和关闭调试选项"></a><strong>6. 打开和关闭调试选项</strong></h4><h5 id="启用调试"><a href="#启用调试" class="headerlink" title="启用调试"></a><strong>启用调试</strong></h5><p>用于调试模型推理的配置。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">EnableProfiling</span>(<span class="string">&quot;profiling_file.json&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="禁用调试"><a href="#禁用调试" class="headerlink" title="禁用调试"></a><strong>禁用调试</strong></h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">DisableProfiling</span>();</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="综合示例"><a href="#综合示例" class="headerlink" title="综合示例"></a><strong>综合示例</strong></h3><p>以下是一个使用 <code>Ort::SessionOptions</code> 的完整示例，展示了如何配置线程数、优化级别以及提供程序：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cuda/cuda_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置线程数</span></span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">4</span>);</span><br><span class="line">    session_options.<span class="built_in">SetInterOpNumThreads</span>(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置优化级别</span></span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加执行提供程序（使用 CUDA 或 CPU）</span></span><br><span class="line">    session_options.<span class="built_in">AppendExecutionProvider_CPU</span>(<span class="number">0</span>);  <span class="comment">// 使用 CPU 提供程序</span></span><br><span class="line">    <span class="comment">// session_options.AppendExecutionProvider_CUDA(0); // 若有 CUDA 支持，则使用 GPU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型并创建会话</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出模型的输入和输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    <span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Model Inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Model Outputs: &quot;</span> &lt;&lt; num_outputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>线程配置</strong>：</p>
<ul>
<li>如果设置了不合理的线程数（如过大或过小），可能导致性能下降或内存问题。</li>
</ul>
</li>
<li><p><strong>执行提供程序的顺序</strong>：</p>
<ul>
<li>ONNX Runtime 会按照添加的顺序选择第一个支持的提供程序。</li>
</ul>
</li>
<li><p><strong>图优化</strong>：</p>
<ul>
<li>高级优化（如 <code>ORT_ENABLE_ALL</code>）可能导致模型行为的细微变化，需要进行充分测试。</li>
</ul>
</li>
<li><p><strong>CUDA 提供程序的依赖</strong>：</p>
<ul>
<li>使用 GPU 时，请确保系统正确配置了 CUDA 和 cuDNN。</li>
</ul>
</li>
<li><p><strong>资源释放</strong>：</p>
<ul>
<li><code>Ort::SessionOptions</code> 在生命周期结束时会自动释放资源。</li>
</ul>
</li>
</ol>
<p>如果需要更深入了解某个功能或遇到问题，可以进一步探讨！</p>
<h2 id="Ort-Session-详解"><a href="#Ort-Session-详解" class="headerlink" title="Ort::Session 详解"></a>Ort::Session 详解</h2><p><code>Ort::Session</code> 是 ONNX Runtime C++ API 的核心类之一，负责加载 ONNX 模型并执行推理操作。一个 <code>Ort::Session</code> 对象对应一个加载的 ONNX 模型，并提供接口来查询模型信息和执行推理。</p>
<p>以下是对 <code>Ort::Session</code> 的详细解析：</p>
<hr>
<h2 id="Ort-Session-的主要功能"><a href="#Ort-Session-的主要功能" class="headerlink" title="Ort::Session 的主要功能"></a><strong><code>Ort::Session</code> 的主要功能</strong></h2><ol>
<li><p><strong>加载模型</strong>：</p>
<ul>
<li>使用 ONNX Runtime 环境 (<code>Ort::Env</code>) 和会话选项 (<code>Ort::SessionOptions</code>) 加载一个 ONNX 模型文件。</li>
</ul>
</li>
<li><p><strong>获取模型元信息</strong>：</p>
<ul>
<li>查询模型的输入和输出节点、数据类型、维度等。</li>
</ul>
</li>
<li><p><strong>执行推理</strong>：</p>
<ul>
<li>使用模型的输入数据进行推理，并返回结果。</li>
</ul>
</li>
<li><p><strong>管理资源</strong>：</p>
<ul>
<li><code>Ort::Session</code> 在其生命周期内负责维护模型相关的资源，并在销毁时自动释放。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="构造和基本用法-1"><a href="#构造和基本用法-1" class="headerlink" title="构造和基本用法"></a><strong>构造和基本用法</strong></h2><h3 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a><strong>构造函数</strong></h3><h4 id="1-默认构造"><a href="#1-默认构造" class="headerlink" title="1. 默认构造"></a>1. <strong>默认构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>参数说明</strong>：<ul>
<li><code>env</code>：一个有效的 <code>Ort::Env</code> 实例。</li>
<li><code>&quot;model.onnx&quot;</code>：ONNX 模型文件的路径。</li>
<li><code>session_options</code>：一个 <code>Ort::SessionOptions</code> 对象，用于配置推理行为。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="主要成员函数-1"><a href="#主要成员函数-1" class="headerlink" title="主要成员函数"></a><strong>主要成员函数</strong></h2><h3 id="1-获取模型输入输出信息"><a href="#1-获取模型输入输出信息" class="headerlink" title="1. 获取模型输入输出信息"></a><strong>1. 获取模型输入输出信息</strong></h3><h4 id="GetInputCount-和-GetOutputCount"><a href="#GetInputCount-和-GetOutputCount" class="headerlink" title="GetInputCount 和 GetOutputCount"></a><strong><code>GetInputCount</code> 和 <code>GetOutputCount</code></strong></h4><ul>
<li>分别返回模型的输入和输出节点数量。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line"><span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="GetInputName-和-GetOutputName"><a href="#GetInputName-和-GetOutputName" class="headerlink" title="GetInputName 和 GetOutputName"></a><strong><code>GetInputName</code> 和 <code>GetOutputName</code></strong></h4><ul>
<li>返回指定输入或输出节点的名称。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="GetInputTypeInfo-和-GetOutputTypeInfo"><a href="#GetInputTypeInfo-和-GetOutputTypeInfo" class="headerlink" title="GetInputTypeInfo 和 GetOutputTypeInfo"></a><strong><code>GetInputTypeInfo</code> 和 <code>GetOutputTypeInfo</code></strong></h4><ul>
<li>返回输入或输出节点的数据类型和维度信息。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">auto</span> input_shape = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"><span class="keyword">auto</span> input_dims = input_shape.<span class="built_in">GetShape</span>(); <span class="comment">// 获取输入的维度</span></span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="2-推理操作"><a href="#2-推理操作" class="headerlink" title="2. 推理操作"></a><strong>2. 推理操作</strong></h3><h4 id="Run"><a href="#Run" class="headerlink" title="Run"></a><strong><code>Run</code></strong></h4><ul>
<li>使用输入数据执行推理并返回结果。</li>
<li><strong>原型</strong>：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;Ort::Value&gt; <span class="title">Run</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Ort::RunOptions&amp; run_options,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span>* input_names,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> Ort::Value* input_tensors,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> input_count,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span>* output_names,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> output_count</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li><strong>参数说明</strong>：<ul>
<li><code>run_options</code>：推理选项，通常可以传入默认值。</li>
<li><code>input_names</code>：输入节点名称数组。</li>
<li><code>input_tensors</code>：输入数据张量数组。</li>
<li><code>input_count</code>：输入节点数量。</li>
<li><code>output_names</code>：输出节点名称数组。</li>
<li><code>output_count</code>：输出节点数量。</li>
</ul>
</li>
<li><strong>返回值</strong>：<ul>
<li>推理结果的 <code>Ort::Value</code> 对象数组。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="综合示例：执行推理"><a href="#综合示例：执行推理" class="headerlink" title="综合示例：执行推理"></a><strong>综合示例：执行推理</strong></h3><p>以下代码展示了如何加载模型并执行推理：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询模型输入信息</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    <span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">auto</span> input_shape = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; input_tensor_values = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_dims = &#123;<span class="number">1</span>, <span class="number">4</span>&#125;; <span class="comment">// 假设输入是一个 1x4 的张量</span></span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        allocator, input_tensor_values.<span class="built_in">data</span>(), input_tensor_values.<span class="built_in">size</span>(),</span><br><span class="line">        input_dims.<span class="built_in">data</span>(), input_dims.<span class="built_in">size</span>()</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">        Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,              <span class="comment">// 默认推理选项</span></span><br><span class="line">        &amp;input_name, &amp;input_tensor, <span class="number">1</span>,         <span class="comment">// 输入名和输入张量</span></span><br><span class="line">        &amp;output_name, <span class="number">1</span>                        <span class="comment">// 输出名</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析输出</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> output_shape = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : output_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Values: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetElementCount</span>(); ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; output_data[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>线程安全性</strong>：</p>
<ul>
<li><code>Ort::Session</code> 是线程安全的，可以在多线程环境中同时使用。</li>
</ul>
</li>
<li><p><strong>内存管理</strong>：</p>
<ul>
<li>使用 ONNX Runtime 的默认分配器 (<code>Ort::AllocatorWithDefaultOptions</code>) 来管理内存。</li>
<li>所有使用分配器分配的内存需要显式释放或交由分配器管理。</li>
</ul>
</li>
<li><p><strong>推理性能</strong>：</p>
<ul>
<li>使用 <code>Ort::SessionOptions</code> 配置优化选项（如线程数和图优化级别）可以显著提升推理性能。</li>
</ul>
</li>
<li><p><strong>模型输入&#x2F;输出验证</strong>：</p>
<ul>
<li>在执行推理之前，确保输入数据的维度和类型与模型的输入定义一致。</li>
</ul>
</li>
<li><p><strong>错误处理</strong>：</p>
<ul>
<li>如果模型文件路径错误或输入数据不匹配，ONNX Runtime 会抛出异常，需要进行捕获和处理。</li>
</ul>
</li>
</ol>
<hr>
<p>如果需要进一步深入了解 <code>Ort::Session</code> 或相关操作（如自定义提供程序、动态输入处理等），可以继续提问！</p>
<h2 id="Ort-AllocatorWithDefaultOptions-详解"><a href="#Ort-AllocatorWithDefaultOptions-详解" class="headerlink" title="Ort::AllocatorWithDefaultOptions 详解"></a>Ort::AllocatorWithDefaultOptions 详解</h2><p><code>Ort::AllocatorWithDefaultOptions</code> 是 ONNX Runtime C++ API 提供的一个简化工具类，用于分配和管理内存资源，尤其在操作模型输入输出信息时非常常用。</p>
<p>它封装了 ONNX Runtime 的默认分配器，简化了内存分配、释放的操作，是处理 <code>Ort::Session</code> 返回的字符串、张量等动态内存资源时的重要工具。</p>
<hr>
<h2 id="Ort-AllocatorWithDefaultOptions-的主要功能"><a href="#Ort-AllocatorWithDefaultOptions-的主要功能" class="headerlink" title="Ort::AllocatorWithDefaultOptions 的主要功能"></a><strong><code>Ort::AllocatorWithDefaultOptions</code> 的主要功能</strong></h2><ol>
<li><p><strong>动态分配内存</strong>：</p>
<ul>
<li>用于获取模型的输入输出名称、类型信息等，这些信息通常是动态分配的内存。</li>
</ul>
</li>
<li><p><strong>简化内存管理</strong>：</p>
<ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 自动处理分配的内存释放，避免手动管理内存的复杂性。</li>
</ul>
</li>
<li><p><strong>轻量级</strong>：</p>
<ul>
<li>这是一个无状态的 RAII（资源获取即初始化）类，创建和销毁的开销非常小。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="用法和接口"><a href="#用法和接口" class="headerlink" title="用法和接口"></a><strong>用法和接口</strong></h2><h3 id="1-构造函数"><a href="#1-构造函数" class="headerlink" title="1. 构造函数"></a><strong>1. 构造函数</strong></h3><p><code>Ort::AllocatorWithDefaultOptions</code> 无需任何参数即可默认构造：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-常用方法"><a href="#2-常用方法" class="headerlink" title="2. 常用方法"></a><strong>2. 常用方法</strong></h3><h4 id="operator-OrtAllocator"><a href="#operator-OrtAllocator" class="headerlink" title="operator OrtAllocator*"></a><strong><code>operator OrtAllocator*</code></strong></h4><ul>
<li>提供一个隐式转换操作符，将对象转换为底层的 <code>OrtAllocator*</code> 类型。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrtAllocator* raw_allocator = allocator;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="内存管理功能"><a href="#内存管理功能" class="headerlink" title="内存管理功能"></a><strong>内存管理功能</strong></h4><ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 自动调用 ONNX Runtime 的默认分配器接口，例如分配和释放内存：<ul>
<li><strong>分配内存</strong>：<code>allocator</code> 在调用 API（如 <code>GetInputName</code>）时会自动分配内存。</li>
<li><strong>释放内存</strong>：当 <code>allocator</code> 作用域结束时，分配的内存会被自动释放。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h2><h3 id="1-获取输入输出名称"><a href="#1-获取输入输出名称" class="headerlink" title="1. 获取输入输出名称"></a><strong>1. 获取输入输出名称</strong></h3><p>在获取模型输入和输出名称时，返回的名称是动态分配的内存，<code>Ort::AllocatorWithDefaultOptions</code> 自动管理这部分内存。</p>
<h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入名称</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出名称</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Name: &quot;</span> &lt;&lt; output_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-与动态分配相关的-API"><a href="#2-与动态分配相关的-API" class="headerlink" title="2. 与动态分配相关的 API"></a><strong>2. 与动态分配相关的 API</strong></h3><p>许多 ONNX Runtime 的 C++ API 使用动态内存分配，<code>Ort::AllocatorWithDefaultOptions</code> 是处理这些 API 的关键工具。例如：</p>
<h4 id="获取输入输出类型信息"><a href="#获取输入输出类型信息" class="headerlink" title="获取输入输出类型信息"></a>获取输入输出类型信息</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">auto</span> input_tensor_info = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取输入的维度信息</span></span><br><span class="line"><span class="keyword">auto</span> input_dims = input_tensor_info.<span class="built_in">GetShape</span>();</span><br></pre></td></tr></table></figure>

<h4 id="获取分配器指针"><a href="#获取分配器指针" class="headerlink" title="获取分配器指针"></a>获取分配器指针</h4><p>当需要与 ONNX Runtime 的低层 API 交互时，可以通过 <code>Ort::AllocatorWithDefaultOptions</code> 提供的 <code>OrtAllocator*</code> 指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrtAllocator* raw_allocator = allocator;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="内部工作原理"><a href="#内部工作原理" class="headerlink" title="内部工作原理"></a><strong>内部工作原理</strong></h2><p><code>Ort::AllocatorWithDefaultOptions</code> 基于 ONNX Runtime 的默认分配器实现，以下是它的工作机制：</p>
<ol>
<li><p><strong>初始化默认分配器</strong>：</p>
<ul>
<li>使用 ONNX Runtime 的 <code>OrtCreateAllocatorWithDefaultOptions</code> 方法创建分配器。</li>
<li>该分配器负责分配和释放内存。</li>
</ul>
</li>
<li><p><strong>自动管理内存</strong>：</p>
<ul>
<li>通过 RAII 模式，确保分配器的生命周期与 <code>Ort::AllocatorWithDefaultOptions</code> 实例绑定。</li>
<li>在分配器超出作用域时，自动释放所有相关资源。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>作用域管理</strong>：</p>
<ul>
<li>确保 <code>Ort::AllocatorWithDefaultOptions</code> 的生命周期覆盖所有需要使用其分配内存的地方。</li>
<li>在使用动态内存后，分配器超出作用域时会自动释放内存。</li>
</ul>
</li>
<li><p><strong>多线程使用</strong>：</p>
<ul>
<li>ONNX Runtime 的默认分配器是线程安全的，可以在多线程环境中使用。</li>
</ul>
</li>
<li><p><strong>避免手动释放</strong>：</p>
<ul>
<li>如果已经使用 <code>Ort::AllocatorWithDefaultOptions</code> 进行内存分配，无需手动释放内存，否则可能导致重复释放的错误。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="完整示例：从模型中提取输入输出信息"><a href="#完整示例：从模型中提取输入输出信息" class="headerlink" title="完整示例：从模型中提取输入输出信息"></a><strong>完整示例：从模型中提取输入输出信息</strong></h2><p>以下是一个综合示例，展示如何使用 <code>Ort::AllocatorWithDefaultOptions</code> 获取输入输出名称和类型信息：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入信息</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(i, allocator);</span><br><span class="line">        <span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">        <span class="keyword">auto</span> input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;  Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of outputs: &quot;</span> &lt;&lt; num_outputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_outputs; ++i) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(i, allocator);</span><br><span class="line">        <span class="keyword">auto</span> output_info = session.<span class="built_in">GetOutputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = output_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">        <span class="keyword">auto</span> output_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Output &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; output_name &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;  Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : output_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 是处理动态分配内存的便捷工具类，尤其在获取输入输出名称等操作中必不可少。</li>
<li>它与 ONNX Runtime 的默认分配器紧密集成，简化了内存管理。</li>
<li>使用时只需关注业务逻辑，分配器的内存生命周期会自动管理。</li>
</ul>
<p>如果需要进一步探索其工作机制或更多使用场景，可以继续提问！</p>
<h2 id="Session-GetInputTypeInfo-详解"><a href="#Session-GetInputTypeInfo-详解" class="headerlink" title="Session::GetInputTypeInfo() 详解"></a>Session::GetInputTypeInfo() 详解</h2><p><code>Session::GetInputTypeInfo()</code> 是 ONNX Runtime C++ API 中的一个重要方法，用于获取模型某个输入节点的类型和形状信息。通过这个方法，你可以详细了解模型输入的张量数据类型和维度，从而确保输入数据格式符合模型的要求。</p>
<hr>
<h2 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::TypeInfo <span class="title">GetInputTypeInfo</span><span class="params">(<span class="type">size_t</span> index)</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h3><ul>
<li><code>index</code>：输入节点的索引，从 <code>0</code> 开始。</li>
</ul>
<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>Ort::TypeInfo</code> 对象，包含输入节点的数据类型和形状信息。</li>
</ul>
<hr>
<h2 id="Ort-TypeInfo-的功能"><a href="#Ort-TypeInfo-的功能" class="headerlink" title="Ort::TypeInfo 的功能"></a><strong>Ort::TypeInfo 的功能</strong></h2><p><code>Ort::TypeInfo</code> 是 ONNX Runtime 用于描述节点数据类型和形状的类。通过 <code>TypeInfo</code> 对象，你可以进一步获取以下信息：</p>
<ol>
<li><strong>节点是否为张量</strong>。</li>
<li><strong>张量的数据类型</strong>。</li>
<li><strong>张量的形状</strong>。</li>
</ol>
<hr>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a><strong>使用方法</strong></h2><p>以下是一个完整的示例，展示如何使用 <code>GetInputTypeInfo()</code> 获取模型输入节点的类型和形状信息：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型的输入节点数量</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历输入节点，获取类型和形状信息</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="comment">// 获取输入节点名称</span></span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(i, allocator);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入节点类型信息</span></span><br><span class="line">        Ort::TypeInfo input_type_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_type_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据类型</span></span><br><span class="line">        ONNXTensorElementDataType input_data_type = tensor_info.<span class="built_in">GetElementType</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Data Type: &quot;</span> &lt;&lt; input_data_type &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入形状</span></span><br><span class="line">        <span class="keyword">auto</span> input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) &#123;</span><br><span class="line">            std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Ort-TensorTypeAndShapeInfo-的功能"><a href="#Ort-TensorTypeAndShapeInfo-的功能" class="headerlink" title="Ort::TensorTypeAndShapeInfo 的功能"></a><strong>Ort::TensorTypeAndShapeInfo 的功能</strong></h2><p>通过 <code>TypeInfo</code> 对象的 <code>GetTensorTypeAndShapeInfo()</code> 方法，可以获取输入节点的详细张量信息，返回的是一个 <code>Ort::TensorTypeAndShapeInfo</code> 对象。</p>
<h3 id="1-获取张量数据类型"><a href="#1-获取张量数据类型" class="headerlink" title="1. 获取张量数据类型"></a><strong>1. 获取张量数据类型</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ONNXTensorElementDataType <span class="title">GetElementType</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回值是一个枚举值，表示张量的数据类型。例如：<ul>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code> 表示 <code>float</code> 类型。</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64</code> 表示 <code>int64_t</code> 类型。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-获取张量形状"><a href="#2-获取张量形状" class="headerlink" title="2. 获取张量形状"></a><strong>2. 获取张量形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">int64_t</span>&gt; <span class="title">GetShape</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回一个包含张量每个维度大小的 <code>std::vector&lt;int64_t&gt;</code>。</li>
<li>对于动态维度，返回的值为 <code>-1</code>。</li>
</ul>
<hr>
<h3 id="3-获取元素总数"><a href="#3-获取元素总数" class="headerlink" title="3. 获取元素总数"></a><strong>3. 获取元素总数</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">GetElementCount</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回张量中元素的总数量，等于所有维度大小的乘积。</li>
<li>如果张量包含动态维度，无法计算总数时会抛出异常。</li>
</ul>
<hr>
<h2 id="输出示例"><a href="#输出示例" class="headerlink" title="输出示例"></a><strong>输出示例</strong></h2><p>假设我们加载的模型有一个输入节点，名称为 <code>input_0</code>，数据类型为 <code>float</code>，形状为 <code>[1, 3, 224, 224]</code>。运行上述代码的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Number of inputs: 1</span><br><span class="line">Input Name: input_0</span><br><span class="line">Data Type: 1</span><br><span class="line">Shape: 1 3 224 224</span><br></pre></td></tr></table></figure>

<ul>
<li>数据类型 <code>1</code> 对应 <code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>。</li>
<li>形状 <code>[1, 3, 224, 224]</code> 表示张量的维度。</li>
</ul>
<hr>
<h2 id="注意事项-4"><a href="#注意事项-4" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>动态维度</strong>：</p>
<ul>
<li>如果模型的某个维度是动态的，返回的值将是 <code>-1</code>。在这种情况下，需要根据具体推理场景动态确定实际的维度。</li>
</ul>
</li>
<li><p><strong>节点索引越界</strong>：</p>
<ul>
<li>如果输入 <code>index</code> 超出范围（大于或等于输入节点数量），会抛出异常，需要确保索引有效。</li>
</ul>
</li>
<li><p><strong>数据类型匹配</strong>：</p>
<ul>
<li>确保推理时提供的输入张量数据类型与模型的输入要求一致，否则会导致运行时错误。</li>
</ul>
</li>
<li><p><strong>多输入模型</strong>：</p>
<ul>
<li>如果模型有多个输入，需要分别对每个输入调用 <code>GetInputTypeInfo()</code> 获取信息。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="综合小结"><a href="#综合小结" class="headerlink" title="综合小结"></a><strong>综合小结</strong></h2><ul>
<li><code>Session::GetInputTypeInfo()</code> 是了解模型输入节点属性的核心方法。</li>
<li>它返回一个 <code>Ort::TypeInfo</code> 对象，进一步提供张量的数据类型和形状信息。</li>
<li>使用时需要注意动态维度的处理和数据类型的匹配。</li>
</ul>
<p>如需进一步了解 <code>Ort::TypeInfo</code> 或动态输入处理，可以继续提问！</p>
<h2 id="TensorTypeAndShapeInfo-GetShape-详解"><a href="#TensorTypeAndShapeInfo-GetShape-详解" class="headerlink" title="TensorTypeAndShapeInfo::GetShape() 详解"></a>TensorTypeAndShapeInfo::GetShape() 详解</h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 是 ONNX Runtime C++ API 中的一个方法，主要用于获取 ONNX 模型的张量输入或输出的形状信息。它返回一个包含张量维度大小的 <code>std::vector&lt;int64_t&gt;</code>，这些维度可以是静态或动态的。</p>
<hr>
<h2 id="函数原型-1"><a href="#函数原型-1" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">int64_t</span>&gt; <span class="title">GetShape</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a><strong>参数</strong></h3><ul>
<li>此方法无参数，它直接作用于 <code>TensorTypeAndShapeInfo</code> 对象。</li>
</ul>
<h3 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>std::vector&lt;int64_t&gt;</code>，其中每个元素表示张量在该维度上的大小。如果维度是动态的，该维度的大小为 <code>-1</code>。</li>
</ul>
<hr>
<h2 id="功能和用途"><a href="#功能和用途" class="headerlink" title="功能和用途"></a><strong>功能和用途</strong></h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 方法的主要作用是返回张量的形状信息，这些信息对于输入数据的格式化和验证至关重要。它允许你获取张量在各个维度上的大小，通常用于确定输入数据或输出数据是否符合模型的要求。</p>
<ul>
<li><strong>静态维度</strong>：对于静态维度（在模型加载时确定的维度），返回的是实际的维度大小。</li>
<li><strong>动态维度</strong>：如果张量的某些维度是动态的（例如在模型训练时，模型的输入维度没有固定大小），返回的维度值为 <code>-1</code>，表示该维度的大小是动态变化的，通常需要根据输入数据在推理时动态确定。</li>
</ul>
<hr>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a><strong>使用示例</strong></h2><p>假设你已经加载了一个 ONNX 模型并且调用了 <code>GetInputTypeInfo()</code> 或 <code>GetOutputTypeInfo()</code> 来获取模型输入或输出的张量类型信息。接下来，你可以使用 <code>GetShape()</code> 获取张量的形状。</p>
<h3 id="示例代码：获取输入形状"><a href="#示例代码：获取输入形状" class="headerlink" title="示例代码：获取输入形状"></a><strong>示例代码：获取输入形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入数量</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历输入节点，获取形状</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="comment">// 获取输入类型信息</span></span><br><span class="line">        Ort::TypeInfo input_type_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_type_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取张量形状</span></span><br><span class="line">        std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) &#123;</span><br><span class="line">            std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="输出示例-1"><a href="#输出示例-1" class="headerlink" title="输出示例"></a><strong>输出示例</strong></h3><p>假设模型的输入是一个 4D 张量，形状为 <code>[1, 3, 224, 224]</code>，那么输出将类似于：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Number of inputs: 1</span><br><span class="line">Input 0 shape: 1 3 224 224</span><br></pre></td></tr></table></figure>

<p>如果输入的某个维度是动态的，例如批量大小（batch size）是动态的，那么输出可能是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Input 0 shape: -1 3 224 224</span><br></pre></td></tr></table></figure>

<p><code>-1</code> 表示批量大小是动态的，推理时由输入数据的批量大小来确定。</p>
<hr>
<h2 id="GetShape-的返回值解析"><a href="#GetShape-的返回值解析" class="headerlink" title="GetShape() 的返回值解析"></a><strong><code>GetShape()</code> 的返回值解析</strong></h2><p><code>GetShape()</code> 返回的 <code>std::vector&lt;int64_t&gt;</code> 每个元素表示一个维度的大小。下面是关于维度和返回值的一些常见情况：</p>
<h3 id="1-静态维度"><a href="#1-静态维度" class="headerlink" title="1. 静态维度"></a><strong>1. 静态维度</strong></h3><p>如果模型的输入或输出具有固定的维度，则 <code>GetShape()</code> 返回的是该张量的具体形状。例如：</p>
<ul>
<li><p>对于一个 <code>1x3x224x224</code> 的图像张量（通常用于图像分类任务），<code>GetShape()</code> 返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于一个二维张量（例如一个大小为 <code>100x50</code> 的矩阵），<code>GetShape()</code> 返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">100</span>, <span class="number">50</span>&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-动态维度"><a href="#2-动态维度" class="headerlink" title="2. 动态维度"></a><strong>2. 动态维度</strong></h3><p>如果模型的某些维度是动态的，则返回值中的该维度将是 <code>-1</code>，表示该维度在推理时会根据实际输入数据动态确定。常见的动态维度包括批量大小（batch size）。</p>
<ul>
<li><p>假设模型的输入形状是 <code>[batch_size, 3, 224, 224]</code>，其中 <code>batch_size</code> 是动态的。<code>GetShape()</code> 可能返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">-1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>-1</code> 表示 <code>batch_size</code> 是动态的，具体的值将在实际推理时由输入数据的大小确定。</p>
</li>
<li><p>如果模型具有 <code>sequence_length</code> 等动态维度，类似地，返回的形状可能为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">10</span>, <span class="number">-1</span>&#125;  <span class="comment">// 10 是固定维度，-1 表示第二维是动态的</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-无效维度"><a href="#3-无效维度" class="headerlink" title="3. 无效维度"></a><strong>3. 无效维度</strong></h3><p>如果张量没有形状（例如是一个标量），<code>GetShape()</code> 返回的向量为空：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#125;  <span class="comment">// 无形状的张量</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="常见问题和注意事项"><a href="#常见问题和注意事项" class="headerlink" title="常见问题和注意事项"></a><strong>常见问题和注意事项</strong></h2><ol>
<li><p><strong>动态维度的处理</strong>：</p>
<ul>
<li>对于动态维度，返回的形状中相应位置的值为 <code>-1</code>，你无法直接知道该维度的大小。通常，实际的维度会在推理时根据输入的大小动态确定。</li>
</ul>
</li>
<li><p><strong>如何获取元素数量</strong>：</p>
<ul>
<li>如果模型的输入具有静态形状，可以通过 <code>GetShape()</code> 返回的维度大小计算张量的元素数量。</li>
<li>例如，假设返回的形状是 <code>[1, 3, 224, 224]</code>，那么元素总数为 <code>1 * 3 * 224 * 224 = 150528</code>。</li>
</ul>
</li>
<li><p><strong>性能优化</strong>：</p>
<ul>
<li>当处理动态维度时，你可能需要动态调整输入的大小。通过 <code>GetShape()</code> 获取输入形状后，你可以根据实际情况调整输入数据的维度。</li>
</ul>
</li>
<li><p><strong>错误处理</strong>：</p>
<ul>
<li>如果在尝试获取形状时遇到问题（如 <code>session.GetInputTypeInfo()</code> 返回错误），确保模型输入的维度与预期一致。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 是 ONNX Runtime 中获取张量形状信息的关键方法。它返回张量的维度信息，帮助你在执行推理之前确认输入和输出数据的结构。了解如何处理静态和动态维度，对于正确地为模型准备输入数据至关重要。</p>
<p>如果你对 <code>GetShape()</code> 或其他 ONNX Runtime 方法有进一步的疑问，欢迎继续提问！</p>
<h2 id="1-3-224-224-表示的什么"><a href="#1-3-224-224-表示的什么" class="headerlink" title="[1, 3, 224, 224] 表示的什么"></a>[1, 3, 224, 224] 表示的什么</h2><p><code>[1, 3, 224, 224]</code> 是一个形状（Shape）表示，通常用于描述图像张量的维度。在计算机视觉和深度学习任务中，这种表示方法是非常常见的。让我们逐步分析它：</p>
<h3 id="1-3-224-224-解释"><a href="#1-3-224-224-解释" class="headerlink" title="[1, 3, 224, 224] 解释"></a><strong><code>[1, 3, 224, 224]</code> 解释</strong></h3><ul>
<li><p><strong>1</strong>：批量大小（Batch Size）</p>
<ul>
<li>这个数字表示图像数据批次中包含的图像数量。在这个例子中，批量大小为 <code>1</code>，意味着这只是一个单独的图像。批量大小是深度学习模型训练和推理过程中常用的概念。</li>
</ul>
</li>
<li><p><strong>3</strong>：颜色通道数（Channels）</p>
<ul>
<li>这个数字表示图像的颜色通道数。在这种情况下，<code>3</code> 代表图像是 RGB 彩色图像，意味着图像包含三个颜色通道：红色（Red）、绿色（Green）和蓝色（Blue）。每个通道包含一个灰度值，表示颜色的强度。</li>
</ul>
</li>
<li><p><strong>224</strong>：图像的高度（Height）</p>
<ul>
<li>这个数字表示图像的垂直尺寸，单位是像素。在这个例子中，图像的高度为 <code>224</code> 像素，表示图像的垂直方向有 224 个像素点。</li>
</ul>
</li>
<li><p><strong>224</strong>：图像的宽度（Width）</p>
<ul>
<li>这个数字表示图像的水平尺寸，单位也是像素。在这个例子中，图像的宽度为 <code>224</code> 像素，表示图像的水平方向有 224 个像素点。</li>
</ul>
</li>
</ul>
<h3 id="综合解释"><a href="#综合解释" class="headerlink" title="综合解释"></a><strong>综合解释</strong></h3><ul>
<li><code>1</code>：批量大小为 <code>1</code>，意味着只有一个图像。</li>
<li><code>3</code>：该图像为彩色图像，采用 RGB 色彩空间，因此有 3 个颜色通道。</li>
<li><code>224</code> 和 <code>224</code>：该图像的尺寸为 224x224 像素，表示图像的高度和宽度都是 224 像素。</li>
</ul>
<p>因此，<code>[1, 3, 224, 224]</code> 表示一个单独的 RGB 彩色图像，尺寸为 224x224 像素，具有 3 个颜色通道（红、绿、蓝）。这种形状常见于深度学习任务，特别是图像分类或卷积神经网络（CNN）的输入。</p>
<hr>
<h3 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h3><p>这个张量形状通常用于深度学习框架，如 PyTorch、ONNX 或 TensorFlow，在图像数据的训练和推理过程中经常出现。例如，如果你使用一个深度学习模型来分类图像，模型通常期望输入的形状是 <code>[B, C, H, W]</code>，其中：</p>
<ul>
<li><code>B</code> 是批量大小。</li>
<li><code>C</code> 是颜色通道数。</li>
<li><code>H</code> 是图像的高度。</li>
<li><code>W</code> 是图像的宽度。</li>
</ul>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>[1, 3, 224, 224]</code> 表示一个具有 224x224 像素尺寸的 RGB 彩色图像，批量大小为 1。</li>
</ul>
<h2 id="什么是图像张量"><a href="#什么是图像张量" class="headerlink" title="什么是图像张量"></a>什么是图像张量</h2><p>图像张量（Image Tensor）是一个用于表示图像数据的多维数组（张量），通常用于深度学习、计算机视觉和其他机器学习任务。张量是一个数学概念，广泛用于表示高维数据。图像张量是张量在计算机视觉领域的具体应用，它通过数值数组的形式表示图像。</p>
<p>在深度学习和计算机视觉任务中，图像通常作为输入数据，通过神经网络进行处理和分析。为了能够处理图像，图像数据需要以一种数值形式表示，而这正是张量的作用所在。</p>
<hr>
<h2 id="图像张量的基本结构"><a href="#图像张量的基本结构" class="headerlink" title="图像张量的基本结构"></a><strong>图像张量的基本结构</strong></h2><p>图像张量的维度（Shape）通常与图像的表示方式、颜色通道数、分辨率和数据类型等因素相关。常见的图像张量表示方式包括以下几种：</p>
<h3 id="1-彩色图像张量"><a href="#1-彩色图像张量" class="headerlink" title="1. 彩色图像张量"></a><strong>1. 彩色图像张量</strong></h3><p>对于彩色图像，通常采用<strong>RGB</strong>（红绿蓝）颜色空间来表示。每个颜色通道的值通常是一个整数（比如 <code>0</code> 到 <code>255</code> 之间）或浮动的值（例如标准化到 <code>[0, 1]</code> 的浮动值）。</p>
<ul>
<li><strong>形状</strong>：<code>[C, H, W]</code><ul>
<li><strong>C</strong>：颜色通道数（通常是 <code>3</code>，表示 RGB 三个颜色通道）。</li>
<li><strong>H</strong>：图像的高度（即图像的行数，表示图像的垂直尺寸）。</li>
<li><strong>W</strong>：图像的宽度（即图像的列数，表示图像的水平尺寸）。</li>
</ul>
</li>
</ul>
<p>例如，一个大小为 <code>224x224</code> 像素的 RGB 彩色图像可以用一个形状为 <code>[3, 224, 224]</code> 的张量表示。</p>
<h3 id="2-灰度图像张量"><a href="#2-灰度图像张量" class="headerlink" title="2. 灰度图像张量"></a><strong>2. 灰度图像张量</strong></h3><p>对于灰度图像，通常只有一个颜色通道，即每个像素只有一个灰度值，表示图像的亮度。</p>
<ul>
<li><strong>形状</strong>：<code>[1, H, W]</code><ul>
<li><strong>1</strong>：表示灰度图像只有一个颜色通道。</li>
<li><strong>H</strong>：图像的高度。</li>
<li><strong>W</strong>：图像的宽度。</li>
</ul>
</li>
</ul>
<p>例如，一个 <code>28x28</code> 像素的灰度图像可以用一个形状为 <code>[1, 28, 28]</code> 的张量表示。</p>
<h3 id="3-批量图像张量"><a href="#3-批量图像张量" class="headerlink" title="3. 批量图像张量"></a><strong>3. 批量图像张量</strong></h3><p>在深度学习中，通常一次性处理多个图像。因此，图像张量通常以批量（batch）的形式输入到神经网络中。</p>
<ul>
<li><strong>形状</strong>：<code>[B, C, H, W]</code><ul>
<li><strong>B</strong>：批量大小（batch size），表示一次处理多少图像。</li>
<li><strong>C</strong>：颜色通道数（通常是 <code>3</code> 表示 RGB，或 <code>1</code> 表示灰度）。</li>
<li><strong>H</strong>：图像的高度。</li>
<li><strong>W</strong>：图像的宽度。</li>
</ul>
</li>
</ul>
<p>例如，一个包含 32 张 <code>224x224</code> 的 RGB 彩色图像的批量张量，可以表示为形状为 <code>[32, 3, 224, 224]</code> 的张量。</p>
<hr>
<h2 id="图像张量的存储格式"><a href="#图像张量的存储格式" class="headerlink" title="图像张量的存储格式"></a><strong>图像张量的存储格式</strong></h2><p>图像张量的存储格式依赖于数据的顺序和布局。最常见的两种存储格式是：</p>
<ol>
<li><p><strong>CHW（Channel, Height, Width）格式</strong>：</p>
<ul>
<li>这是深度学习框架如 PyTorch、ONNX、Caffe 中常见的图像张量格式。颜色通道在前，接着是高度和宽度。</li>
<li>示例：<code>[3, 224, 224]</code> 表示一个 RGB 彩色图像，具有 3 个通道，尺寸为 <code>224x224</code>。</li>
</ul>
</li>
<li><p><strong>HWC（Height, Width, Channel）格式</strong>：</p>
<ul>
<li>这是图像处理库如 OpenCV 和 TensorFlow 中常见的图像表示格式。图像的高度和宽度在前，颜色通道在后。</li>
<li>示例：<code>[224, 224, 3]</code> 表示一个 RGB 彩色图像，具有 3 个通道，尺寸为 <code>224x224</code>。</li>
</ul>
</li>
</ol>
<p>不同框架和工具可能采用不同的存储格式，但它们都可以通过相应的工具进行转换。</p>
<hr>
<h2 id="图像张量的用途"><a href="#图像张量的用途" class="headerlink" title="图像张量的用途"></a><strong>图像张量的用途</strong></h2><p>图像张量在计算机视觉和深度学习中的用途非常广泛，主要包括：</p>
<ol>
<li><p><strong>输入神经网络</strong>：</p>
<ul>
<li>图像张量是深度学习模型，特别是卷积神经网络（CNN）的输入数据。通过神经网络的前向传播，模型会根据图像的特征进行分类、检测、分割等任务。</li>
</ul>
</li>
<li><p><strong>图像预处理</strong>：</p>
<ul>
<li>在将图像输入神经网络之前，通常会进行一些预处理操作，如调整图像的大小、标准化颜色值、数据增强（例如旋转、裁剪、翻转等）。这些操作通常会改变图像张量的形状或数值。</li>
</ul>
</li>
<li><p><strong>图像生成</strong>：</p>
<ul>
<li>在生成模型（如生成对抗网络 GAN）中，图像张量不仅作为输入，还作为模型的输出，生成新的图像。</li>
</ul>
</li>
<li><p><strong>图像变换</strong>：</p>
<ul>
<li>图像张量在图像处理任务中，涉及图像的各种变换操作，比如图像的滤波、边缘检测、图像增强等。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="图像张量的示例"><a href="#图像张量的示例" class="headerlink" title="图像张量的示例"></a><strong>图像张量的示例</strong></h2><p>以下是一个示例，展示如何使用 Python 和 NumPy 创建图像张量（以 NumPy 为例，虽然 Python 是常见的深度学习语言，但 ONNX Runtime 或 PyTorch 等框架也使用类似的张量表示方法）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 224x224 的 RGB 图像张量（3 个颜色通道）</span></span><br><span class="line">image_tensor = np.random.rand(<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 随机生成一个图像张量</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)  <span class="comment"># 输出张量的形状，应该是 (3, 224, 224)</span></span><br></pre></td></tr></table></figure>

<p>如果是批量图像（例如批量大小为 32）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个包含 32 张 224x224 RGB 图像的张量</span></span><br><span class="line">batch_image_tensor = np.random.rand(<span class="number">32</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(batch_image_tensor.shape)  <span class="comment"># 输出张量的形状，应该是 (32, 3, 224, 224)</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h2><ul>
<li>图像张量是一个多维数组，表示图像的数值数据，通常用于深度学习中的输入数据。</li>
<li>图像张量的常见形状包括 <code>[C, H, W]</code>（单张图像）和 <code>[B, C, H, W]</code>（批量图像）。</li>
<li>图像张量可以是彩色图像（RGB）或灰度图像，并通过颜色通道、图像的高度和宽度来表示。</li>
<li>在深度学习和计算机视觉任务中，图像张量是神经网络输入和输出的重要形式。</li>
</ul>
<p>图像张量为机器学习模型提供了可以处理的格式，是许多计算机视觉任务的基础。</p>
<h2 id="C-onnxruntime-Ort-Value-详解"><a href="#C-onnxruntime-Ort-Value-详解" class="headerlink" title="C++ onnxruntime Ort::Value 详解"></a>C++ onnxruntime Ort::Value 详解</h2><p>在 ONNX Runtime 中，<code>Ort::Value</code> 是一个非常重要的类，用于表示张量（tensor）数据。它封装了模型的输入和输出数据，并提供了一种与 ONNX Runtime 进行交互的方式。</p>
<h3 id="Ort-Value-介绍"><a href="#Ort-Value-介绍" class="headerlink" title="Ort::Value 介绍"></a><strong>Ort::Value 介绍</strong></h3><p><code>Ort::Value</code> 是 ONNX Runtime API 中用于表示数据的一个类，它可以用于存储模型的输入和输出数据，并为数据提供访问接口。你可以将数据包装到 <code>Ort::Value</code> 中，从而通过 ONNX Runtime 执行推理操作。它提供了对张量的访问、内存管理以及各种用于操作张量数据的功能。</p>
<h3 id="主要功能-1"><a href="#主要功能-1" class="headerlink" title="主要功能"></a><strong>主要功能</strong></h3><ol>
<li><p><strong>张量表示</strong>：<code>Ort::Value</code> 用于表示和存储模型的输入、输出数据，这些数据通常是多维数组，表示为张量（Tensor）。</p>
</li>
<li><p><strong>数据类型支持</strong>：<code>Ort::Value</code> 支持各种数据类型，如浮动类型（<code>float</code>、<code>double</code>）、整数类型（<code>int</code>、<code>long</code>）、布尔类型等。它也支持不同的张量格式，例如：<code>float32</code>、<code>int64</code> 等。</p>
</li>
<li><p><strong>内存管理</strong>：<code>Ort::Value</code> 内部管理内存，并负责在不再需要时释放内存。ONNX Runtime 会通过 RAII（资源获取即初始化）管理内存的生命周期。</p>
</li>
</ol>
<h3 id="创建-Ort-Value"><a href="#创建-Ort-Value" class="headerlink" title="创建 Ort::Value"></a><strong>创建 Ort::Value</strong></h3><p><code>Ort::Value</code> 可以通过几种不同的方式创建，以下是一些常见的方法：</p>
<ol>
<li><p><strong>从原始数据创建</strong>：<br>使用 <code>Ort::Value::CreateTensor</code> 或类似函数，可以从原始内存或数组创建一个张量。常见的场景是创建张量并作为输入传递给模型。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">std::vector&lt;<span class="type">float</span>&gt; input_data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>&#125;;</span><br><span class="line">std::array&lt;<span class="type">int64_t</span>, 1&gt; input_shape = &#123;<span class="number">3</span>&#125;;</span><br><span class="line">Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(allocator, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>从现有的内存（例如：std::vector）创建</strong>：<br>可以直接使用 <code>Ort::Value</code> 来包装现有的数据。可以通过 <code>Ort::Value::CreateTensor</code> 方法指定数据类型和形状。</p>
</li>
<li><p><strong>从 C++ 标准容器（如 <code>std::vector</code> 或 <code>std::array</code>）创建张量</strong>：<br><code>Ort::Value</code> 可以直接从容器对象创建张量，方便快速构建推理的输入数据。</p>
</li>
</ol>
<h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a><strong>常用方法</strong></h3><h4 id="1-获取数据类型"><a href="#1-获取数据类型" class="headerlink" title="1. 获取数据类型"></a>1. <strong>获取数据类型</strong></h4><p><code>Ort::Value</code> 提供了方法来获取存储的数据类型。例如，获取张量的数据类型（如 <code>float32</code>，<code>int64</code> 等）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ONNXTensorElementDataType type = input_tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetElementType</span>();</span><br></pre></td></tr></table></figure>

<h4 id="2-获取张量的形状"><a href="#2-获取张量的形状" class="headerlink" title="2. 获取张量的形状"></a>2. <strong>获取张量的形状</strong></h4><p>通过 <code>Ort::Value</code> 的 <code>GetTensorTypeAndShapeInfo()</code> 方法，你可以获取张量的形状（即它的维度）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ort::TensorTypeAndShapeInfo tensor_info = input_tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">std::vector&lt;<span class="type">int64_t</span>&gt; shape = tensor_info.<span class="built_in">GetShape</span>();</span><br></pre></td></tr></table></figure>

<h4 id="3-获取数据指针"><a href="#3-获取数据指针" class="headerlink" title="3. 获取数据指针"></a>3. <strong>获取数据指针</strong></h4><p><code>Ort::Value</code> 也允许你访问底层数据。可以使用 <code>GetTensorData</code> 来获取存储张量数据的指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* float_data = input_tensor.<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br></pre></td></tr></table></figure>

<h4 id="4-设置张量的值"><a href="#4-设置张量的值" class="headerlink" title="4. 设置张量的值"></a>4. <strong>设置张量的值</strong></h4><p>你可以通过 <code>Ort::Value</code> 设置张量的数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_tensor.<span class="built_in">SetTensorData</span>&lt;<span class="type">float</span>&gt;(input_data.<span class="built_in">data</span>());</span><br></pre></td></tr></table></figure>

<h4 id="5-转换为其他类型"><a href="#5-转换为其他类型" class="headerlink" title="5. 转换为其他类型"></a>5. <strong>转换为其他类型</strong></h4><p>通过 ONNX Runtime 提供的 API，你可以将 <code>Ort::Value</code> 转换为其他类型（例如：从张量转为 NumPy 数组，或通过其他方式访问数据）。</p>
<h3 id="示例代码：推理输入和输出"><a href="#示例代码：推理输入和输出" class="headerlink" title="示例代码：推理输入和输出"></a><strong>示例代码：推理输入和输出</strong></h3><p>下面是一个基本示例，展示了如何使用 <code>Ort::Value</code> 来执行 ONNX 模型推理：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_ptr.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/ort_value.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个 SessionOptions 对象</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载 ONNX 模型</span></span><br><span class="line">    std::string model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">onnx_session</span><span class="params">(env, model_path.c_str(), session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 准备输入数据 (例如: 3x3 数字)</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; input_data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>, <span class="number">5.0f</span>, <span class="number">6.0f</span>, <span class="number">7.0f</span>, <span class="number">8.0f</span>, <span class="number">9.0f</span>&#125;;</span><br><span class="line">    std::array&lt;<span class="type">int64_t</span>, 2&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">9</span>&#125;;  <span class="comment">// 假设这是一个 1x9 的输入</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Ort::Value 对象作为输入张量</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(allocator, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Ort::Value 对象作为输出张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_tensors;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    onnx_session.<span class="built_in">Run</span>(Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;, </span><br><span class="line">                     &amp;input_name, &amp;input_tensor, <span class="number">1</span>, <span class="comment">// 输入</span></span><br><span class="line">                     &amp;output_name, &amp;output_tensors, <span class="number">1</span>); <span class="comment">// 输出</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference result: &quot;</span> &lt;&lt; output_data[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::Value</code> 是 ONNX Runtime 用于表示和操作张量的核心类之一。</li>
<li>它提供了一种接口来管理输入和输出数据，支持多种数据类型和形状。</li>
<li>你可以通过 <code>Ort::Value</code> 来包装原始数据（如 <code>std::vector</code>），并将其传递给模型进行推理。</li>
<li>通过 <code>Ort::Value</code> 提供的接口，你可以方便地访问张量的数据、形状以及数据类型。</li>
</ul>
<p><code>Ort::Value</code> 的使用使得与 ONNX Runtime 进行交互变得简便而高效，能够帮助开发者轻松处理模型推理所需的各种数据。</p>
<h2 id="Ort-Value-CreateTensor-详解"><a href="#Ort-Value-CreateTensor-详解" class="headerlink" title="Ort::Value::CreateTensor 详解"></a>Ort::Value::CreateTensor 详解</h2><p><code>Ort::Value::CreateTensor</code> 是 ONNX Runtime 中用于创建张量（Tensor）数据的一个静态函数。张量是 ONNX Runtime 中数据的核心表示形式，通常用于表示模型的输入和输出。通过 <code>CreateTensor</code>，你可以将数据封装成 <code>Ort::Value</code> 对象，供模型推理使用。</p>
<h3 id="函数原型-2"><a href="#函数原型-2" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">static</span> Ort::Value <span class="title">CreateTensor</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Ort::Allocator&amp; allocator, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">void</span>* data, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> size, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int64_t</span>* shape, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> shape_len, </span></span></span><br><span class="line"><span class="params"><span class="function">    ONNXTensorElementDataType type)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a><strong>参数详解</strong></h3><ol>
<li><p>**<code>allocator</code>**（类型：<code>Ort::Allocator&amp;</code>）：</p>
<ul>
<li>用于分配内存的分配器，ONNX Runtime 会使用它来管理张量的内存。</li>
<li>推荐使用 <code>Ort::AllocatorWithDefaultOptions</code> 来创建一个默认的内存分配器。</li>
</ul>
</li>
<li><p>**<code>data</code>**（类型：<code>void*</code>）：</p>
<ul>
<li>一个指向存储数据的内存块的指针。数据将被存储在该指针指向的内存位置。</li>
<li>你需要将数据放入这块内存中，数据的类型和结构应该符合所需的张量类型和形状。</li>
</ul>
</li>
<li><p>**<code>size</code>**（类型：<code>size_t</code>）：</p>
<ul>
<li>数据的总大小，以字节为单位。</li>
<li><code>size</code> 应该等于张量的元素数量乘以每个元素的字节大小。例如，如果每个元素是 <code>float</code>（4 字节），而张量包含 100 个元素，那么 <code>size</code> 应该是 <code>100 * 4</code>。</li>
</ul>
</li>
<li><p>**<code>shape</code>**（类型：<code>const int64_t*</code>）：</p>
<ul>
<li>张量的形状，即每个维度的大小。形状是一个整型数组，表示张量的多维尺寸。</li>
<li>例如，对于一个 2D 张量，形状可能是 <code>&#123;3, 4&#125;</code>，表示该张量有 3 行 4 列。</li>
</ul>
</li>
<li><p>**<code>shape_len</code>**（类型：<code>size_t</code>）：</p>
<ul>
<li>张量形状的维度数量，即 <code>shape</code> 数组的长度。</li>
<li>例如，如果张量是 2D，<code>shape_len</code> 应该是 2；如果是 3D，<code>shape_len</code> 应该是 3，依此类推。</li>
</ul>
</li>
<li><p>**<code>type</code>**（类型：<code>ONNXTensorElementDataType</code>）：</p>
<ul>
<li>张量的数据类型，指定张量中元素的类型。ONNX 允许多种数据类型，包括：<ul>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>（<code>float</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32</code>（<code>int32</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64</code>（<code>int64</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8</code>（<code>uint8</code> 类型）</li>
<li>等等。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="返回值-2"><a href="#返回值-2" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>Ort::Value</code> 对象，表示创建的张量。</li>
<li><code>Ort::Value</code> 对象封装了张量的数据和形状，并可以在后续的推理过程中作为输入或输出使用。</li>
</ul>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h3><p>以下是一个创建张量并将其用于模型推理的完整示例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/ort_value.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_ptr.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/common.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/onnxruntime_typeinfo.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个简单的 2D 张量数据（例如：2x3 的矩阵）</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>, <span class="number">5.0f</span>, <span class="number">6.0f</span>&#125;;</span><br><span class="line">    std::array&lt;<span class="type">int64_t</span>, 2&gt; shape = &#123;<span class="number">2</span>, <span class="number">3</span>&#125;;  <span class="comment">// 张量的形状是 2x3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建张量对象</span></span><br><span class="line">    Ort::Value tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        allocator, </span><br><span class="line">        data.<span class="built_in">data</span>(),            <span class="comment">// 数据指针</span></span><br><span class="line">        data.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>),  <span class="comment">// 数据大小，单位是字节</span></span><br><span class="line">        shape.<span class="built_in">data</span>(),           <span class="comment">// 形状</span></span><br><span class="line">        shape.<span class="built_in">size</span>(),           <span class="comment">// 形状的长度</span></span><br><span class="line">        ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT  <span class="comment">// 数据类型</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取张量的类型和形状信息</span></span><br><span class="line">    Ort::TensorTypeAndShapeInfo tensor_info = tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; tensor_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印张量形状</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Tensor Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : tensor_shape) &#123;</span><br><span class="line">        std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 访问张量数据</span></span><br><span class="line">    <span class="type">float</span>* tensor_data = tensor.<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Tensor Data: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; data.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; tensor_data[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="详细解释"><a href="#详细解释" class="headerlink" title="详细解释"></a><strong>详细解释</strong></h3><ol>
<li><p><strong>内存分配</strong>：</p>
<ul>
<li>使用 <code>Ort::AllocatorWithDefaultOptions</code> 创建一个分配器对象，它将在创建张量时分配内存。</li>
</ul>
</li>
<li><p><strong>数据和形状</strong>：</p>
<ul>
<li>数据是一个 <code>std::vector&lt;float&gt;</code>，代表了一个 2x3 张量的数据。</li>
<li>形状 <code>shape</code> 是一个 <code>std::array&lt;int64_t, 2&gt;</code>，表示张量是 2 行 3 列的。</li>
</ul>
</li>
<li><p><strong>创建张量</strong>：</p>
<ul>
<li><code>Ort::Value::CreateTensor&lt;float&gt;</code> 方法用来创建一个 <code>float</code> 类型的张量。我们传入数据指针、数据大小、形状以及数据类型。</li>
<li>该方法将返回一个 <code>Ort::Value</code> 对象，封装了张量的所有信息。</li>
</ul>
</li>
<li><p><strong>获取张量信息</strong>：</p>
<ul>
<li>使用 <code>tensor.GetTensorTypeAndShapeInfo()</code> 获取张量的类型和形状信息。</li>
<li>使用 <code>tensor.GetTensorData&lt;float&gt;()</code> 获取存储在张量中的数据指针。</li>
</ul>
</li>
<li><p><strong>打印结果</strong>：</p>
<ul>
<li>打印张量的形状和数据内容。</li>
</ul>
</li>
</ol>
<h3 id="注意事项-5"><a href="#注意事项-5" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ul>
<li><strong>内存管理</strong>：<code>Ort::Value</code> 对象会自动管理张量的内存。因此，用户不需要手动释放张量的内存，只需保证它的生命周期在需要时有效。</li>
<li><strong>数据类型</strong>：确保传入的 <code>data</code> 的类型与指定的 <code>ONNXTensorElementDataType</code> 类型匹配。例如，如果选择 <code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>，则 <code>data</code> 必须是 <code>float</code> 类型的数据。</li>
</ul>
<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::Value::CreateTensor</code> 是一个用于创建张量的静态方法，可以将数据、形状和数据类型封装成 <code>Ort::Value</code> 对象。</li>
<li>它为 ONNX Runtime 模型推理提供了输入和输出的数据结构。</li>
<li>使用时需要提供内存分配器、数据、数据大小、形状以及数据类型等信息。</li>
</ul>
<h2 id="Session-Run-函数-详解"><a href="#Session-Run-函数-详解" class="headerlink" title="Session::Run() 函数 详解"></a>Session::Run() 函数 详解</h2><p><code>Session::Run()</code> 是 ONNX Runtime 中的一个关键函数，负责执行模型推理（即前向传播）。它的主要作用是根据输入数据进行推理，并返回推理结果。你通过它将输入数据传递给 ONNX 模型，并得到相应的输出。</p>
<h3 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a><strong>函数定义</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Run</span><span class="params">(<span class="type">const</span> Session::RunOptions&amp; run_options,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;std::string&gt;&amp; input_names,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;<span class="type">const</span> Ort::Value*&gt;&amp; input_values,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;std::string&gt;&amp; output_names,</span></span></span><br><span class="line"><span class="params"><span class="function">           std::vector&lt;Ort::Value&gt;&amp; output_values)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a><strong>参数说明</strong></h3><ol>
<li><p>**<code>run_options</code> (Session::RunOptions)**：</p>
<ul>
<li><strong>类型</strong>：<code>const Session::RunOptions&amp;</code></li>
<li><strong>描述</strong>：设置推理过程中的一些选项，比如是否使用优化等。可以为空，表示使用默认选项。</li>
</ul>
</li>
<li><p>**<code>input_names</code> (std::vector<a href="std::string">std::string</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;std::string&gt;&amp;</code></li>
<li><strong>描述</strong>：输入张量的名称列表。对于每个输入张量，你需要提供对应的名称，这些名称必须与模型中定义的输入名称一致。</li>
</ul>
</li>
<li><p>**<code>input_values</code> (std::vector&lt;const Ort::Value*&gt;)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;const Ort::Value*&gt;&amp;</code></li>
<li><strong>描述</strong>：输入数据的值列表。每个输入对应一个 <code>Ort::Value</code> 对象，这些对象包含了输入数据。数据的类型和形状应与模型要求的输入一致。</li>
</ul>
</li>
<li><p>**<code>output_names</code> (std::vector<a href="std::string">std::string</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;std::string&gt;&amp;</code></li>
<li><strong>描述</strong>：输出张量的名称列表。指定你希望从模型中获取的输出名称。如果模型有多个输出，你需要提供所有输出的名称。</li>
</ul>
</li>
<li><p>**<code>output_values</code> (std::vector<a href="Ort::Value">Ort::Value</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>std::vector&lt;Ort::Value&gt;&amp;</code></li>
<li><strong>描述</strong>：模型推理的输出结果。执行完推理后，结果会被填充到这个 <code>Ort::Value</code> 的 vector 中。</li>
</ul>
</li>
</ol>
<h3 id="返回值-3"><a href="#返回值-3" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li><strong>类型</strong>：<code>Status</code></li>
<li><strong>描述</strong>：函数返回一个 <code>Status</code> 对象，表示推理过程的状态。通常在成功时返回 <code>Status::OK()</code>，如果失败，则返回相应的错误信息。</li>
</ul>
<h3 id="函数工作流程"><a href="#函数工作流程" class="headerlink" title="函数工作流程"></a><strong>函数工作流程</strong></h3><p><code>Session::Run()</code> 函数在 ONNX Runtime 中负责以下几个步骤：</p>
<ol>
<li><p><strong>输入检查</strong>：</p>
<ul>
<li>检查提供的输入数据是否与模型的输入要求（如名称、形状、数据类型）一致。</li>
</ul>
</li>
<li><p><strong>推理执行</strong>：</p>
<ul>
<li>使用提供的输入数据执行推理。ONNX Runtime 会根据模型的计算图进行推理操作。</li>
</ul>
</li>
<li><p><strong>输出生成</strong>：</p>
<ul>
<li>根据指定的输出名称，返回计算结果。输出的数据会被填充到 <code>output_values</code> 中，通常这些数据是 <code>Ort::Value</code> 对象，包含了推理结果（如分类的概率、图像的预测框等）。</li>
</ul>
</li>
</ol>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h3><p>以下是一个简单的使用 <code>Session::Run()</code> 函数的示例，它展示了如何加载一个 ONNX 模型并运行推理。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_library/provider_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/utils.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/common.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/session.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> std::string model_path = <span class="string">&quot;path_to_your_model.onnx&quot;</span>;</span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">onnx_session</span><span class="params">(env, model_path.c_str(), session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">const</span> <span class="type">char</span>*&gt; input_names = &#123;<span class="string">&quot;input_tensor_name&quot;</span>&#125;;  <span class="comment">// 假设模型有一个输入张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; input_values;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入 tensor（假设模型要求输入大小为 1x3x224x224）</span></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>;  <span class="comment">// 示例输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;  <span class="comment">// 输入张量的形状</span></span><br><span class="line">    input_values.<span class="built_in">push_back</span>(Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(input_data.<span class="built_in">data</span>(), input_shape));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置输出数据</span></span><br><span class="line">    std::vector&lt;<span class="type">const</span> <span class="type">char</span>*&gt; output_names = &#123;<span class="string">&quot;output_tensor_name&quot;</span>&#125;;  <span class="comment">// 假设模型有一个输出张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_values;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        onnx_session.<span class="built_in">Run</span>(Ort::<span class="built_in">RunOptions</span>(), input_names, input_values.<span class="built_in">data</span>(), input_names.<span class="built_in">size</span>(), output_names, output_values);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理推理结果</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; output : output_values) &#123;</span><br><span class="line">            <span class="comment">// 处理每个输出的结果</span></span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Output tensor: &quot;</span> &lt;&lt; output.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>()[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception&amp; e) &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Error during inference: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a><strong>解释</strong></h3><ol>
<li><strong>初始化环境</strong>：首先创建一个 <code>Ort::Env</code> 对象来初始化 ONNX Runtime 环境。</li>
<li><strong>加载模型</strong>：通过 <code>Ort::Session</code> 加载一个 ONNX 模型，并使用 <code>SessionOptions</code> 进行配置。</li>
<li><strong>准备输入数据</strong>：创建输入数据（如一张图像），并使用 <code>Ort::Value::CreateTensor</code> 方法将其转换为 <code>Ort::Value</code> 类型。</li>
<li><strong>执行推理</strong>：使用 <code>Run</code> 方法执行推理，传入输入数据和请求的输出数据名称。</li>
<li><strong>输出结果</strong>：推理结果被存储在 <code>output_values</code> 中，之后可以进一步处理或打印输出。</li>
</ol>
<h3 id="注意事项-6"><a href="#注意事项-6" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>输入和输出数据的类型一致性</strong>：</p>
<ul>
<li>需要确保输入数据的类型（如 <code>float</code>）与模型要求的数据类型一致。</li>
<li>输入数据的形状应与模型的输入形状匹配，否则推理会失败。</li>
</ul>
</li>
<li><p><strong>异常处理</strong>：</p>
<ul>
<li><code>Session::Run()</code> 可能会抛出异常（如数据类型不匹配、形状不匹配等），因此通常需要在调用时使用 <code>try-catch</code> 来捕获并处理异常。</li>
</ul>
</li>
<li><p><strong>性能优化</strong>：</p>
<ul>
<li><code>Session::Run()</code> 是一个同步操作，因此如果需要优化推理性能，可以考虑异步执行（如通过 <code>RunOptions</code> 或使用多线程）或使用硬件加速（如使用 GPU）。</li>
</ul>
</li>
</ol>
<h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>Session::Run()</code> 是 ONNX Runtime 推理流程中的核心函数，它负责将输入数据传递给模型并返回结果。理解和正确使用该函数对于模型推理至关重要。通过合适的参数配置（如输入输出的名称和数据类型），可以实现高效、正确的模型推理。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>感谢老板支持！敬礼(^^ゞ</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="zhang junyi WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="zhang junyi Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CppLibrary/" rel="tag"># CppLibrary</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/12/30/notebook/DeepLearning/OpenVINO/2024-12-30-OpenVino/" rel="prev" title="OpenVino">
      <i class="fa fa-chevron-left"></i> OpenVino
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/12/30/notebook/DeepLearning/2024-12-30-%E5%A4%84%E7%90%86%E5%99%A8%E6%9E%B6%E6%9E%84/" rel="next" title="处理器架构">
      处理器架构 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#onnxruntime%E5%BA%93-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">2.</span> <span class="nav-text">onnxruntime库 是什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9"><span class="nav-number">2.1.</span> <span class="nav-text">核心特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD"><span class="nav-number">2.2.</span> <span class="nav-text">主要功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">2.3.</span> <span class="nav-text">使用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">2.4.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ONNX-%E7%9A%84%E6%9D%A5%E6%BA%90"><span class="nav-number">2.5.</span> <span class="nav-text">ONNX 的来源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ONNX-Runtime-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">2.6.</span> <span class="nav-text">ONNX Runtime 的优势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#onnxruntime-C-%E6%8E%A5%E5%8F%A3-%E8%AF%A6%E8%A7%A3"><span class="nav-number">3.</span> <span class="nav-text">onnxruntime C++接口 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%B1%BB"><span class="nav-number">3.1.</span> <span class="nav-text">核心概念与类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%AD%A5%E9%AA%A4%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">关键步骤与方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="nav-number">3.3.</span> <span class="nav-text">完整示例代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E4%B8%8E%E6%89%A9%E5%B1%95"><span class="nav-number">3.4.</span> <span class="nav-text">优化与扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">3.5.</span> <span class="nav-text">常见问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-onnxruntime-Ort-Env-%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.</span> <span class="nav-text">C++ onnxruntime Ort::Env 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ort-Env-%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-number">4.1.</span> <span class="nav-text">Ort::Env 的功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ort-Env-%E7%9A%84%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-number">4.2.</span> <span class="nav-text">Ort::Env 的构造函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%AE%80%E5%8D%95%E6%9E%84%E9%80%A0"><span class="nav-number">4.2.1.</span> <span class="nav-text">1. 简单构造</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%AB%98%E7%BA%A7%E6%9E%84%E9%80%A0"><span class="nav-number">4.2.2.</span> <span class="nav-text">2. 高级构造</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95"><span class="nav-number">4.3.</span> <span class="nav-text">常见用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96-ONNX-Runtime-%E7%8E%AF%E5%A2%83"><span class="nav-number">4.3.1.</span> <span class="nav-text">初始化 ONNX Runtime 环境</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E5%90%88%E6%8E%A8%E7%90%86%E4%BC%9A%E8%AF%9D"><span class="nav-number">4.3.2.</span> <span class="nav-text">结合推理会话</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ort-Env-%E7%9A%84%E4%B8%BB%E8%A6%81%E6%8E%A5%E5%8F%A3"><span class="nav-number">4.4.</span> <span class="nav-text">Ort::Env 的主要接口</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB"><span class="nav-number">4.4.1.</span> <span class="nav-text">设置日志级别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%80%89%E9%A1%B9"><span class="nav-number">4.4.2.</span> <span class="nav-text">设置线程池选项</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">4.5.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.6.</span> <span class="nav-text">完整示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-SessionOptions-%E8%AF%A6%E8%A7%A3"><span class="nav-number">5.</span> <span class="nav-text">Ort::SessionOptions 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ort-SessionOptions-%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-number">5.1.</span> <span class="nav-text">Ort::SessionOptions 的功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">构造和基本用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%BB%98%E8%AE%A4%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-number">5.2.1.</span> <span class="nav-text">默认构造函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0"><span class="nav-number">5.3.</span> <span class="nav-text">主要成员函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%AE%BE%E7%BD%AE%E7%BA%BF%E7%A8%8B%E6%95%B0"><span class="nav-number">5.3.1.</span> <span class="nav-text">1. 设置线程数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SetIntraOpNumThreads"><span class="nav-number">5.3.1.1.</span> <span class="nav-text">SetIntraOpNumThreads</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SetInterOpNumThreads"><span class="nav-number">5.3.1.2.</span> <span class="nav-text">SetInterOpNumThreads</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%AE%BE%E7%BD%AE%E4%BC%98%E5%8C%96%E7%BA%A7%E5%88%AB"><span class="nav-number">5.3.2.</span> <span class="nav-text">2. 设置优化级别</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SetGraphOptimizationLevel"><span class="nav-number">5.3.2.1.</span> <span class="nav-text">SetGraphOptimizationLevel</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%85%8D%E7%BD%AE%E6%89%A7%E8%A1%8C%E6%8F%90%E4%BE%9B%E7%A8%8B%E5%BA%8F"><span class="nav-number">5.3.3.</span> <span class="nav-text">3. 配置执行提供程序</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CPU-%E6%8F%90%E4%BE%9B%E7%A8%8B%E5%BA%8F"><span class="nav-number">5.3.3.1.</span> <span class="nav-text">CPU 提供程序</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GPU-%E6%8F%90%E4%BE%9B%E7%A8%8B%E5%BA%8F%EF%BC%88CUDA%EF%BC%89"><span class="nav-number">5.3.3.2.</span> <span class="nav-text">GPU 提供程序（CUDA）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E9%85%8D%E7%BD%AE%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="nav-number">5.3.4.</span> <span class="nav-text">4. 配置内存分配</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#EnableMemPattern"><span class="nav-number">5.3.4.1.</span> <span class="nav-text">EnableMemPattern</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DisableMemPattern"><span class="nav-number">5.3.4.2.</span> <span class="nav-text">DisableMemPattern</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E4%BD%BF%E7%94%A8%E5%88%86%E9%85%8D%E5%99%A8"><span class="nav-number">5.3.5.</span> <span class="nav-text">5. 使用分配器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E5%88%86%E9%85%8D%E5%99%A8%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="nav-number">5.3.5.1.</span> <span class="nav-text">设置分配器（可选）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-%E6%89%93%E5%BC%80%E5%92%8C%E5%85%B3%E9%97%AD%E8%B0%83%E8%AF%95%E9%80%89%E9%A1%B9"><span class="nav-number">5.3.6.</span> <span class="nav-text">6. 打开和关闭调试选项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E7%94%A8%E8%B0%83%E8%AF%95"><span class="nav-number">5.3.6.1.</span> <span class="nav-text">启用调试</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A6%81%E7%94%A8%E8%B0%83%E8%AF%95"><span class="nav-number">5.3.6.2.</span> <span class="nav-text">禁用调试</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E7%A4%BA%E4%BE%8B"><span class="nav-number">5.4.</span> <span class="nav-text">综合示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-1"><span class="nav-number">5.5.</span> <span class="nav-text">注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-Session-%E8%AF%A6%E8%A7%A3"><span class="nav-number">6.</span> <span class="nav-text">Ort::Session 详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-Session-%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD"><span class="nav-number">7.</span> <span class="nav-text">Ort::Session 的主要功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95-1"><span class="nav-number">8.</span> <span class="nav-text">构造和基本用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-number">8.1.</span> <span class="nav-text">构造函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%BB%98%E8%AE%A4%E6%9E%84%E9%80%A0"><span class="nav-number">8.1.1.</span> <span class="nav-text">1. 默认构造</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0-1"><span class="nav-number">9.</span> <span class="nav-text">主要成员函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E4%BF%A1%E6%81%AF"><span class="nav-number">9.1.</span> <span class="nav-text">1. 获取模型输入输出信息</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GetInputCount-%E5%92%8C-GetOutputCount"><span class="nav-number">9.1.1.</span> <span class="nav-text">GetInputCount 和 GetOutputCount</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GetInputName-%E5%92%8C-GetOutputName"><span class="nav-number">9.1.2.</span> <span class="nav-text">GetInputName 和 GetOutputName</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GetInputTypeInfo-%E5%92%8C-GetOutputTypeInfo"><span class="nav-number">9.1.3.</span> <span class="nav-text">GetInputTypeInfo 和 GetOutputTypeInfo</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%8E%A8%E7%90%86%E6%93%8D%E4%BD%9C"><span class="nav-number">9.2.</span> <span class="nav-text">2. 推理操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Run"><span class="nav-number">9.2.1.</span> <span class="nav-text">Run</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%89%A7%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="nav-number">9.3.</span> <span class="nav-text">综合示例：执行推理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-2"><span class="nav-number">10.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-AllocatorWithDefaultOptions-%E8%AF%A6%E8%A7%A3"><span class="nav-number">11.</span> <span class="nav-text">Ort::AllocatorWithDefaultOptions 详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-AllocatorWithDefaultOptions-%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD"><span class="nav-number">12.</span> <span class="nav-text">Ort::AllocatorWithDefaultOptions 的主要功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E6%B3%95%E5%92%8C%E6%8E%A5%E5%8F%A3"><span class="nav-number">13.</span> <span class="nav-text">用法和接口</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-number">13.1.</span> <span class="nav-text">1. 构造函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">13.2.</span> <span class="nav-text">2. 常用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#operator-OrtAllocator"><span class="nav-number">13.2.1.</span> <span class="nav-text">operator OrtAllocator*</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8A%9F%E8%83%BD"><span class="nav-number">13.2.2.</span> <span class="nav-text">内存管理功能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF-1"><span class="nav-number">14.</span> <span class="nav-text">使用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%90%8D%E7%A7%B0"><span class="nav-number">14.1.</span> <span class="nav-text">1. 获取输入输出名称</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A"><span class="nav-number">14.1.1.</span> <span class="nav-text">示例：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%B8%8E%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D%E7%9B%B8%E5%85%B3%E7%9A%84-API"><span class="nav-number">14.2.</span> <span class="nav-text">2. 与动态分配相关的 API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%B1%BB%E5%9E%8B%E4%BF%A1%E6%81%AF"><span class="nav-number">14.2.1.</span> <span class="nav-text">获取输入输出类型信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E5%88%86%E9%85%8D%E5%99%A8%E6%8C%87%E9%92%88"><span class="nav-number">14.2.2.</span> <span class="nav-text">获取分配器指针</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E9%83%A8%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">15.</span> <span class="nav-text">内部工作原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-3"><span class="nav-number">16.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B%EF%BC%9A%E4%BB%8E%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%8F%90%E5%8F%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E4%BF%A1%E6%81%AF"><span class="nav-number">17.</span> <span class="nav-text">完整示例：从模型中提取输入输出信息</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">17.1.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-GetInputTypeInfo-%E8%AF%A6%E8%A7%A3"><span class="nav-number">18.</span> <span class="nav-text">Session::GetInputTypeInfo() 详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B"><span class="nav-number">19.</span> <span class="nav-text">函数原型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0"><span class="nav-number">19.1.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC"><span class="nav-number">19.2.</span> <span class="nav-text">返回值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-TypeInfo-%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-number">20.</span> <span class="nav-text">Ort::TypeInfo 的功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">21.</span> <span class="nav-text">使用方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-TensorTypeAndShapeInfo-%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-number">22.</span> <span class="nav-text">Ort::TensorTypeAndShapeInfo 的功能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">22.1.</span> <span class="nav-text">1. 获取张量数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%8E%B7%E5%8F%96%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6"><span class="nav-number">22.2.</span> <span class="nav-text">2. 获取张量形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%8E%B7%E5%8F%96%E5%85%83%E7%B4%A0%E6%80%BB%E6%95%B0"><span class="nav-number">22.3.</span> <span class="nav-text">3. 获取元素总数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E7%A4%BA%E4%BE%8B"><span class="nav-number">23.</span> <span class="nav-text">输出示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-4"><span class="nav-number">24.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E5%B0%8F%E7%BB%93"><span class="nav-number">25.</span> <span class="nav-text">综合小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorTypeAndShapeInfo-GetShape-%E8%AF%A6%E8%A7%A3"><span class="nav-number">26.</span> <span class="nav-text">TensorTypeAndShapeInfo::GetShape() 详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B-1"><span class="nav-number">27.</span> <span class="nav-text">函数原型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0-1"><span class="nav-number">27.1.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-1"><span class="nav-number">27.2.</span> <span class="nav-text">返回值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD%E5%92%8C%E7%94%A8%E9%80%94"><span class="nav-number">28.</span> <span class="nav-text">功能和用途</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-number">29.</span> <span class="nav-text">使用示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%9A%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5%E5%BD%A2%E7%8A%B6"><span class="nav-number">29.1.</span> <span class="nav-text">示例代码：获取输入形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">29.2.</span> <span class="nav-text">输出示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GetShape-%E7%9A%84%E8%BF%94%E5%9B%9E%E5%80%BC%E8%A7%A3%E6%9E%90"><span class="nav-number">30.</span> <span class="nav-text">GetShape() 的返回值解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%9D%99%E6%80%81%E7%BB%B4%E5%BA%A6"><span class="nav-number">30.1.</span> <span class="nav-text">1. 静态维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%8A%A8%E6%80%81%E7%BB%B4%E5%BA%A6"><span class="nav-number">30.2.</span> <span class="nav-text">2. 动态维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%97%A0%E6%95%88%E7%BB%B4%E5%BA%A6"><span class="nav-number">30.3.</span> <span class="nav-text">3. 无效维度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%92%8C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">31.</span> <span class="nav-text">常见问题和注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">32.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-224-224-%E8%A1%A8%E7%A4%BA%E7%9A%84%E4%BB%80%E4%B9%88"><span class="nav-number">33.</span> <span class="nav-text">[1, 3, 224, 224] 表示的什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-224-224-%E8%A7%A3%E9%87%8A"><span class="nav-number">33.1.</span> <span class="nav-text">[1, 3, 224, 224] 解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E8%A7%A3%E9%87%8A"><span class="nav-number">33.2.</span> <span class="nav-text">综合解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF-2"><span class="nav-number">33.3.</span> <span class="nav-text">使用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">33.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F"><span class="nav-number">34.</span> <span class="nav-text">什么是图像张量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="nav-number">35.</span> <span class="nav-text">图像张量的基本结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F"><span class="nav-number">35.1.</span> <span class="nav-text">1. 彩色图像张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F"><span class="nav-number">35.2.</span> <span class="nav-text">2. 灰度图像张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%89%B9%E9%87%8F%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F"><span class="nav-number">35.3.</span> <span class="nav-text">3. 批量图像张量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="nav-number">36.</span> <span class="nav-text">图像张量的存储格式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F%E7%9A%84%E7%94%A8%E9%80%94"><span class="nav-number">37.</span> <span class="nav-text">图像张量的用途</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%BC%A0%E9%87%8F%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="nav-number">38.</span> <span class="nav-text">图像张量的示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-3"><span class="nav-number">39.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-onnxruntime-Ort-Value-%E8%AF%A6%E8%A7%A3"><span class="nav-number">40.</span> <span class="nav-text">C++ onnxruntime Ort::Value 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ort-Value-%E4%BB%8B%E7%BB%8D"><span class="nav-number">40.1.</span> <span class="nav-text">Ort::Value 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD-1"><span class="nav-number">40.2.</span> <span class="nav-text">主要功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Ort-Value"><span class="nav-number">40.3.</span> <span class="nav-text">创建 Ort::Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">40.4.</span> <span class="nav-text">常用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">40.4.1.</span> <span class="nav-text">1. 获取数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%8E%B7%E5%8F%96%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6"><span class="nav-number">40.4.2.</span> <span class="nav-text">2. 获取张量的形状</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E6%8C%87%E9%92%88"><span class="nav-number">40.4.3.</span> <span class="nav-text">3. 获取数据指针</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E8%AE%BE%E7%BD%AE%E5%BC%A0%E9%87%8F%E7%9A%84%E5%80%BC"><span class="nav-number">40.4.4.</span> <span class="nav-text">4. 设置张量的值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%85%B6%E4%BB%96%E7%B1%BB%E5%9E%8B"><span class="nav-number">40.4.5.</span> <span class="nav-text">5. 转换为其他类型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%9A%E6%8E%A8%E7%90%86%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA"><span class="nav-number">40.5.</span> <span class="nav-text">示例代码：推理输入和输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-4"><span class="nav-number">40.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ort-Value-CreateTensor-%E8%AF%A6%E8%A7%A3"><span class="nav-number">41.</span> <span class="nav-text">Ort::Value::CreateTensor 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B-2"><span class="nav-number">41.1.</span> <span class="nav-text">函数原型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3"><span class="nav-number">41.2.</span> <span class="nav-text">参数详解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-2"><span class="nav-number">41.3.</span> <span class="nav-text">返回值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="nav-number">41.4.</span> <span class="nav-text">示例代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A"><span class="nav-number">41.5.</span> <span class="nav-text">详细解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-5"><span class="nav-number">41.6.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-5"><span class="nav-number">41.7.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-Run-%E5%87%BD%E6%95%B0-%E8%AF%A6%E8%A7%A3"><span class="nav-number">42.</span> <span class="nav-text">Session::Run() 函数 详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="nav-number">42.1.</span> <span class="nav-text">函数定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">42.2.</span> <span class="nav-text">参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-3"><span class="nav-number">42.3.</span> <span class="nav-text">返回值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-number">42.4.</span> <span class="nav-text">函数工作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-1"><span class="nav-number">42.5.</span> <span class="nav-text">示例代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A"><span class="nav-number">42.6.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-6"><span class="nav-number">42.7.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-6"><span class="nav-number">42.8.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhang junyi</p>
  <div class="site-description" itemprop="description">工作学习笔记</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">672</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">98</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/junyiha" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;junyiha" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/zhangjunyiha" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;zhangjunyiha" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhang junyi</span>
</div>

<!--
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>
-->

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
