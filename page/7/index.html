<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"junyiha.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="工作学习笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="junyi&#39;s blog">
<meta property="og:url" content="https://junyiha.github.io/page/7/index.html">
<meta property="og:site_name" content="junyi&#39;s blog">
<meta property="og:description" content="工作学习笔记">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zhang junyi">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://junyiha.github.io/page/7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>junyi's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">junyi's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">hahahahaha</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/13/notebook/Qt/%E5%B8%B8%E8%A7%81%E7%B1%BB/2025-01-13-14-QTextEdit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/13/notebook/Qt/%E5%B8%B8%E8%A7%81%E7%B1%BB/2025-01-13-14-QTextEdit/" class="post-title-link" itemprop="url">QTextEdit</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-13 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-13T09:00:00+08:00">2025-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Qt/" itemprop="url" rel="index"><span itemprop="name">Qt</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>QTextEdit 类 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/13/notebook/Qt/%E5%B8%B8%E8%A7%81%E7%B1%BB/2025-01-13-14-QTextEdit/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/11/notebook/Python/python_3_%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/2025-01-11-nmap/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/11/notebook/Python/python_3_%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/2025-01-11-nmap/" class="post-title-link" itemprop="url">nmap模块</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-11 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-11T09:00:00+08:00">2025-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>python中的nmap模块</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/11/notebook/Python/python_3_%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/2025-01-11-nmap/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/10/notebook/Books/2025-01-10-C++%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/10/notebook/Books/2025-01-10-C++%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">C++并发编程实战</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-10 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-10T09:00:00+08:00">2025-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Books/" itemprop="url" rel="index"><span itemprop="name">Books</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>C++并发编程实战书籍的阅读，学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/10/notebook/Books/2025-01-10-C++%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/06/notebook/Compiler/2025-01-06-ninja/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/notebook/Compiler/2025-01-06-ninja/" class="post-title-link" itemprop="url">ninja</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-06 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-06T09:00:00+08:00">2025-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Compiler/" itemprop="url" rel="index"><span itemprop="name">Compiler</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ninja 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/06/notebook/Compiler/2025-01-06-ninja/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/Compiler/2025-01-05-%E5%B8%B8%E8%A7%81%E7%BC%96%E8%AF%91%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/Compiler/2025-01-05-%E5%B8%B8%E8%A7%81%E7%BC%96%E8%AF%91%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">常见编译问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Compiler/" itemprop="url" rel="index"><span itemprop="name">Compiler</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="windows下C-项目-监控编译速度慢的问题的工具有哪些"><a href="#windows下C-项目-监控编译速度慢的问题的工具有哪些" class="headerlink" title="windows下C++项目 监控编译速度慢的问题的工具有哪些"></a>windows下C++项目 监控编译速度慢的问题的工具有哪些</h2><p>在Windows下C++项目中，如果遇到编译速度较慢的问题，可以使用以下工具来监控和分析问题，帮助优化编译速度：</p>
<hr>
<h3 id="1-编译时间分析工具"><a href="#1-编译时间分析工具" class="headerlink" title="1. 编译时间分析工具"></a>1. <strong>编译时间分析工具</strong></h3><ul>
<li><p><strong>Clang Build Analyzer</strong><br>Clang 提供的工具，可以帮助分析编译时间瓶颈。虽然它主要针对Clang编译器，但如果项目使用Clang编译，可以直接使用此工具。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://llvm.org/docs/ClangTools.html#clang-build-analyzer">Clang Build Analyzer</a></li>
<li><strong>用途</strong>: 分析哪些头文件或代码模块导致了编译速度慢。</li>
</ul>
</li>
<li><p><strong>MSBuild Binary and Structured Log Viewer</strong><br>如果你的项目使用MSBuild构建，可以通过MSBuild日志工具查看具体的构建时间段。</p>
<ul>
<li><strong>下载</strong>: <a target="_blank" rel="noopener" href="https://github.com/KirillOsenkov/MSBuildStructuredLog">MSBuild Log Viewer</a></li>
<li><strong>用途</strong>: 生成并可视化MSBuild日志，快速定位长时间任务。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-依赖分析工具"><a href="#2-依赖分析工具" class="headerlink" title="2. 依赖分析工具"></a>2. <strong>依赖分析工具</strong></h3><ul>
<li><p><strong>Include What You Use (IWYU)</strong><br>用于分析C++项目中头文件的依赖情况，减少不必要的头文件包含。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://include-what-you-use.org/">IWYU</a></li>
<li><strong>用途</strong>: 帮助减少头文件依赖，降低编译时间。</li>
</ul>
</li>
<li><p><strong>Visual Studio 编译依赖分析器</strong><br>如果使用Visual Studio，可以通过“项目属性”中的编译依赖设置来分析和优化头文件的使用。</p>
<ul>
<li><strong>用途</strong>: 检测包含文件树，分析冗余包含。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-构建性能优化工具"><a href="#3-构建性能优化工具" class="headerlink" title="3. 构建性能优化工具"></a>3. <strong>构建性能优化工具</strong></h3><ul>
<li><p><strong>CMake Timing Tools</strong><br>如果使用CMake作为构建系统，可以通过 <code>--trace</code> 和 <code>--trace-expand</code> 参数分析配置时间。还可以开启构建性能统计（例如 <code>Ninja</code> 提供的构建时间统计）。</p>
<ul>
<li><strong>用途</strong>: 分析构建过程中每一步的耗时。</li>
</ul>
</li>
<li><p><strong>IncrediBuild</strong><br>专为Windows环境设计的分布式构建工具，可大幅加速编译过程。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://www.incredibuild.com/">IncrediBuild</a></li>
<li><strong>用途</strong>: 通过分布式任务并行化提升编译速度。</li>
</ul>
</li>
<li><p><strong>ccache</strong><br>高效的编译缓存工具，用于加速重复编译。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://ccache.dev/">ccache</a></li>
<li><strong>用途</strong>: 缓存已编译文件，避免重复编译。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-性能监控工具"><a href="#4-性能监控工具" class="headerlink" title="4. 性能监控工具"></a>4. <strong>性能监控工具</strong></h3><ul>
<li><p><strong>Windows Performance Analyzer (WPA)</strong><br>使用Windows自带的性能分析工具监控磁盘、CPU和内存等资源使用情况，定位瓶颈是否由系统资源导致。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/windows-hardware/test/wpt/">Windows Performance Toolkit</a></li>
</ul>
</li>
<li><p><strong>Process Monitor (ProcMon)</strong><br>监控构建过程中文件系统和进程的读写操作，定位慢点是否由磁盘I&#x2F;O引起。</p>
<ul>
<li><strong>下载</strong>: <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/sysinternals/downloads/procmon">ProcMon</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-其他辅助工具"><a href="#5-其他辅助工具" class="headerlink" title="5. 其他辅助工具"></a>5. <strong>其他辅助工具</strong></h3><ul>
<li><p><strong>Precompiled Headers (PCH)</strong><br>利用预编译头减少重复的头文件解析时间。</p>
</li>
<li><p><strong>Ninja</strong><br>替代传统的构建工具（如MSBuild或Make），以更快的依赖管理和任务调度提升编译速度。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://ninja-build.org/">Ninja</a></li>
</ul>
</li>
<li><p><strong>Clang-Tidy</strong><br>通过静态分析工具检查代码质量，识别可能导致编译变慢的代码模式。</p>
<ul>
<li><strong>官网</strong>: <a target="_blank" rel="noopener" href="https://clang.llvm.org/extra/clang-tidy/">Clang-Tidy</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>可以从头文件优化、构建工具、分布式构建和编译缓存等方面入手。推荐先使用<code>IWYU</code>和<code>Clang Build Analyzer</code>分析依赖，再结合<code>ccache</code>和<code>Ninja</code>等工具优化构建流程。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/Compiler/2025-01-05-%E7%BC%96%E8%AF%91%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/Compiler/2025-01-05-%E7%BC%96%E8%AF%91%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">编译基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Compiler/" itemprop="url" rel="index"><span itemprop="name">Compiler</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>编译相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/Compiler/2025-01-05-%E7%BC%96%E8%AF%91%E5%9F%BA%E7%A1%80/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-CompiledModel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-CompiledModel/" class="post-title-link" itemprop="url">ov::CompiledModel</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ov::CompiledModel 类 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-CompiledModel/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Output/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Output/" class="post-title-link" itemprop="url">ov::Output</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="ov-Output-详解"><a href="#ov-Output-详解" class="headerlink" title="ov::Output 详解"></a>ov::Output 详解</h2><p><code>ov::Output</code> 是 OpenVINO 中的一个核心类，用于表示模型图中节点的输出。它封装了一个节点的输出端口，允许用户访问输出的形状、数据类型以及与其他节点的连接信息。通过 <code>ov::Output</code>，可以在计算图中导航并操作节点间的连接。</p>
<hr>
<h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a><strong>核心功能</strong></h2><ol>
<li><p><strong>表示节点的输出端口</strong><br>每个 <code>ov::Node</code> 可能有一个或多个输出端口，<code>ov::Output</code> 表示其中一个端口。</p>
</li>
<li><p><strong>获取元信息</strong><br>包括输出的形状、数据类型和元素类型等。</p>
</li>
<li><p><strong>管理连接关系</strong><br>用于查看输出与哪些节点的输入相连接。</p>
</li>
<li><p><strong>支持模板化</strong><br><code>ov::Output&lt;T&gt;</code> 可以作用于不同类型的节点，例如 <code>ov::Node</code> 或 <code>ov::Function</code>。</p>
</li>
</ol>
<hr>
<h2 id="创建和使用-ov-Output"><a href="#创建和使用-ov-Output" class="headerlink" title="创建和使用 ov::Output"></a><strong>创建和使用 <code>ov::Output</code></strong></h2><p>通常，<code>ov::Output</code> 是从 <code>ov::Node</code> 的 <code>outputs()</code> 方法或 <code>output(i)</code> 方法获取的，而不是直接创建。</p>
<h3 id="获取节点的输出"><a href="#获取节点的输出" class="headerlink" title="获取节点的输出"></a><strong>获取节点的输出</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取节点的第一个输出</span></span><br><span class="line">ov::Output&lt;ov::Node&gt; output = node-&gt;<span class="built_in">output</span>(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="主要方法和属性"><a href="#主要方法和属性" class="headerlink" title="主要方法和属性"></a><strong>主要方法和属性</strong></h2><p>以下是 <code>ov::Output</code> 的常用方法及其功能：</p>
<h3 id="1-获取输出形状"><a href="#1-获取输出形状" class="headerlink" title="1. 获取输出形状"></a><strong>1. 获取输出形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ov::Shape shape = output.<span class="built_in">get_shape</span>();</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Shape: &quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; dim : shape) &#123;</span><br><span class="line">    std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">std::cout &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回输出的静态形状（<code>ov::Shape</code>）。</li>
</ul>
<h3 id="2-获取数据类型"><a href="#2-获取数据类型" class="headerlink" title="2. 获取数据类型"></a><strong>2. 获取数据类型</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ov::element::Type type = output.<span class="built_in">get_element_type</span>();</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Data type: &quot;</span> &lt;&lt; type &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回输出的数据类型（例如 <code>f32</code>、<code>i32</code> 等）。</li>
</ul>
<h3 id="3-检查输出是否为动态形状"><a href="#3-检查输出是否为动态形状" class="headerlink" title="3. 检查输出是否为动态形状"></a><strong>3. 检查输出是否为动态形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (output.<span class="built_in">get_partial_shape</span>().<span class="built_in">is_dynamic</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output has a dynamic shape.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用 <code>get_partial_shape()</code> 检查是否为动态形状。</li>
</ul>
<h3 id="4-获取连接的输入"><a href="#4-获取连接的输入" class="headerlink" title="4. 获取连接的输入"></a><strong>4. 获取连接的输入</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; target_input : output.<span class="built_in">get_target_inputs</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Connected to input of node: &quot;</span> </span><br><span class="line">              &lt;&lt; target_input.<span class="built_in">get_node</span>()-&gt;<span class="built_in">get_friendly_name</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>get_target_inputs()</code> 返回所有连接到该输出的输入端口。</li>
</ul>
<h3 id="5-获取输出所属的节点"><a href="#5-获取输出所属的节点" class="headerlink" title="5. 获取输出所属的节点"></a><strong>5. 获取输出所属的节点</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> owner_node = output.<span class="built_in">get_node</span>();</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Output belongs to node: &quot;</span> </span><br><span class="line">          &lt;&lt; owner_node-&gt;<span class="built_in">get_friendly_name</span>() &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>get_node()</code> 返回该输出所属的节点。</li>
</ul>
<h3 id="6-设置输出名称"><a href="#6-设置输出名称" class="headerlink" title="6. 设置输出名称"></a><strong>6. 设置输出名称</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output.<span class="built_in">set_names</span>(&#123;<span class="string">&quot;custom_output_name&quot;</span>&#125;);</span><br></pre></td></tr></table></figure>
<ul>
<li>设置输出的自定义名称，便于识别和访问。</li>
</ul>
<hr>
<h2 id="动态修改模型图"><a href="#动态修改模型图" class="headerlink" title="动态修改模型图"></a><strong>动态修改模型图</strong></h2><p>通过 <code>ov::Output</code>，可以修改计算图中节点的连接关系。</p>
<h3 id="1-替换输出连接"><a href="#1-替换输出连接" class="headerlink" title="1. 替换输出连接"></a><strong>1. 替换输出连接</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ov::Output&lt;ov::Node&gt; new_output = some_other_node-&gt;<span class="built_in">output</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 替换当前输出与新输出的连接</span></span><br><span class="line">ov::<span class="built_in">replace_output</span>(output, new_output);</span><br></pre></td></tr></table></figure>

<h3 id="2-添加新连接"><a href="#2-添加新连接" class="headerlink" title="2. 添加新连接"></a><strong>2. 添加新连接</strong></h3><p>可以将一个输出连接到多个输入节点：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output.<span class="built_in">add_target_input</span>(new_node-&gt;<span class="built_in">input</span>(<span class="number">0</span>));</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实际应用示例"><a href="#实际应用示例" class="headerlink" title="实际应用示例"></a><strong>实际应用示例</strong></h2><p>以下示例展示了如何遍历模型节点的输出，并动态修改模型：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;openvino/openvino.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ov::Core core;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="keyword">auto</span> model = core.<span class="built_in">read_model</span>(<span class="string">&quot;model.xml&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历模型中的所有节点</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; node : model-&gt;<span class="built_in">get_ops</span>()) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; node-&gt;<span class="built_in">get_output_size</span>(); ++i) &#123;</span><br><span class="line">            ov::Output&lt;ov::Node&gt; output = node-&gt;<span class="built_in">output</span>(i);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出节点信息</span></span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Node: &quot;</span> &lt;&lt; node-&gt;<span class="built_in">get_friendly_name</span>()</span><br><span class="line">                      &lt;&lt; <span class="string">&quot;, Output index: &quot;</span> &lt;&lt; i</span><br><span class="line">                      &lt;&lt; <span class="string">&quot;, Shape: &quot;</span> &lt;&lt; output.<span class="built_in">get_shape</span>()</span><br><span class="line">                      &lt;&lt; <span class="string">&quot;, Type: &quot;</span> &lt;&lt; output.<span class="built_in">get_element_type</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 修改连接（例如替换输出连接）</span></span><br><span class="line">            <span class="keyword">if</span> (output.<span class="built_in">get_element_type</span>() == ov::element::f32) &#123;</span><br><span class="line">                <span class="keyword">auto</span> new_node = std::<span class="built_in">make_shared</span>&lt;ov::op::v0::Relu&gt;(output);</span><br><span class="line">                ov::<span class="built_in">replace_output</span>(output, new_node-&gt;<span class="built_in">output</span>(<span class="number">0</span>));</span><br><span class="line">                std::cout &lt;&lt; <span class="string">&quot;Replaced output with a ReLU node.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存修改后的模型</span></span><br><span class="line">    core.<span class="built_in">compile_model</span>(model, <span class="string">&quot;CPU&quot;</span>)-&gt;<span class="built_in">export_model</span>(<span class="string">&quot;modified_model.xml&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="高级功能"><a href="#高级功能" class="headerlink" title="高级功能"></a><strong>高级功能</strong></h2><h3 id="使用动态形状的输出"><a href="#使用动态形状的输出" class="headerlink" title="使用动态形状的输出"></a><strong>使用动态形状的输出</strong></h3><p>对于动态形状的输出，可以进一步设置具体形状：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (output.<span class="built_in">get_partial_shape</span>().<span class="built_in">is_dynamic</span>()) &#123;</span><br><span class="line">    output.<span class="built_in">set_partial_shape</span>(&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="获取扩展的形状信息"><a href="#获取扩展的形状信息" class="headerlink" title="获取扩展的形状信息"></a><strong>获取扩展的形状信息</strong></h3><p>获取动态形状的约束：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ov::PartialShape partial_shape = output.<span class="built_in">get_partial_shape</span>();</span><br><span class="line"><span class="keyword">if</span> (partial_shape.<span class="built_in">rank</span>().<span class="built_in">is_dynamic</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Rank is dynamic.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Rank: &quot;</span> &lt;&lt; partial_shape.<span class="built_in">rank</span>().<span class="built_in">get_length</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="性能注意事项"><a href="#性能注意事项" class="headerlink" title="性能注意事项"></a><strong>性能注意事项</strong></h2><ol>
<li><p><strong>批量操作</strong><br>尽量减少对输出的逐个操作，使用批量方法操作多个节点或输出。</p>
</li>
<li><p><strong>减少拷贝</strong><br>使用引用或共享机制避免不必要的输出数据拷贝。</p>
</li>
<li><p><strong>动态形状处理</strong><br>在动态模型中，尽量在推理开始前明确所有动态形状。</p>
</li>
</ol>
<hr>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a><strong>常见问题</strong></h2><ol>
<li><p><strong>动态形状的操作限制</strong><br>对动态形状的输出操作时，需要先设置具体形状，否则部分操作可能失败。</p>
</li>
<li><p><strong>数据类型不匹配</strong><br>在连接节点时，确保输出和输入的数据类型一致。</p>
</li>
<li><p><strong>输出未连接</strong><br>如果某些输出未连接，可能导致模型推理失败或结果异常。</p>
</li>
</ol>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p><code>ov::Output</code> 是 OpenVINO 图操作中的关键类，用于表示和管理节点的输出端口。通过 <code>ov::Output</code>，开发者可以轻松获取输出的元信息，管理节点间的连接关系，并动态修改计算图。熟练掌握 <code>ov::Output</code> 的功能，可以大大提升在 OpenVINO 图操作中的开发效率。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Node/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Node/" class="post-title-link" itemprop="url">ov::Node</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ov::Node 类 学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Node/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-InferRequest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-InferRequest/" class="post-title-link" itemprop="url">ov::InferRequest</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ov::InferRequest 类 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-InferRequest/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Tensor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Tensor/" class="post-title-link" itemprop="url">ov::Tensor</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>Tensor 类 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Tensor/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Shape/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Shape/" class="post-title-link" itemprop="url">ov::Shape</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ov::Shape 类 学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-Shape/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-5-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-5-Core/" class="post-title-link" itemprop="url">ov::Core</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>ov::Core 类 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-5-Core/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/" class="post-title-link" itemprop="url">常见编程技巧</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 09:00:00" itemprop="dateCreated datePublished" datetime="2025-01-05T09:00:00+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>OpenVINO 常见编程技巧</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/05/notebook/DeepLearning/OpenVINO/2025-01-05-%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/31/notebook/DeepLearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/2024-12-31-%E5%89%8D%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/31/notebook/DeepLearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/2024-12-31-%E5%89%8D%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">前处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-31 09:00:00" itemprop="dateCreated datePublished" datetime="2024-12-31T09:00:00+08:00">2024-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>模型前处理相关学习笔记</li>
</ul>
<h2 id="深度学习模型部署-前处理"><a href="#深度学习模型部署-前处理" class="headerlink" title="深度学习模型部署 前处理"></a>深度学习模型部署 前处理</h2><p>在深度学习模型的部署过程中，前处理是一个至关重要的步骤。前处理的目标是将原始数据转换为模型可以接受的格式，以确保推理结果的准确性和一致性。以下是前处理的一些常见操作及其实现方法。</p>
<hr>
<h3 id="前处理的主要步骤"><a href="#前处理的主要步骤" class="headerlink" title="前处理的主要步骤"></a>前处理的主要步骤</h3><h4 id="1-数据加载"><a href="#1-数据加载" class="headerlink" title="1. 数据加载"></a>1. <strong>数据加载</strong></h4><ul>
<li>从文件、摄像头、传感器、API 等读取数据。</li>
<li>支持的格式可能包括图像（JPEG&#x2F;PNG）、文本、视频、音频等。</li>
<li>常用库：<ul>
<li>图像：<code>OpenCV</code>、<code>Pillow</code></li>
<li>文本：<code>NLTK</code>、<code>spaCy</code></li>
<li>音频：<code>librosa</code>、<code>torchaudio</code></li>
<li>视频：<code>OpenCV</code>、<code>moviepy</code></li>
</ul>
</li>
</ul>
<h4 id="2-尺寸调整-Resizing"><a href="#2-尺寸调整-Resizing" class="headerlink" title="2. 尺寸调整 (Resizing)"></a>2. <strong>尺寸调整 (Resizing)</strong></h4><ul>
<li>模型通常需要固定大小的输入，例如 224x224。</li>
<li>工具：<ul>
<li><strong>Python</strong>:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">resized = cv2.resize(image, (<span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure></li>
<li><strong>C++</strong> (OpenCV):<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">resize</span>(image, resized_image, cv::<span class="built_in">Size</span>(<span class="number">224</span>, <span class="number">224</span>));</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="3-归一化-Normalization"><a href="#3-归一化-Normalization" class="headerlink" title="3. 归一化 (Normalization)"></a>3. <strong>归一化 (Normalization)</strong></h4><ul>
<li>将输入数据的像素值范围调整到 <code>[0, 1]</code> 或 <code>[-1, 1]</code>。</li>
<li>公式：<br>[<br>x_{\text{normalized}} &#x3D; \frac{x - \text{mean}}{\text{std}}<br>]</li>
<li>常用库实现：<ul>
<li>PyTorch：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line">transform = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">normalized_image = transform(image)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="4-数据格式转换"><a href="#4-数据格式转换" class="headerlink" title="4. 数据格式转换"></a>4. <strong>数据格式转换</strong></h4><ul>
<li>深度学习模型可能需要特定的数据格式（例如 NHWC 或 NCHW）。<ul>
<li><strong>NHWC</strong>: <code>[Batch, Height, Width, Channels]</code>（TensorFlow 通常使用）</li>
<li><strong>NCHW</strong>: <code>[Batch, Channels, Height, Width]</code>（PyTorch 通常使用）</li>
</ul>
</li>
<li>转换：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">nhwc_image = image.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; NHWC</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="5-类型转换"><a href="#5-类型转换" class="headerlink" title="5. 类型转换"></a>5. <strong>类型转换</strong></h4><ul>
<li>确保数据的类型与模型要求一致，例如 <code>float32</code>、<code>int8</code> 等。</li>
<li>转换：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float_image = image.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="6-数据归一化与标准化的结合"><a href="#6-数据归一化与标准化的结合" class="headerlink" title="6. 数据归一化与标准化的结合"></a>6. <strong>数据归一化与标准化的结合</strong></h4><ul>
<li>对于图像：<ul>
<li>首先缩放到 <code>[0, 1]</code> 范围。</li>
<li>再进行归一化（减去均值，除以标准差）。</li>
</ul>
</li>
<li>对于文本或序列数据：<ul>
<li>可能需要 Tokenization 和 Padding。</li>
</ul>
</li>
</ul>
<h4 id="7-其他特定处理"><a href="#7-其他特定处理" class="headerlink" title="7. 其他特定处理"></a>7. <strong>其他特定处理</strong></h4><ul>
<li>数据增强：裁剪、旋转、翻转（在推理时一般用于 TTA）。</li>
<li>嵌入：对文本进行词嵌入转换。</li>
<li>频谱图：将音频信号转换为频谱图。</li>
</ul>
<hr>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><p>以下是一个典型的图像模型的前处理示例，使用 Python：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_image</span>(<span class="params">image_path, input_size=(<span class="params"><span class="number">224</span>, <span class="number">224</span></span>)</span>):</span><br><span class="line">    <span class="comment"># 加载图像</span></span><br><span class="line">    image = cv2.imread(image_path)</span><br><span class="line">    <span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Image not found or unable to read!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调整大小</span></span><br><span class="line">    resized = cv2.resize(image, input_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为浮点数并归一化到 [0, 1]</span></span><br><span class="line">    normalized = resized / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为 NCHW 格式</span></span><br><span class="line">    nchw_image = np.transpose(normalized, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加 Batch 维度并转换为 float32</span></span><br><span class="line">    batch_image = np.expand_dims(nchw_image, axis=<span class="number">0</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_image</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="部署中的前处理工具"><a href="#部署中的前处理工具" class="headerlink" title="部署中的前处理工具"></a>部署中的前处理工具</h3><ol>
<li><p><strong>TensorFlow&#x2F;TF-Serving</strong></p>
<ul>
<li>使用 <code>tf.image</code> 模块进行前处理。</li>
<li>可将前处理代码直接集成到模型中（SavedModel）。</li>
</ul>
</li>
<li><p><strong>PyTorch&#x2F;TorchServe</strong></p>
<ul>
<li>使用 <code>torchvision.transforms</code> 执行前处理。</li>
<li>在自定义 <code>Handler</code> 中实现前处理。</li>
</ul>
</li>
<li><p><strong>ONNX Runtime</strong></p>
<ul>
<li>在推理时依赖外部前处理，但支持优化后的动态尺寸输入。</li>
</ul>
</li>
<li><p><strong>OpenCV&#x2F;DNN 模块</strong></p>
<ul>
<li>OpenCV 支持直接加载和推理模型，可在前处理阶段使用其高效工具。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="前处理常见问题与优化"><a href="#前处理常见问题与优化" class="headerlink" title="前处理常见问题与优化"></a>前处理常见问题与优化</h3><ol>
<li><p><strong>不匹配问题</strong></p>
<ul>
<li>原因：模型训练和推理时的前处理设置不同。</li>
<li>解决：确保推理阶段与训练一致（归一化参数、数据格式等）。</li>
</ul>
</li>
<li><p><strong>性能问题</strong></p>
<ul>
<li>原因：前处理耗时过长。</li>
<li>解决：<ul>
<li>使用并行库（如 DALI）。</li>
<li>在 GPU&#x2F;TPU 上完成前处理。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>动态尺寸</strong></p>
<ul>
<li>原因：输入尺寸不固定。</li>
<li>解决：使用动态尺寸模型，或在前处理阶段适配输入。</li>
</ul>
</li>
</ol>
<p>如果你有具体的部署场景（如平台或数据类型），我可以提供更细致的实现建议！</p>
<h2 id="前处理-数据加载"><a href="#前处理-数据加载" class="headerlink" title="前处理 数据加载"></a>前处理 数据加载</h2><p>在深度学习模型部署中，<strong>前处理</strong>的目的是将原始输入数据转换为模型可以接受的格式和结构。数据加载是前处理的重要组成部分，涉及从存储中读取数据、对数据进行解析、归一化、格式化等步骤，为后续的推理做好准备。</p>
<p>以下是深度学习模型部署中 <strong>数据加载</strong> 的详细解析：</p>
<hr>
<h3 id="1-数据加载的核心任务"><a href="#1-数据加载的核心任务" class="headerlink" title="1. 数据加载的核心任务"></a><strong>1. 数据加载的核心任务</strong></h3><p>数据加载的主要任务包括：</p>
<ol>
<li><p><strong>从各种来源读取数据</strong>  </p>
<ul>
<li>常见数据来源：文件系统、数据库、API、消息队列等。</li>
<li>文件格式：图片（JPEG&#x2F;PNG）、视频（MP4）、音频（WAV&#x2F;MP3）、文本（JSON&#x2F;CSV）、二进制数据等。</li>
</ul>
</li>
<li><p><strong>解析和转换数据</strong>  </p>
<ul>
<li>解析数据格式（如图片解码、JSON 解析）。</li>
<li>将数据转化为模型可用的张量（Tensor）格式。</li>
</ul>
</li>
<li><p><strong>批量加载和优化性能</strong>  </p>
<ul>
<li>支持批量加载（Batching）以提高吞吐量。</li>
<li>利用多线程或异步加载优化性能。</li>
</ul>
</li>
<li><p><strong>对输入数据进行前处理</strong>  </p>
<ul>
<li>标准化、归一化。</li>
<li>调整大小、裁剪、填充。</li>
<li>数据增强（如旋转、翻转等）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-数据加载的典型步骤"><a href="#2-数据加载的典型步骤" class="headerlink" title="2. 数据加载的典型步骤"></a><strong>2. 数据加载的典型步骤</strong></h3><h4 id="1-数据读取"><a href="#1-数据读取" class="headerlink" title="(1) 数据读取"></a><strong>(1) 数据读取</strong></h4><p>根据数据来源和格式，选择适当的读取方法：</p>
<ul>
<li><strong>本地文件系统：</strong> 使用操作系统原生文件读写函数（如 <code>open()</code>、<code>fopen()</code>）。</li>
<li><strong>云存储&#x2F;网络资源：</strong> 使用库（如 <code>requests</code>、<code>boto3</code>）下载文件。</li>
<li><strong>数据库：</strong> 使用 SQL 或 NoSQL 查询获取数据。</li>
</ul>
<p>示例（读取本地图片）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;image.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-数据解析"><a href="#2-数据解析" class="headerlink" title="(2) 数据解析"></a><strong>(2) 数据解析</strong></h4><ul>
<li>将原始数据解析为内存中的对象。例如：<ul>
<li>图片数据解码为像素矩阵。</li>
<li>JSON 数据解析为 Python 字典。</li>
<li>CSV 数据解析为 DataFrame 或数组。</li>
</ul>
</li>
</ul>
<h4 id="3-数据转换为张量"><a href="#3-数据转换为张量" class="headerlink" title="(3) 数据转换为张量"></a><strong>(3) 数据转换为张量</strong></h4><p>将解析后的数据转换为深度学习框架支持的张量（如 PyTorch 的 <code>torch.Tensor</code> 或 TensorFlow 的 <code>tf.Tensor</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.ToTensor()</span><br><span class="line">tensor = transform(image)</span><br></pre></td></tr></table></figure>

<h4 id="4-数据预处理"><a href="#4-数据预处理" class="headerlink" title="(4) 数据预处理"></a><strong>(4) 数据预处理</strong></h4><p>根据模型需求对数据进行调整：</p>
<ul>
<li><strong>归一化：</strong> 将像素值缩放到 [0, 1] 或 [-1, 1] 区间。</li>
<li><strong>调整大小：</strong> 将图片调整为固定尺寸。</li>
<li><strong>格式转换：</strong> 如 RGB -&gt; BGR 或 NHWC -&gt; NCHW。</li>
</ul>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),   <span class="comment"># 调整大小</span></span><br><span class="line">    transforms.ToTensor(),           <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 归一化</span></span><br><span class="line">])</span><br><span class="line">processed_image = transform(image)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="3-高效数据加载技巧"><a href="#3-高效数据加载技巧" class="headerlink" title="3. 高效数据加载技巧"></a><strong>3. 高效数据加载技巧</strong></h3><h4 id="1-批量加载"><a href="#1-批量加载" class="headerlink" title="(1) 批量加载"></a><strong>(1) 批量加载</strong></h4><p>通过批量加载数据，可以减少 I&#x2F;O 操作的开销，提高吞吐量。</p>
<ul>
<li><strong>示例（PyTorch DataLoader）：</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_paths, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.file_paths = file_paths</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.file_paths)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        image = Image.<span class="built_in">open</span>(<span class="variable language_">self</span>.file_paths[idx])</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">        <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line">dataset = CustomDataset(file_paths=[<span class="string">&quot;img1.jpg&quot;</span>, <span class="string">&quot;img2.jpg&quot;</span>], transform=transform)</span><br><span class="line">data_loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-异步加载"><a href="#2-异步加载" class="headerlink" title="(2) 异步加载"></a><strong>(2) 异步加载</strong></h4><p>使用多线程或多进程加载数据，在数据加载的同时进行推理。</p>
<ul>
<li><strong>示例（PyTorch 的多进程加载）：</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-数据缓存"><a href="#3-数据缓存" class="headerlink" title="(3) 数据缓存"></a><strong>(3) 数据缓存</strong></h4><p>对于频繁使用的数据，可以将其缓存到内存或更快的存储设备（如 SSD），减少加载时间。</p>
<hr>
<h3 id="4-常见的数据加载场景"><a href="#4-常见的数据加载场景" class="headerlink" title="4. 常见的数据加载场景"></a><strong>4. 常见的数据加载场景</strong></h3><h4 id="1-图片数据加载"><a href="#1-图片数据加载" class="headerlink" title="(1) 图片数据加载"></a><strong>(1) 图片数据加载</strong></h4><ul>
<li><strong>格式：</strong> JPEG、PNG、TIFF 等。</li>
<li><strong>前处理：</strong> <ul>
<li>调整大小、裁剪。</li>
<li>归一化为 [0, 1] 或 [-1, 1]。</li>
</ul>
</li>
<li><strong>库支持：</strong> OpenCV、Pillow、Torchvision、TensorFlow。</li>
</ul>
<h4 id="2-视频数据加载"><a href="#2-视频数据加载" class="headerlink" title="(2) 视频数据加载"></a><strong>(2) 视频数据加载</strong></h4><ul>
<li><strong>格式：</strong> MP4、AVI 等。</li>
<li><strong>前处理：</strong><ul>
<li>抽帧、调整帧率。</li>
<li>归一化。</li>
</ul>
</li>
<li><strong>库支持：</strong> OpenCV、FFmpeg、Decord。</li>
</ul>
<h4 id="3-文本数据加载"><a href="#3-文本数据加载" class="headerlink" title="(3) 文本数据加载"></a><strong>(3) 文本数据加载</strong></h4><ul>
<li><strong>格式：</strong> JSON、CSV、纯文本。</li>
<li><strong>前处理：</strong><ul>
<li>分词、去停用词。</li>
<li>将文本转换为向量（如词嵌入）。</li>
</ul>
</li>
<li><strong>库支持：</strong> pandas、NLTK、Hugging Face。</li>
</ul>
<h4 id="4-音频数据加载"><a href="#4-音频数据加载" class="headerlink" title="(4) 音频数据加载"></a><strong>(4) 音频数据加载</strong></h4><ul>
<li><strong>格式：</strong> WAV、MP3。</li>
<li><strong>前处理：</strong><ul>
<li>采样率调整。</li>
<li>转换为梅尔频谱或 MFCC 特征。</li>
</ul>
</li>
<li><strong>库支持：</strong> librosa、torchaudio。</li>
</ul>
<hr>
<h3 id="5-数据加载中的常见问题及优化"><a href="#5-数据加载中的常见问题及优化" class="headerlink" title="5. 数据加载中的常见问题及优化"></a><strong>5. 数据加载中的常见问题及优化</strong></h3><h4 id="问题-1：数据加载瓶颈"><a href="#问题-1：数据加载瓶颈" class="headerlink" title="问题 1：数据加载瓶颈"></a><strong>问题 1：数据加载瓶颈</strong></h4><ul>
<li><strong>原因：</strong> 数据加载速度不足以跟上推理速度。</li>
<li><strong>优化：</strong><ul>
<li>增加 <code>num_workers</code> 使用多线程&#x2F;多进程。</li>
<li>缓存数据。</li>
<li>使用高效的数据格式（如 TFRecord）。</li>
</ul>
</li>
</ul>
<h4 id="问题-2：内存占用过高"><a href="#问题-2：内存占用过高" class="headerlink" title="问题 2：内存占用过高"></a><strong>问题 2：内存占用过高</strong></h4><ul>
<li><strong>原因：</strong> 一次性加载过多数据。</li>
<li><strong>优化：</strong><ul>
<li>使用生成器或流式加载。</li>
<li>控制批量大小。</li>
</ul>
</li>
</ul>
<h4 id="问题-3：数据预处理耗时长"><a href="#问题-3：数据预处理耗时长" class="headerlink" title="问题 3：数据预处理耗时长"></a><strong>问题 3：数据预处理耗时长</strong></h4><ul>
<li><strong>原因：</strong> 复杂的预处理步骤在加载时完成。</li>
<li><strong>优化：</strong><ul>
<li>将预处理离线完成，保存为中间格式。</li>
<li>使用 GPU 加速数据预处理（如 DALI）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="6-高级工具与框架支持"><a href="#6-高级工具与框架支持" class="headerlink" title="6. 高级工具与框架支持"></a><strong>6. 高级工具与框架支持</strong></h3><ul>
<li><p><strong>NVIDIA DALI（Data Loading Library）：</strong></p>
<ul>
<li>高效的数据加载和预处理库，支持 GPU 加速。</li>
<li>示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nvidia.dali.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">import</span> nvidia.dali.ops <span class="keyword">as</span> ops</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimplePipeline</span>(<span class="title class_ inherited__">Pipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_threads, device_id</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimplePipeline, <span class="variable language_">self</span>).__init__(batch_size, num_threads, device_id)</span><br><span class="line">        <span class="variable language_">self</span>.decode = ops.ImageDecoder(device=<span class="string">&quot;mixed&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.resize = ops.Resize(resize_x=<span class="number">224</span>, resize_y=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">define_graph</span>(<span class="params">self</span>):</span><br><span class="line">        images = <span class="variable language_">self</span>.decode(<span class="variable language_">self</span>.<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.resize(images)</span><br><span class="line"></span><br><span class="line">pipe = SimplePipeline(batch_size=<span class="number">32</span>, num_threads=<span class="number">4</span>, device_id=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>TensorFlow Data API：</strong></p>
<ul>
<li>支持高效的输入管道。</li>
<li>示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">dataset = tf.data.Dataset.list_files(<span class="string">&quot;images/*.jpg&quot;</span>)</span><br><span class="line">dataset = dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: preprocess(x))</span><br><span class="line">dataset = dataset.batch(<span class="number">32</span>).prefetch(tf.data.AUTOTUNE)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>数据加载在深度学习模型部署中至关重要。通过高效的读取、解析、转换、批量加载和预处理，能够显著提升模型推理的性能与可靠性。根据具体场景选择合适的工具和优化策略，可以大大改善部署效果。</p>
<h2 id="图像数据转为模型可用的张量格式-详解"><a href="#图像数据转为模型可用的张量格式-详解" class="headerlink" title="图像数据转为模型可用的张量格式 详解"></a>图像数据转为模型可用的张量格式 详解</h2><p>将图像数据转换为深度学习模型可用的张量格式，是图像处理和模型前处理中的关键步骤。不同的深度学习框架（如 TensorFlow、PyTorch）对张量的具体格式要求略有不同，但总体过程类似。</p>
<p>以下是将图像数据转为模型可用的张量格式的详细步骤和注意事项：</p>
<hr>
<h3 id="1-张量的基本要求"><a href="#1-张量的基本要求" class="headerlink" title="1. 张量的基本要求"></a><strong>1. 张量的基本要求</strong></h3><p>张量是一个多维数组，常见的格式包括：</p>
<ol>
<li><strong>形状（Shape）：</strong> 模型通常要求特定形状的输入，例如 <code>(Batch, Channels, Height, Width)</code> 或 <code>(Batch, Height, Width, Channels)</code>。</li>
<li><strong>数据类型（Data Type）：</strong> 常用 <code>float32</code> 或 <code>float16</code>。</li>
<li><strong>数值范围：</strong> <ul>
<li>一般要求归一化为 <code>[0, 1]</code> 或标准化为均值为 0、方差为 1。</li>
<li>部分模型（如 ResNet、MobileNet）可能要求特定的归一化范围或通道顺序。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-转换步骤"><a href="#2-转换步骤" class="headerlink" title="2. 转换步骤"></a><strong>2. 转换步骤</strong></h3><h4 id="1-读取图像数据"><a href="#1-读取图像数据" class="headerlink" title="(1) 读取图像数据"></a><strong>(1) 读取图像数据</strong></h4><ul>
<li>使用图像处理库读取图像文件，将其转化为像素矩阵（通常是 <code>Height x Width x Channels</code> 格式）。</li>
<li>常用库：<ul>
<li><strong>OpenCV (<code>cv2</code>)：</strong> 默认读取为 BGR 格式。</li>
<li><strong>Pillow (<code>PIL</code>)：</strong> 默认读取为 RGB 格式。</li>
</ul>
</li>
</ul>
<p>示例（读取图片）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;image.jpg&quot;</span>)  <span class="comment"># 使用 Pillow 读取图片</span></span><br></pre></td></tr></table></figure>

<h4 id="2-调整尺寸"><a href="#2-调整尺寸" class="headerlink" title="(2) 调整尺寸"></a><strong>(2) 调整尺寸</strong></h4><ul>
<li>深度学习模型通常要求固定尺寸的输入图像，例如 <code>224x224</code>。</li>
<li>可以通过调整大小（<code>resize</code>）、裁剪（<code>crop</code>）、填充（<code>pad</code>）等方式实现。</li>
</ul>
<p>示例（调整大小）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image = image.resize((<span class="number">224</span>, <span class="number">224</span>))  <span class="comment"># 调整到 224x224</span></span><br></pre></td></tr></table></figure>

<h4 id="3-转换为数组"><a href="#3-转换为数组" class="headerlink" title="(3) 转换为数组"></a><strong>(3) 转换为数组</strong></h4><ul>
<li>将图像对象转换为数值数组（通常为 NumPy 数组）。</li>
</ul>
<p>示例（Pillow 转 NumPy）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">image_array = np.array(image)  <span class="comment"># 转换为 NumPy 数组</span></span><br></pre></td></tr></table></figure>

<h4 id="4-归一化或标准化"><a href="#4-归一化或标准化" class="headerlink" title="(4) 归一化或标准化"></a><strong>(4) 归一化或标准化</strong></h4><ul>
<li>将像素值范围从 <code>[0, 255]</code> 转换为 <code>[0, 1]</code> 或 [-1, 1]。</li>
<li>如果模型要求，可以使用特定均值和标准差进行标准化。</li>
</ul>
<p>示例（归一化为 [0, 1]）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_array = image_array / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>

<p>示例（标准化）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">image_array = (image_array - mean) / std</span><br></pre></td></tr></table></figure>

<h4 id="5-调整通道顺序"><a href="#5-调整通道顺序" class="headerlink" title="(5) 调整通道顺序"></a><strong>(5) 调整通道顺序</strong></h4><ul>
<li>不同框架要求的张量通道顺序可能不同：<ul>
<li><strong>PyTorch:</strong> 通道优先，即 <code>(Channels, Height, Width)</code>。</li>
<li><strong>TensorFlow&#x2F;Keras:</strong> 通道最后，即 <code>(Height, Width, Channels)</code>。</li>
</ul>
</li>
<li>可以通过 <code>transpose</code> 改变通道顺序。</li>
</ul>
<p>示例（HWC 转为 CHW）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_array = np.transpose(image_array, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># 从 HWC 转为 CHW</span></span><br></pre></td></tr></table></figure>

<h4 id="6-转换为张量"><a href="#6-转换为张量" class="headerlink" title="(6) 转换为张量"></a><strong>(6) 转换为张量</strong></h4><ul>
<li>将 NumPy 数组转换为深度学习框架支持的张量。</li>
<li>框架示例：<ul>
<li><strong>PyTorch：<code>torch.Tensor</code></strong></li>
<li><strong>TensorFlow：<code>tf.convert_to_tensor</code></strong></li>
</ul>
</li>
</ul>
<p>示例（PyTorch 张量转换）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image_tensor = torch.tensor(image_array, dtype=torch.float32)  <span class="comment"># 转为张量</span></span><br><span class="line">image_tensor = image_tensor.unsqueeze(<span class="number">0</span>)  <span class="comment"># 增加 Batch 维度</span></span><br></pre></td></tr></table></figure>

<p>示例（TensorFlow 张量转换）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">image_tensor = tf.convert_to_tensor(image_array, dtype=tf.float32)  <span class="comment"># 转为张量</span></span><br><span class="line">image_tensor = tf.expand_dims(image_tensor, axis=<span class="number">0</span>)  <span class="comment"># 增加 Batch 维度</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="3-不同框架的常见操作"><a href="#3-不同框架的常见操作" class="headerlink" title="3. 不同框架的常见操作"></a><strong>3. 不同框架的常见操作</strong></h3><h4 id="1-PyTorch"><a href="#1-PyTorch" class="headerlink" title="(1) PyTorch"></a><strong>(1) PyTorch</strong></h4><p>使用 <code>torchvision.transforms</code> 提供了高效的图像预处理工具。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整大小</span></span><br><span class="line">    transforms.ToTensor(),          <span class="comment"># 转为张量并归一化为 [0, 1]</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;image.jpg&quot;</span>)</span><br><span class="line">image_tensor = transform(image).unsqueeze(<span class="number">0</span>)  <span class="comment"># 增加 Batch 维度</span></span><br></pre></td></tr></table></figure>

<h4 id="2-TensorFlow"><a href="#2-TensorFlow" class="headerlink" title="(2) TensorFlow"></a><strong>(2) TensorFlow</strong></h4><p>使用 <code>tf.image</code> 提供了一系列操作。</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">image = tf.io.read_file(<span class="string">&quot;image.jpg&quot;</span>)</span><br><span class="line">image = tf.image.decode_jpeg(image, channels=<span class="number">3</span>)</span><br><span class="line">image = tf.image.resize(image, [<span class="number">224</span>, <span class="number">224</span>])  <span class="comment"># 调整大小</span></span><br><span class="line">image = image / <span class="number">255.0</span>  <span class="comment"># 归一化</span></span><br><span class="line">image = tf.expand_dims(image, axis=<span class="number">0</span>)  <span class="comment"># 增加 Batch 维度</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-常见问题与优化"><a href="#4-常见问题与优化" class="headerlink" title="4. 常见问题与优化"></a><strong>4. 常见问题与优化</strong></h3><h4 id="问题-1：通道顺序错误"><a href="#问题-1：通道顺序错误" class="headerlink" title="问题 1：通道顺序错误"></a><strong>问题 1：通道顺序错误</strong></h4><ul>
<li>模型可能因通道顺序不同而报错（如 TensorFlow 使用 NHWC，而 PyTorch 使用 NCHW）。</li>
<li><strong>解决：</strong> 确认模型要求并调整顺序。</li>
</ul>
<h4 id="问题-2：未归一化或标准化"><a href="#问题-2：未归一化或标准化" class="headerlink" title="问题 2：未归一化或标准化"></a><strong>问题 2：未归一化或标准化</strong></h4><ul>
<li>部分模型对输入的数值范围敏感，未归一化可能导致推理结果异常。</li>
<li><strong>解决：</strong> 检查模型要求并按需处理。</li>
</ul>
<h4 id="问题-3：性能瓶颈"><a href="#问题-3：性能瓶颈" class="headerlink" title="问题 3：性能瓶颈"></a><strong>问题 3：性能瓶颈</strong></h4><ul>
<li>数据加载和预处理可能成为瓶颈，尤其是在实时推理中。</li>
<li><strong>解决：</strong> <ul>
<li>使用多线程&#x2F;多进程。</li>
<li>将预处理步骤迁移到 GPU（如 NVIDIA DALI）。</li>
<li>提前预处理并保存为高效格式（如 TFRecord）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h3><p>将图像数据转为模型可用的张量格式是模型推理的关键步骤，核心流程包括：</p>
<ol>
<li>读取图像数据（文件、内存或流式读取）。</li>
<li>调整尺寸、归一化、标准化。</li>
<li>转换通道顺序和数据类型。</li>
<li>将数据打包为张量格式并加入批量维度。</li>
</ol>
<p>不同框架提供了便利的工具和 API 支持，高效实现图像的张量化，能够显著提升模型的推理性能和兼容性。</p>
<h2 id="前处理中的归一化-详解"><a href="#前处理中的归一化-详解" class="headerlink" title="前处理中的归一化 详解"></a>前处理中的归一化 详解</h2><h3 id="深度学习前处理中的归一化详解"><a href="#深度学习前处理中的归一化详解" class="headerlink" title="深度学习前处理中的归一化详解"></a><strong>深度学习前处理中的归一化详解</strong></h3><p><strong>归一化</strong>是深度学习模型训练和推理中的重要步骤之一，旨在将输入数据的特征值范围调整到特定范围或分布，以提高模型的收敛速度和预测稳定性。本文将详细解析归一化的概念、目的、方法、应用场景和注意事项。</p>
<hr>
<h3 id="1-为什么需要归一化？"><a href="#1-为什么需要归一化？" class="headerlink" title="1. 为什么需要归一化？"></a><strong>1. 为什么需要归一化？</strong></h3><ol>
<li><p><strong>减少特征尺度差异：</strong></p>
<ul>
<li>输入数据可能具有不同的特征范围（例如像素值范围为 <code>[0, 255]</code>，而温度值范围为 <code>[-30, 50]</code>）。</li>
<li>归一化可以让特征值具有相似的尺度，避免某些特征对模型的影响过大。</li>
</ul>
</li>
<li><p><strong>加速模型收敛：</strong></p>
<ul>
<li>无归一化的数据可能导致梯度下降过程缓慢或震荡，归一化能够稳定梯度下降。</li>
</ul>
</li>
<li><p><strong>增强数值稳定性：</strong></p>
<ul>
<li>极端数值可能导致计算不稳定，归一化可以避免溢出或下溢。</li>
</ul>
</li>
<li><p><strong>适应特定模型需求：</strong></p>
<ul>
<li>某些预训练模型要求输入数据在特定范围（如 <code>[0, 1]</code> 或 <code>[-1, 1]</code>）或具有特定的均值和标准差。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-常见的归一化方法"><a href="#2-常见的归一化方法" class="headerlink" title="2. 常见的归一化方法"></a><strong>2. 常见的归一化方法</strong></h3><p>归一化的方法根据应用场景和数据类型有所不同，以下是常见的归一化技术。</p>
<h4 id="1-Min-Max-归一化"><a href="#1-Min-Max-归一化" class="headerlink" title="(1) Min-Max 归一化"></a><strong>(1) Min-Max 归一化</strong></h4><ul>
<li><p><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \frac{x - x_{\min}}{x_{\max} - x_{\min}}<br>]</p>
</li>
<li><p><strong>范围：</strong> 通常将数据归一化到 <code>[0, 1]</code>，或通过线性变换调整为其他范围（如 <code>[-1, 1]</code>）。</p>
</li>
<li><p><strong>应用场景：</strong></p>
<ul>
<li>图像像素值归一化到 <code>[0, 1]</code>。</li>
<li>特征值范围固定的数据。</li>
</ul>
</li>
<li><p><strong>示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = np.array([<span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>, <span class="number">200</span>])</span><br><span class="line">data_min = np.<span class="built_in">min</span>(data)</span><br><span class="line">data_max = np.<span class="built_in">max</span>(data)</span><br><span class="line">normalized_data = (data - data_min) / (data_max - data_min)</span><br><span class="line"><span class="built_in">print</span>(normalized_data)  <span class="comment"># 输出 [0. , 0.333, 0.666, 1. ]</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-均值-标准差标准化（Z-Score-标准化）"><a href="#2-均值-标准差标准化（Z-Score-标准化）" class="headerlink" title="(2) 均值-标准差标准化（Z-Score 标准化）"></a><strong>(2) 均值-标准差标准化（Z-Score 标准化）</strong></h4><ul>
<li><p><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \frac{x - \mu}{\sigma}<br>]<br>其中 ( \mu ) 是均值，( \sigma ) 是标准差。</p>
</li>
<li><p><strong>范围：</strong> 数据转换为均值为 <code>0</code>，标准差为 <code>1</code> 的分布。</p>
</li>
<li><p><strong>应用场景：</strong></p>
<ul>
<li>特征值分布不固定（如自然语言处理中的词向量）。</li>
<li>深度学习预训练模型（如 ImageNet 预训练模型）常使用固定均值和标准差归一化。</li>
</ul>
</li>
<li><p><strong>示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mean = np.mean(data)</span><br><span class="line">std = np.std(data)</span><br><span class="line">standardized_data = (data - mean) / std</span><br><span class="line"><span class="built_in">print</span>(standardized_data)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-归一化到特定均值和标准差"><a href="#3-归一化到特定均值和标准差" class="headerlink" title="(3) 归一化到特定均值和标准差"></a><strong>(3) 归一化到特定均值和标准差</strong></h4><ul>
<li><p><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \frac{x - \mu}{\sigma}<br>]<br>并根据目标分布调整：<br>[<br>x_{\text{scaled}} &#x3D; x_{\text{norm}} \times \sigma_{\text{target}} + \mu_{\text{target}}<br>]</p>
</li>
<li><p><strong>应用场景：</strong></p>
<ul>
<li>针对预训练模型（如 ResNet、VGG 等），调整到特定的均值和标准差。</li>
</ul>
</li>
<li><p><strong>示例（PyTorch）：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">transform = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="4-对数归一化（Log-Normalization）"><a href="#4-对数归一化（Log-Normalization）" class="headerlink" title="(4) 对数归一化（Log Normalization）"></a><strong>(4) 对数归一化（Log Normalization）</strong></h4><ul>
<li><p><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \log(x + 1)<br>]</p>
</li>
<li><p><strong>应用场景：</strong></p>
<ul>
<li>数据分布具有较大偏态（如高光强度图像、文档字频统计）。</li>
</ul>
</li>
<li><p><strong>注意：</strong></p>
<ul>
<li>输入值必须非负。</li>
</ul>
</li>
<li><p><strong>示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log_normalized_data = np.log(data + <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="5-最大绝对值归一化"><a href="#5-最大绝对值归一化" class="headerlink" title="(5) 最大绝对值归一化"></a><strong>(5) 最大绝对值归一化</strong></h4><ul>
<li><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \frac{x}{\max(|x|)}<br>]</li>
<li><strong>范围：</strong> 归一化到 <code>[-1, 1]</code>。</li>
<li><strong>应用场景：</strong><ul>
<li>数据具有正负值，但范围不固定。</li>
</ul>
</li>
</ul>
<h4 id="6-L2-归一化"><a href="#6-L2-归一化" class="headerlink" title="(6) L2 归一化"></a><strong>(6) L2 归一化</strong></h4><ul>
<li><strong>公式：</strong><br>[<br>x_{\text{norm}} &#x3D; \frac{x}{|x|_2}<br>]<br>其中 ( |x|_2 &#x3D; \sqrt{\sum x_i^2} )。</li>
<li><strong>范围：</strong> 单位向量，适合向量空间建模。</li>
<li><strong>应用场景：</strong><ul>
<li>特征向量的相似性计算（如余弦相似度）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-图像数据中的归一化"><a href="#3-图像数据中的归一化" class="headerlink" title="3. 图像数据中的归一化"></a><strong>3. 图像数据中的归一化</strong></h3><p>在图像处理中，归一化的目标是将原始像素值（通常是 <code>[0, 255]</code>）转换为深度学习模型所需的格式。</p>
<h4 id="1-常见归一化方法"><a href="#1-常见归一化方法" class="headerlink" title="(1) 常见归一化方法"></a><strong>(1) 常见归一化方法</strong></h4><ol>
<li><strong>归一化到 <code>[0, 1]</code>：</strong><ul>
<li>公式：<code>x / 255.0</code></li>
<li>应用于大多数深度学习框架。</li>
</ul>
</li>
<li><strong>归一化到 <code>[-1, 1]</code>：</strong><ul>
<li>公式：<code>(x / 127.5) - 1</code></li>
<li>常用于 GAN 和部分图像处理模型。</li>
</ul>
</li>
<li><strong>均值和标准差标准化：</strong><ul>
<li>对应 ImageNet 的 RGB 通道均值和标准差：<ul>
<li>均值：<code>[0.485, 0.456, 0.406]</code></li>
<li>标准差：<code>[0.229, 0.224, 0.225]</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="2-示例代码"><a href="#2-示例代码" class="headerlink" title="(2) 示例代码"></a><strong>(2) 示例代码</strong></h4><p><strong>归一化到 <code>[0, 1]</code>：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">image = np.random.randint(<span class="number">0</span>, <span class="number">256</span>, (<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">image_normalized = image / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>

<p><strong>归一化到特定均值和标准差：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 image 是一个 PIL.Image 对象</span></span><br><span class="line">normalized_image = transform(image)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-深度学习框架中的归一化工具"><a href="#4-深度学习框架中的归一化工具" class="headerlink" title="4. 深度学习框架中的归一化工具"></a><strong>4. 深度学习框架中的归一化工具</strong></h3><h4 id="1-PyTorch-1"><a href="#1-PyTorch-1" class="headerlink" title="(1) PyTorch"></a><strong>(1) PyTorch</strong></h4><p><code>torchvision.transforms.Normalize</code></p>
<ul>
<li>用法：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transforms.Normalize(mean, std)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-TensorFlow-Keras"><a href="#2-TensorFlow-Keras" class="headerlink" title="(2) TensorFlow&#x2F;Keras"></a><strong>(2) TensorFlow&#x2F;Keras</strong></h4><p><code>tf.image.per_image_standardization</code> 或手动实现归一化。</p>
<hr>
<h3 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5. 注意事项"></a><strong>5. 注意事项</strong></h3><ol>
<li><strong>模型需求：</strong> 确保归一化过程符合模型的预处理要求。</li>
<li><strong>一致性：</strong> 训练和推理时的归一化方式必须一致。</li>
<li><strong>异常值处理：</strong> 对异常值进行裁剪或忽略，避免对归一化结果产生不利影响。</li>
</ol>
<hr>
<p>归一化是模型预处理中的关键步骤，能够显著提升模型性能。根据不同场景选择合适的归一化方法，有助于稳定和加速训练，同时提高推理精度。</p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><ul>
<li>常用的方法是通过对原始数据进行线性变换把数据映射到 [0, 1] 或 [-1, 1]之间。主要是：<ul>
<li>为了数据处理方便，更加便捷快速。</li>
<li>把有量纲表达式变为无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。</li>
</ul>
</li>
<li>归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为 纯量</li>
<li>变换函数为：<br><img src="/images/Deeplearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E5%BD%92%E4%B8%80%E5%8C%96%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%85%AC%E5%BC%8F.png" alt="归一化线性变换公式"></li>
</ul>
<h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><ul>
<li>常用的方法是 z-score 标准化，经过处理后的数据均值为0，标准差为1</li>
<li>在机器学习中，我们可能要处理不同种类的资料。例如，音讯和图片上的像素值，这些资料可能是高纬度的，资料标准化后使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)，标准差变为1，这个方法被广泛的使用在许多机器学习中，例如：支持向量机，逻辑回归和类神经网络<br><img src="/images/Deeplearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A0%87%E5%87%86%E5%8C%96%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%85%AC%E5%BC%8F.png" alt="标准化线性变换公式"></li>
</ul>
<h2 id="归一化和标准化的区别"><a href="#归一化和标准化的区别" class="headerlink" title="归一化和标准化的区别"></a>归一化和标准化的区别</h2><ul>
<li>归一化是将样本的特征值转换到同一量纲下把数据映射到 [0, 1]或者 [-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。</li>
<li>标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。</li>
<li>它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。</li>
</ul>
<hr>
<h2 id="标准化和中心化的区别"><a href="#标准化和中心化的区别" class="headerlink" title="标准化和中心化的区别"></a>标准化和中心化的区别</h2><ul>
<li>标准化是原始分数减去平均数然后除以标准差</li>
<li>中心化是原始分数减去平均数。所以一般流程为先中心化再标准化。</li>
</ul>
<h2 id="无量纲"><a href="#无量纲" class="headerlink" title="无量纲"></a>无量纲</h2><ul>
<li>无量纲，是通过某种方法能去表实际过程中的单位，从而简化计算。</li>
</ul>
<h2 id="为什么要归一化-标准化"><a href="#为什么要归一化-标准化" class="headerlink" title="为什么要归一化&#x2F;标准化"></a>为什么要归一化&#x2F;标准化</h2><ul>
<li><p>归一化&#x2F;标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成失效，反而能提高数据的表现，这些性质是归一化&#x2F;标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。</p>
</li>
<li><p>简单来说有以下好处</p>
<ul>
<li>由于原始数据值的范围差异很大，因此在某些机器学习算法中，如果没有归一化，目标函数将无法正常工作。</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/31/notebook/DeepLearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/2024-12-31-%E6%8E%A8%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/31/notebook/DeepLearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/2024-12-31-%E6%8E%A8%E7%90%86/" class="post-title-link" itemprop="url">推理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-31 09:00:00" itemprop="dateCreated datePublished" datetime="2024-12-31T09:00:00+08:00">2024-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>深度学习模型部署 推理部分相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/12/31/notebook/DeepLearning/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/2024-12-31-%E6%8E%A8%E7%90%86/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/30/notebook/OpenCV/opencv_2_%E5%B8%B8%E7%94%A8%E7%B1%BB/2024-12-30-Rect/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/30/notebook/OpenCV/opencv_2_%E5%B8%B8%E7%94%A8%E7%B1%BB/2024-12-30-Rect/" class="post-title-link" itemprop="url">Rect</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-30 10:00:00" itemprop="dateCreated datePublished" datetime="2024-12-30T10:00:00+08:00">2024-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/OpenCV/" itemprop="url" rel="index"><span itemprop="name">OpenCV</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="cv-Rect-类-详解"><a href="#cv-Rect-类-详解" class="headerlink" title="cv::Rect 类 详解"></a>cv::Rect 类 详解</h2><p><code>cv::Rect</code> 是 OpenCV 中的一个类，表示一个矩形区域。它常用于图像处理中的矩形区域定义，尤其是在图像裁剪、目标检测、区域分析等任务中。<code>cv::Rect</code> 类非常简洁和高效，提供了多种构造方法和常用操作，使得矩形区域的处理变得简单方便。</p>
<hr>
<h2 id="cv-Rect-类定义"><a href="#cv-Rect-类定义" class="headerlink" title="cv::Rect 类定义"></a><strong><code>cv::Rect</code> 类定义</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">cv</span>::Rect</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 4个构造函数</span></span><br><span class="line">    <span class="built_in">Rect</span>();  <span class="comment">// 默认构造函数</span></span><br><span class="line">    <span class="built_in">Rect</span>(<span class="type">int</span> _x, <span class="type">int</span> _y, <span class="type">int</span> _width, <span class="type">int</span> _height);  <span class="comment">// 使用坐标和宽高构造矩形</span></span><br><span class="line">    <span class="built_in">Rect</span>(<span class="type">const</span> Point&amp; pt1, <span class="type">const</span> Point&amp; pt2);  <span class="comment">// 使用对角线两个点构造矩形</span></span><br><span class="line">    <span class="built_in">Rect</span>(<span class="type">const</span> Rect&amp; r);  <span class="comment">// 拷贝构造函数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 成员变量</span></span><br><span class="line">    <span class="type">int</span> x;     <span class="comment">// 矩形左上角的 x 坐标</span></span><br><span class="line">    <span class="type">int</span> y;     <span class="comment">// 矩形左上角的 y 坐标</span></span><br><span class="line">    <span class="type">int</span> width; <span class="comment">// 矩形的宽度</span></span><br><span class="line">    <span class="type">int</span> height;<span class="comment">// 矩形的高度</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a><strong>参数详解</strong></h2><ol>
<li><p>**<code>x</code>**：</p>
<ul>
<li>矩形左上角的 x 坐标。</li>
</ul>
</li>
<li><p>**<code>y</code>**：</p>
<ul>
<li>矩形左上角的 y 坐标。</li>
</ul>
</li>
<li><p>**<code>width</code>**：</p>
<ul>
<li>矩形的宽度，表示矩形水平方向的长度。</li>
</ul>
</li>
<li><p>**<code>height</code>**：</p>
<ul>
<li>矩形的高度，表示矩形垂直方向的长度。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a><strong>构造方法</strong></h2><ol>
<li><p><strong>默认构造函数</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Rect</span>();  <span class="comment">// 默认值 x=0, y=0, width=0, height=0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个默认的矩形，通常是 (0, 0) 为左上角，宽度和高度都为 0。</li>
</ul>
</li>
<li><p><strong>指定坐标和宽高构造矩形</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Rect</span>(<span class="type">int</span> _x, <span class="type">int</span> _y, <span class="type">int</span> _width, <span class="type">int</span> _height);</span><br></pre></td></tr></table></figure>

<ul>
<li>通过指定矩形的左上角 <code>(x, y)</code> 和矩形的 <code>width</code>（宽度）和 <code>height</code>（高度）来构造矩形。</li>
</ul>
</li>
<li><p><strong>通过对角线两个点构造矩形</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Rect</span>(<span class="type">const</span> Point&amp; pt1, <span class="type">const</span> Point&amp; pt2);</span><br></pre></td></tr></table></figure>

<ul>
<li>使用两个对角点 <code>pt1</code> 和 <code>pt2</code> 来创建矩形，<code>pt1</code> 和 <code>pt2</code> 表示矩形的对角线的两个点。</li>
</ul>
</li>
<li><p><strong>拷贝构造函数</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Rect</span>(<span class="type">const</span> Rect&amp; r);</span><br></pre></td></tr></table></figure>

<ul>
<li>通过另一个矩形对象来拷贝构造一个新的矩形。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="成员函数"><a href="#成员函数" class="headerlink" title="成员函数"></a><strong>成员函数</strong></h2><ol>
<li><p>**<code>area()</code>**：</p>
<ul>
<li>返回矩形的面积（即 <code>width * height</code>）。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">area</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>**<code>empty()</code>**：</p>
<ul>
<li>检查矩形是否为空（即宽度或高度为 0）。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>**<code>br()</code>**：</p>
<ul>
<li>返回矩形的右下角点。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Point <span class="title">br</span><span class="params">()</span> <span class="type">const</span></span>;  <span class="comment">// 返回 (x + width, y + height)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>**<code>tl()</code>**：</p>
<ul>
<li>返回矩形的左上角点。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Point <span class="title">tl</span><span class="params">()</span> <span class="type">const</span></span>;  <span class="comment">// 返回 (x, y)</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a><strong>常用操作</strong></h2><ol>
<li><p><strong>矩形相加</strong>：</p>
<ul>
<li>两个矩形相加，得到一个包含两个矩形的最小外接矩形。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Rect <span class="title">r1</span><span class="params">(<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line"><span class="function">Rect <span class="title">r2</span><span class="params">(<span class="number">5</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line">Rect r3 = r1 | r2;  <span class="comment">// 得到一个包含两个矩形的外接矩形</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>矩形相交</strong>：</p>
<ul>
<li>得到两个矩形的交集部分，返回一个新的矩形。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Rect <span class="title">r1</span><span class="params">(<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line"><span class="function">Rect <span class="title">r2</span><span class="params">(<span class="number">5</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line">Rect r3 = r1 &amp; r2;  <span class="comment">// 得到两个矩形的交集</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>矩形移动</strong>：</p>
<ul>
<li>移动矩形的位置。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Rect <span class="title">r</span><span class="params">(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span></span>;</span><br><span class="line">r.x += <span class="number">5</span>;  <span class="comment">// 修改 x 坐标</span></span><br><span class="line">r.y += <span class="number">5</span>;  <span class="comment">// 修改 y 坐标</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>矩形包含检测</strong>：</p>
<ul>
<li>判断一个点是否在矩形内，或者一个矩形是否包含另一个矩形。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Rect <span class="title">r</span><span class="params">(<span class="number">10</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">50</span>)</span></span>;</span><br><span class="line"><span class="function">Point <span class="title">pt</span><span class="params">(<span class="number">30</span>, <span class="number">30</span>)</span></span>;</span><br><span class="line"><span class="type">bool</span> inside = r.<span class="built_in">contains</span>(pt);  <span class="comment">// 判断点 (30, 30) 是否在矩形内</span></span><br><span class="line"></span><br><span class="line"><span class="function">Rect <span class="title">r1</span><span class="params">(<span class="number">10</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">50</span>)</span></span>;</span><br><span class="line"><span class="function">Rect <span class="title">r2</span><span class="params">(<span class="number">20</span>, <span class="number">20</span>, <span class="number">10</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line"><span class="type">bool</span> contained = r<span class="number">1.</span><span class="built_in">contains</span>(r2);  <span class="comment">// 判断 r2 是否完全包含在 r1 中</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="常见应用场景"><a href="#常见应用场景" class="headerlink" title="常见应用场景"></a><strong>常见应用场景</strong></h2><ol>
<li><p><strong>图像裁剪</strong>：</p>
<ul>
<li>在图像处理任务中，<code>cv::Rect</code> 用于指定感兴趣区域（ROI）。通过矩形定义裁剪区域，可以快速地对图像进行裁剪。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mat img = <span class="built_in">imread</span>(<span class="string">&quot;image.jpg&quot;</span>);</span><br><span class="line"><span class="function">Rect <span class="title">roi</span><span class="params">(<span class="number">100</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">200</span>)</span></span>;  <span class="comment">// 定义感兴趣区域</span></span><br><span class="line">Mat cropped = <span class="built_in">img</span>(roi);        <span class="comment">// 裁剪图像</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>目标检测</strong>：</p>
<ul>
<li>在目标检测任务中，通常使用矩形框来表示检测到的目标。例如，检测到的人脸、车辆或其他物体通常会使用矩形框进行标记。</li>
</ul>
</li>
<li><p><strong>图像窗口和标注</strong>：</p>
<ul>
<li><code>cv::Rect</code> 用于表示图像中的标注区域或绘制图像的窗口。在 OpenCV 中经常会使用矩形框来标记特定区域，如绘制矩形框标记目标。</li>
</ul>
</li>
<li><p><strong>区域分析</strong>：</p>
<ul>
<li>在图像分析中，<code>cv::Rect</code> 常用于表示分析的区域，尤其是在分割、提取特征等任务中。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h2><h3 id="创建矩形并输出其属性"><a href="#创建矩形并输出其属性" class="headerlink" title="创建矩形并输出其属性"></a><strong>创建矩形并输出其属性</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建矩形</span></span><br><span class="line">    <span class="function">Rect <span class="title">r1</span><span class="params">(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>)</span></span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Rect 1: (&quot;</span> &lt;&lt; r<span class="number">1.</span>x &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; r<span class="number">1.</span>y &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; r<span class="number">1.</span>width &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; r<span class="number">1.</span>height &lt;&lt; <span class="string">&quot;)&quot;</span> &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 计算矩形面积</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Area: &quot;</span> &lt;&lt; r<span class="number">1.</span><span class="built_in">area</span>() &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 判断矩形是否为空</span></span><br><span class="line">    <span class="keyword">if</span> (r<span class="number">1.</span><span class="built_in">empty</span>()) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Rect is empty&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Rect is not empty&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取矩形的右下角</span></span><br><span class="line">    Point br = r<span class="number">1.</span><span class="built_in">br</span>();</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Bottom-right corner: (&quot;</span> &lt;&lt; br.x &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; br.y &lt;&lt; <span class="string">&quot;)&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>输出</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rect 1: (10, 20, 30, 40)</span><br><span class="line">Area: 1200</span><br><span class="line">Rect is not empty</span><br><span class="line">Bottom-right corner: (40, 60)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><ul>
<li><code>cv::Rect</code> 是一个简单而强大的类，用于表示矩形区域，在图像处理、目标检测、图像裁剪、区域分析等多种应用中都有广泛的用途。</li>
<li>它提供了多种构造方法、成员函数和常用操作，使得矩形的使用非常方便。</li>
<li><code>cv::Rect</code> 主要用于图像处理中的区域定义、裁剪、目标检测、区域计算等任务，是 OpenCV 中不可或缺的重要类。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/30/notebook/OpenCV/opencv_2_%E5%B8%B8%E7%94%A8%E7%B1%BB/2024-12-30-Mat/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/30/notebook/OpenCV/opencv_2_%E5%B8%B8%E7%94%A8%E7%B1%BB/2024-12-30-Mat/" class="post-title-link" itemprop="url">Mat</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-30 10:00:00" itemprop="dateCreated datePublished" datetime="2024-12-30T10:00:00+08:00">2024-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/OpenCV/" itemprop="url" rel="index"><span itemprop="name">OpenCV</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>cv::Mat 类相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/12/30/notebook/OpenCV/opencv_2_%E5%B8%B8%E7%94%A8%E7%B1%BB/2024-12-30-Mat/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/30/notebook/DeepLearning/2024-12-30-%E5%A4%84%E7%90%86%E5%99%A8%E6%9E%B6%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/30/notebook/DeepLearning/2024-12-30-%E5%A4%84%E7%90%86%E5%99%A8%E6%9E%B6%E6%9E%84/" class="post-title-link" itemprop="url">处理器架构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-30 09:00:00" itemprop="dateCreated datePublished" datetime="2024-12-30T09:00:00+08:00">2024-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>常见处理器架构 CPU&#x2F;GPU&#x2F;FPGA 相关学习笔记</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/12/30/notebook/DeepLearning/2024-12-30-%E5%A4%84%E7%90%86%E5%99%A8%E6%9E%B6%E6%9E%84/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://junyiha.github.io/2024/12/30/notebook/C++/C++_04_%E5%B8%B8%E7%94%A8%E5%BA%93/2024-12-30-onnxruntime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhang junyi">
      <meta itemprop="description" content="工作学习笔记">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="junyi's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/30/notebook/C++/C++_04_%E5%B8%B8%E7%94%A8%E5%BA%93/2024-12-30-onnxruntime/" class="post-title-link" itemprop="url">onnxruntime</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-30 09:00:00" itemprop="dateCreated datePublished" datetime="2024-12-30T09:00:00+08:00">2024-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-28 16:08:39" itemprop="dateModified" datetime="2025-04-28T16:08:39+08:00">2025-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cpp/" itemprop="url" rel="index"><span itemprop="name">Cpp</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>onnxruntime库相关学习笔记</li>
</ul>
<h2 id="onnxruntime库-是什么"><a href="#onnxruntime库-是什么" class="headerlink" title="onnxruntime库 是什么"></a>onnxruntime库 是什么</h2><p><strong><code>ONNX Runtime</code></strong> 是一个高性能推理引擎，专门用于执行基于 ONNX（Open Neural Network Exchange） 格式的机器学习模型。它由 <strong>微软（Microsoft）</strong> 开发，旨在提供跨平台、高效、灵活的推理支持，支持从云到边缘设备的多种部署场景。</p>
<hr>
<h3 id="核心特点"><a href="#核心特点" class="headerlink" title="核心特点"></a><strong>核心特点</strong></h3><ol>
<li><p><strong>跨平台支持</strong>：</p>
<ul>
<li>可在 Windows、Linux、MacOS 上运行，也支持移动设备（iOS 和 Android）和嵌入式系统。</li>
</ul>
</li>
<li><p><strong>高性能</strong>：</p>
<ul>
<li>提供高效的硬件加速，支持 CPU、GPU，以及特殊硬件（如 Intel OpenVINO、NVIDIA TensorRT）。</li>
</ul>
</li>
<li><p><strong>多语言支持</strong>：</p>
<ul>
<li>提供 Python、C++、C# 等语言绑定，方便开发者集成到不同语言的项目中。</li>
</ul>
</li>
<li><p><strong>灵活性</strong>：</p>
<ul>
<li>支持从多种框架（如 PyTorch、TensorFlow、Keras、MXNet）导出的模型。</li>
<li>提供优化工具（如 <code>onnxruntime-tools</code>），帮助提升模型推理性能。</li>
</ul>
</li>
<li><p><strong>云与边缘兼容</strong>：</p>
<ul>
<li>适用于云端服务和边缘设备（如 IoT 和移动端），满足多种场景的部署需求。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a><strong>主要功能</strong></h3><ol>
<li><p><strong>模型推理（Inference）</strong>：</p>
<ul>
<li>通过加载 ONNX 格式的模型，快速进行预测或分类。</li>
<li>示例（Python）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">session = ort.InferenceSession(<span class="string">&quot;model.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">input_data = &#123;<span class="string">&quot;input&quot;</span>: some_numpy_array&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理</span></span><br><span class="line">output = session.run(<span class="literal">None</span>, input_data)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>硬件加速支持</strong>：</p>
<ul>
<li>自动选择最优的硬件执行提供程序（Execution Provider），如：<ul>
<li>CPU（默认）</li>
<li>GPU（CUDA、DirectML）</li>
<li>专用加速（TensorRT、OpenVINO 等）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>动态与静态模型支持</strong>：</p>
<ul>
<li>兼容静态计算图和动态计算图模型。</li>
</ul>
</li>
<li><p><strong>优化和量化</strong>：</p>
<ul>
<li>提供模型优化工具（如模型裁剪、量化）以减少模型大小，提高推理速度。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h3><ol>
<li><strong>Web 服务</strong>：部署深度学习模型进行实时推理（如聊天机器人、图像分类服务）。</li>
<li><strong>移动应用</strong>：将经过优化的 ONNX 模型嵌入到移动端以进行离线预测。</li>
<li><strong>边缘设备</strong>：在嵌入式系统中运行 ONNX 模型（如工业物联网传感器）。</li>
<li><strong>高性能计算</strong>：利用 GPU 或其他加速器大规模推理。</li>
</ol>
<hr>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>使用 pip 安装 ONNX Runtime 的 Python 包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnxruntime</span><br></pre></td></tr></table></figure>

<p>如果需要 GPU 支持，可以安装 GPU 版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnxruntime-gpu</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="ONNX-的来源"><a href="#ONNX-的来源" class="headerlink" title="ONNX 的来源"></a><strong>ONNX 的来源</strong></h3><p>ONNX 是一种开放的深度学习模型格式，支持不同框架间的模型互操作。通过将模型转换为 ONNX 格式，ONNX Runtime 能够高效加载并执行这些模型。</p>
<hr>
<h3 id="ONNX-Runtime-的优势"><a href="#ONNX-Runtime-的优势" class="headerlink" title="ONNX Runtime 的优势"></a><strong>ONNX Runtime 的优势</strong></h3><ul>
<li><strong>简化开发和部署流程</strong>：无需为每种硬件配置单独编写代码。</li>
<li><strong>兼容多框架模型</strong>：同一推理引擎支持从多个框架导出的模型。</li>
<li><strong>高效运行</strong>：内置优化能力，可在多种硬件上实现卓越性能。</li>
</ul>
<hr>
<p>如果你有更具体的需求（如如何优化模型或特定硬件部署），可以告诉我，我会详细解答！</p>
<h2 id="onnxruntime-C-接口-详解"><a href="#onnxruntime-C-接口-详解" class="headerlink" title="onnxruntime C++接口 详解"></a>onnxruntime C++接口 详解</h2><p><code>ONNX Runtime</code> 提供了功能强大的 C++ 接口，用于高性能地加载和推理 ONNX 模型。以下是对其 C++ 接口的详细解析，包括核心概念、关键类、常用方法及完整示例。</p>
<hr>
<h3 id="核心概念与类"><a href="#核心概念与类" class="headerlink" title="核心概念与类"></a><strong>核心概念与类</strong></h3><p>ONNX Runtime 的 C++ API 基于以下核心组件：</p>
<ol>
<li><p>**<code>Ort::Env</code>**：</p>
<ul>
<li>表示 ONNX Runtime 的环境，所有会话共享一个环境。</li>
<li>主要负责初始化 ONNX Runtime，设置日志等。</li>
</ul>
</li>
<li><p>**<code>Ort::Session</code>**：</p>
<ul>
<li>表示一个模型会话，用于加载和执行 ONNX 模型。</li>
<li>会话加载模型后可以多次推理。</li>
</ul>
</li>
<li><p>**<code>Ort::MemoryInfo</code>**：</p>
<ul>
<li>描述用于输入&#x2F;输出张量的内存分配信息。</li>
</ul>
</li>
<li><p>**<code>Ort::Allocator</code>**：</p>
<ul>
<li>管理内存分配，用于推理中获取分配的结果张量。</li>
</ul>
</li>
<li><p>**<code>Ort::Value</code>**：</p>
<ul>
<li>表示 ONNX 模型的输入和输出张量。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键步骤与方法"><a href="#关键步骤与方法" class="headerlink" title="关键步骤与方法"></a><strong>关键步骤与方法</strong></h3><ol>
<li><p><strong>初始化环境</strong>：</p>
<ul>
<li>使用 <code>Ort::Env</code> 创建 ONNX Runtime 环境：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>加载模型</strong>：</p>
<ul>
<li>使用 <code>Ort::Session</code> 加载 <code>.onnx</code> 模型文件：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line"><span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>准备输入和输出</strong>：</p>
<ul>
<li>创建输入张量并指定维度和数据类型：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;; <span class="comment">// Example shape for an image</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>; <span class="comment">// Initialize with dummy data</span></span><br><span class="line">Ort::MemoryInfo memory_info = Ort::MemoryInfo::<span class="built_in">CreateCpu</span>(OrtArenaAllocator, OrtMemTypeDefault);</span><br><span class="line">Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">    memory_info, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>获取模型输入&#x2F;输出名称</strong>：</p>
<ul>
<li>用于绑定输入和输出：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> num_input_nodes = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>执行推理</strong>：</p>
<ul>
<li>调用 <code>Run</code> 方法执行推理：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">    Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,    <span class="comment">// 默认运行选项</span></span><br><span class="line">    &amp;input_name,                <span class="comment">// 输入名称</span></span><br><span class="line">    &amp;input_tensor,              <span class="comment">// 输入张量</span></span><br><span class="line">    <span class="number">1</span>,                          <span class="comment">// 输入数量</span></span><br><span class="line">    &amp;output_name,               <span class="comment">// 输出名称</span></span><br><span class="line">    <span class="number">1</span>                           <span class="comment">// 输出数量</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>处理输出</strong>：</p>
<ul>
<li>输出通常是一个张量，可以通过 <code>GetTensorMutableData</code> 获取数据指针：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<hr>
<h3 id="完整示例代码"><a href="#完整示例代码" class="headerlink" title="完整示例代码"></a><strong>完整示例代码</strong></h3><p>下面是一个完整的 C++ 程序，演示如何使用 ONNX Runtime 推理一个 ONNX 模型：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载 ONNX 模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型输入信息</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>; <span class="comment">// Dummy input</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入张量</span></span><br><span class="line">    Ort::MemoryInfo memory_info = Ort::MemoryInfo::<span class="built_in">CreateCpu</span>(OrtArenaAllocator, OrtMemTypeDefault);</span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        memory_info, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型输出信息</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="keyword">auto</span> output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">        Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,</span><br><span class="line">        &amp;input_name, &amp;input_tensor, <span class="number">1</span>,</span><br><span class="line">        &amp;output_name, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出张量数据</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出结果（示例）</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output value: &quot;</span> &lt;&lt; output_data[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优化与扩展"><a href="#优化与扩展" class="headerlink" title="优化与扩展"></a><strong>优化与扩展</strong></h3><ol>
<li><p><strong>多线程优化</strong>：</p>
<ul>
<li>使用 <code>session_options.SetIntraOpNumThreads()</code> 和 <code>SetInterOpNumThreads()</code> 配置多线程。</li>
</ul>
</li>
<li><p><strong>GPU 支持</strong>：</p>
<ul>
<li>安装 ONNX Runtime 的 GPU 版本，并通过 <code>session_options.AppendExecutionProvider_CUDA()</code> 启用 GPU。</li>
</ul>
</li>
<li><p><strong>动态输入支持</strong>：</p>
<ul>
<li>对于不确定输入形状的模型，可在运行时动态设置张量维度。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a><strong>常见问题</strong></h3><ol>
<li><p><strong>模型加载失败</strong>：</p>
<ul>
<li>确保模型是有效的 ONNX 格式。</li>
<li>检查模型是否包含动态维度（需要正确处理）。</li>
</ul>
</li>
<li><p><strong>输出数据为空</strong>：</p>
<ul>
<li>确认输入张量的数据和维度与模型的预期一致。</li>
</ul>
</li>
<li><p><strong>硬件加速问题</strong>：</p>
<ul>
<li>检查是否正确安装了支持 GPU 的 ONNX Runtime 版本。</li>
</ul>
</li>
</ol>
<hr>
<p>如果有具体需求或问题，比如如何处理动态输入、支持特定硬件等，可以进一步讨论！</p>
<h2 id="C-onnxruntime-Ort-Env-详解"><a href="#C-onnxruntime-Ort-Env-详解" class="headerlink" title="C++ onnxruntime Ort::Env 详解"></a>C++ onnxruntime Ort::Env 详解</h2><p><code>Ort::Env</code> 是 ONNX Runtime 的 C++ API 中的一个核心类，用于初始化 ONNX Runtime 环境并设置相关的全局配置。ONNX Runtime 是一个高性能的推理引擎，<code>Ort::Env</code> 是使用 C++ API 的入口点之一。</p>
<p>以下是关于 <code>Ort::Env</code> 的详细解析：</p>
<hr>
<h3 id="Ort-Env-的功能"><a href="#Ort-Env-的功能" class="headerlink" title="Ort::Env 的功能"></a><strong><code>Ort::Env</code> 的功能</strong></h3><ol>
<li><p><strong>初始化 ONNX Runtime</strong>：</p>
<ul>
<li><code>Ort::Env</code> 是一个 RAII（资源获取即初始化）类，在创建时初始化 ONNX Runtime，销毁时释放相关资源。</li>
<li>在一个应用中，只需创建一个 <code>Ort::Env</code> 实例即可。</li>
</ul>
</li>
<li><p><strong>配置日志和线程池</strong>：</p>
<ul>
<li>允许设置日志级别、日志输出路径等全局配置。</li>
<li>配置线程池的行为（如并行度）。</li>
</ul>
</li>
<li><p><strong>管理会话的上下文环境</strong>：</p>
<ul>
<li>所有的推理会话 (<code>Ort::Session</code>) 都依赖于 <code>Ort::Env</code> 的存在。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Ort-Env-的构造函数"><a href="#Ort-Env-的构造函数" class="headerlink" title="Ort::Env 的构造函数"></a><strong><code>Ort::Env</code> 的构造函数</strong></h3><p><code>Ort::Env</code> 提供多种构造方式，允许用户灵活设置日志和多线程相关的参数。</p>
<h4 id="1-简单构造"><a href="#1-简单构造" class="headerlink" title="1. 简单构造"></a>1. <strong>简单构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntime&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>参数说明</strong>：</p>
<ul>
<li><code>ORT_LOGGING_LEVEL_WARNING</code>：日志级别，ONNX Runtime 只记录警告及更高级别的日志。</li>
<li><code>&quot;ONNXRuntime&quot;</code>：日志的默认记录器名称，用于区分日志源。</li>
</ul>
</li>
<li><p><strong>日志级别选项</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ORT_LOGGING_LEVEL_VERBOSE   <span class="comment">// 记录所有日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_INFO      <span class="comment">// 记录信息级别及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_WARNING   <span class="comment">// 记录警告及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_ERROR     <span class="comment">// 记录错误及以上的日志</span></span><br><span class="line">ORT_LOGGING_LEVEL_FATAL     <span class="comment">// 记录致命错误日志</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-高级构造"><a href="#2-高级构造" class="headerlink" title="2. 高级构造"></a>2. <strong>高级构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_VERBOSE, <span class="string">&quot;ONNXRuntime&quot;</span>, OrtThreadingOptions&#123;&#125;)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用 <code>OrtThreadingOptions</code> 配置线程池行为。</li>
<li><strong><code>OrtThreadingOptions</code> 示例</strong>：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OrtThreadingOptions* options = Ort::<span class="built_in">GetApi</span>().<span class="built_in">CreateThreadingOptions</span>();</span><br><span class="line">Ort::<span class="built_in">GetApi</span>().<span class="built_in">SetGlobalIntraOpThreadAffinity</span>(options, <span class="literal">true</span>);</span><br><span class="line">Ort::<span class="built_in">GetApi</span>().<span class="built_in">ReleaseThreadingOptions</span>(options);</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a><strong>常见用法</strong></h3><h4 id="初始化-ONNX-Runtime-环境"><a href="#初始化-ONNX-Runtime-环境" class="headerlink" title="初始化 ONNX Runtime 环境"></a><strong>初始化 ONNX Runtime 环境</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 其他操作，如创建会话等</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="结合推理会话"><a href="#结合推理会话" class="headerlink" title="结合推理会话"></a><strong>结合推理会话</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建推理会话</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>); <span class="comment">// 设置线程数</span></span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Ort-Env-的主要接口"><a href="#Ort-Env-的主要接口" class="headerlink" title="Ort::Env 的主要接口"></a><strong><code>Ort::Env</code> 的主要接口</strong></h3><h4 id="设置日志级别"><a href="#设置日志级别" class="headerlink" title="设置日志级别"></a><strong>设置日志级别</strong></h4><p>设置日志的输出级别。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.<span class="built_in">SetLogLevel</span>(ORT_LOGGING_LEVEL_INFO);</span><br></pre></td></tr></table></figure>

<h4 id="设置线程池选项"><a href="#设置线程池选项" class="headerlink" title="设置线程池选项"></a><strong>设置线程池选项</strong></h4><p>可以通过 <code>OrtThreadingOptions</code> 配置线程池行为，如线程数、线程亲和性等。</p>
<hr>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>单实例原则</strong>：</p>
<ul>
<li>一个应用中通常只需要一个 <code>Ort::Env</code> 实例。</li>
<li>多个实例可能导致资源冲突或性能问题。</li>
</ul>
</li>
<li><p><strong>线程安全性</strong>：</p>
<ul>
<li><code>Ort::Env</code> 本身是线程安全的，可以在多线程环境中安全使用。</li>
</ul>
</li>
<li><p><strong>销毁时机</strong>：</p>
<ul>
<li>当 <code>Ort::Env</code> 实例销毁时，ONNX Runtime 的资源会被自动释放。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="完整示例"><a href="#完整示例" class="headerlink" title="完整示例"></a><strong>完整示例</strong></h3><p>以下是一个完整的推理流程，展示了如何使用 <code>Ort::Env</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">2</span>);</span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型输入输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_input_nodes = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    <span class="type">size_t</span> num_output_nodes = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_input_nodes &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of outputs: &quot;</span> &lt;&lt; num_output_nodes &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>如果需要进一步了解某些特性（如日志配置、线程选项等）或其他问题，可以继续提问！</p>
<h2 id="Ort-SessionOptions-详解"><a href="#Ort-SessionOptions-详解" class="headerlink" title="Ort::SessionOptions 详解"></a>Ort::SessionOptions 详解</h2><p><code>Ort::SessionOptions</code> 是 ONNX Runtime C++ API 中的重要类，用于配置推理会话 (<code>Ort::Session</code>) 的运行行为。通过该类，可以设置线程数、优化级别、自定义执行提供程序等。</p>
<p>以下是对 <code>Ort::SessionOptions</code> 的详细解析：</p>
<hr>
<h3 id="Ort-SessionOptions-的功能"><a href="#Ort-SessionOptions-的功能" class="headerlink" title="Ort::SessionOptions 的功能"></a><strong><code>Ort::SessionOptions</code> 的功能</strong></h3><ol>
<li><p><strong>线程配置</strong>：</p>
<ul>
<li>控制并行线程数。</li>
<li>配置线程间的交互模式（单线程或多线程）。</li>
</ul>
</li>
<li><p><strong>图优化</strong>：</p>
<ul>
<li>优化计算图以提升推理性能。</li>
</ul>
</li>
<li><p><strong>执行提供程序</strong>：</p>
<ul>
<li>指定使用的计算设备（如 CPU、GPU）。</li>
</ul>
</li>
<li><p><strong>自定义选项</strong>：</p>
<ul>
<li>支持自定义分配器、上下文、会话行为等。</li>
</ul>
</li>
<li><p><strong>内存分配和 IO 绑定</strong>：</p>
<ul>
<li>允许用户自定义内存分配器和 IO 绑定。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="构造和基本用法"><a href="#构造和基本用法" class="headerlink" title="构造和基本用法"></a><strong>构造和基本用法</strong></h3><h4 id="默认构造函数"><a href="#默认构造函数" class="headerlink" title="默认构造函数"></a><strong>默认构造函数</strong></h4><p>创建一个默认的 <code>Ort::SessionOptions</code> 对象：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ort::SessionOptions session_options;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="主要成员函数"><a href="#主要成员函数" class="headerlink" title="主要成员函数"></a><strong>主要成员函数</strong></h3><h4 id="1-设置线程数"><a href="#1-设置线程数" class="headerlink" title="1. 设置线程数"></a><strong>1. 设置线程数</strong></h4><h5 id="SetIntraOpNumThreads"><a href="#SetIntraOpNumThreads" class="headerlink" title="SetIntraOpNumThreads"></a><strong><code>SetIntraOpNumThreads</code></strong></h5><ul>
<li>设置单个运算操作（如矩阵乘法）的最大线程数。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">4</span>);</span><br></pre></td></tr></table></figure></li>
<li>默认值为 0，表示由 ONNX Runtime 自动决定线程数。</li>
</ul>
<h5 id="SetInterOpNumThreads"><a href="#SetInterOpNumThreads" class="headerlink" title="SetInterOpNumThreads"></a><strong><code>SetInterOpNumThreads</code></strong></h5><ul>
<li>设置多个操作之间的最大线程数。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetInterOpNumThreads</span>(<span class="number">2</span>);</span><br></pre></td></tr></table></figure></li>
<li>默认值为 0，表示自动决定线程数。</li>
</ul>
<hr>
<h4 id="2-设置优化级别"><a href="#2-设置优化级别" class="headerlink" title="2. 设置优化级别"></a><strong>2. 设置优化级别</strong></h4><h5 id="SetGraphOptimizationLevel"><a href="#SetGraphOptimizationLevel" class="headerlink" title="SetGraphOptimizationLevel"></a><strong><code>SetGraphOptimizationLevel</code></strong></h5><ul>
<li>设置 ONNX 模型图的优化级别。</li>
<li>可用优化级别：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ORT_DISABLE_ALL            <span class="comment">// 禁用所有优化</span></span><br><span class="line">ORT_ENABLE_BASIC           <span class="comment">// 启用基本优化（默认值）</span></span><br><span class="line">ORT_ENABLE_EXTENDED        <span class="comment">// 启用扩展优化（包括内存优化）</span></span><br><span class="line">ORT_ENABLE_ALL             <span class="comment">// 启用所有优化</span></span><br></pre></td></tr></table></figure></li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="3-配置执行提供程序"><a href="#3-配置执行提供程序" class="headerlink" title="3. 配置执行提供程序"></a><strong>3. 配置执行提供程序</strong></h4><h5 id="CPU-提供程序"><a href="#CPU-提供程序" class="headerlink" title="CPU 提供程序"></a><strong>CPU 提供程序</strong></h5><ul>
<li>默认情况下，ONNX Runtime 使用 CPU 提供程序。</li>
<li>如果明确指定：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">AppendExecutionProvider_CPU</span>(<span class="number">0</span>); <span class="comment">// 0 表示使用默认配置</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="GPU-提供程序（CUDA）"><a href="#GPU-提供程序（CUDA）" class="headerlink" title="GPU 提供程序（CUDA）"></a><strong>GPU 提供程序（CUDA）</strong></h5><ul>
<li>需要安装 ONNX Runtime 的 CUDA 支持版本。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cuda/cuda_provider_factory.h&gt;</span></span></span><br><span class="line">Ort::SessionOptions session_options;</span><br><span class="line">session_options.<span class="built_in">AppendExecutionProvider_CUDA</span>(<span class="number">0</span>); <span class="comment">// 0 表示 GPU ID</span></span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="4-配置内存分配"><a href="#4-配置内存分配" class="headerlink" title="4. 配置内存分配"></a><strong>4. 配置内存分配</strong></h4><h5 id="EnableMemPattern"><a href="#EnableMemPattern" class="headerlink" title="EnableMemPattern"></a><strong><code>EnableMemPattern</code></strong></h5><ul>
<li>启用&#x2F;禁用内存模式优化（默认启用）。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">EnableMemPattern</span>(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="DisableMemPattern"><a href="#DisableMemPattern" class="headerlink" title="DisableMemPattern"></a><strong><code>DisableMemPattern</code></strong></h5><ul>
<li>禁用内存模式优化。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">DisableMemPattern</span>();</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="5-使用分配器"><a href="#5-使用分配器" class="headerlink" title="5. 使用分配器"></a><strong>5. 使用分配器</strong></h4><h5 id="设置分配器（可选）"><a href="#设置分配器（可选）" class="headerlink" title="设置分配器（可选）"></a><strong>设置分配器（可选）</strong></h5><p>用户可以指定自定义的内存分配器：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line">OrtAllocator* custom_allocator = ...; <span class="comment">// 用户自定义分配器</span></span><br><span class="line">session_options.<span class="built_in">AddConfigEntry</span>(<span class="string">&quot;memory.custom_allocator&quot;</span>, custom_allocator);</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="6-打开和关闭调试选项"><a href="#6-打开和关闭调试选项" class="headerlink" title="6. 打开和关闭调试选项"></a><strong>6. 打开和关闭调试选项</strong></h4><h5 id="启用调试"><a href="#启用调试" class="headerlink" title="启用调试"></a><strong>启用调试</strong></h5><p>用于调试模型推理的配置。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">EnableProfiling</span>(<span class="string">&quot;profiling_file.json&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="禁用调试"><a href="#禁用调试" class="headerlink" title="禁用调试"></a><strong>禁用调试</strong></h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_options.<span class="built_in">DisableProfiling</span>();</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="综合示例"><a href="#综合示例" class="headerlink" title="综合示例"></a><strong>综合示例</strong></h3><p>以下是一个使用 <code>Ort::SessionOptions</code> 的完整示例，展示了如何配置线程数、优化级别以及提供程序：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cuda/cuda_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置线程数</span></span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">4</span>);</span><br><span class="line">    session_options.<span class="built_in">SetInterOpNumThreads</span>(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置优化级别</span></span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加执行提供程序（使用 CUDA 或 CPU）</span></span><br><span class="line">    session_options.<span class="built_in">AppendExecutionProvider_CPU</span>(<span class="number">0</span>);  <span class="comment">// 使用 CPU 提供程序</span></span><br><span class="line">    <span class="comment">// session_options.AppendExecutionProvider_CUDA(0); // 若有 CUDA 支持，则使用 GPU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型并创建会话</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出模型的输入和输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    <span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Model Inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Model Outputs: &quot;</span> &lt;&lt; num_outputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>线程配置</strong>：</p>
<ul>
<li>如果设置了不合理的线程数（如过大或过小），可能导致性能下降或内存问题。</li>
</ul>
</li>
<li><p><strong>执行提供程序的顺序</strong>：</p>
<ul>
<li>ONNX Runtime 会按照添加的顺序选择第一个支持的提供程序。</li>
</ul>
</li>
<li><p><strong>图优化</strong>：</p>
<ul>
<li>高级优化（如 <code>ORT_ENABLE_ALL</code>）可能导致模型行为的细微变化，需要进行充分测试。</li>
</ul>
</li>
<li><p><strong>CUDA 提供程序的依赖</strong>：</p>
<ul>
<li>使用 GPU 时，请确保系统正确配置了 CUDA 和 cuDNN。</li>
</ul>
</li>
<li><p><strong>资源释放</strong>：</p>
<ul>
<li><code>Ort::SessionOptions</code> 在生命周期结束时会自动释放资源。</li>
</ul>
</li>
</ol>
<p>如果需要更深入了解某个功能或遇到问题，可以进一步探讨！</p>
<h2 id="Ort-Session-详解"><a href="#Ort-Session-详解" class="headerlink" title="Ort::Session 详解"></a>Ort::Session 详解</h2><p><code>Ort::Session</code> 是 ONNX Runtime C++ API 的核心类之一，负责加载 ONNX 模型并执行推理操作。一个 <code>Ort::Session</code> 对象对应一个加载的 ONNX 模型，并提供接口来查询模型信息和执行推理。</p>
<p>以下是对 <code>Ort::Session</code> 的详细解析：</p>
<hr>
<h2 id="Ort-Session-的主要功能"><a href="#Ort-Session-的主要功能" class="headerlink" title="Ort::Session 的主要功能"></a><strong><code>Ort::Session</code> 的主要功能</strong></h2><ol>
<li><p><strong>加载模型</strong>：</p>
<ul>
<li>使用 ONNX Runtime 环境 (<code>Ort::Env</code>) 和会话选项 (<code>Ort::SessionOptions</code>) 加载一个 ONNX 模型文件。</li>
</ul>
</li>
<li><p><strong>获取模型元信息</strong>：</p>
<ul>
<li>查询模型的输入和输出节点、数据类型、维度等。</li>
</ul>
</li>
<li><p><strong>执行推理</strong>：</p>
<ul>
<li>使用模型的输入数据进行推理，并返回结果。</li>
</ul>
</li>
<li><p><strong>管理资源</strong>：</p>
<ul>
<li><code>Ort::Session</code> 在其生命周期内负责维护模型相关的资源，并在销毁时自动释放。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="构造和基本用法-1"><a href="#构造和基本用法-1" class="headerlink" title="构造和基本用法"></a><strong>构造和基本用法</strong></h2><h3 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a><strong>构造函数</strong></h3><h4 id="1-默认构造"><a href="#1-默认构造" class="headerlink" title="1. 默认构造"></a>1. <strong>默认构造</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::Session <span class="title">session</span><span class="params">(env, <span class="string">&quot;model.onnx&quot;</span>, session_options)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>参数说明</strong>：<ul>
<li><code>env</code>：一个有效的 <code>Ort::Env</code> 实例。</li>
<li><code>&quot;model.onnx&quot;</code>：ONNX 模型文件的路径。</li>
<li><code>session_options</code>：一个 <code>Ort::SessionOptions</code> 对象，用于配置推理行为。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="主要成员函数-1"><a href="#主要成员函数-1" class="headerlink" title="主要成员函数"></a><strong>主要成员函数</strong></h2><h3 id="1-获取模型输入输出信息"><a href="#1-获取模型输入输出信息" class="headerlink" title="1. 获取模型输入输出信息"></a><strong>1. 获取模型输入输出信息</strong></h3><h4 id="GetInputCount-和-GetOutputCount"><a href="#GetInputCount-和-GetOutputCount" class="headerlink" title="GetInputCount 和 GetOutputCount"></a><strong><code>GetInputCount</code> 和 <code>GetOutputCount</code></strong></h4><ul>
<li>分别返回模型的输入和输出节点数量。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line"><span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="GetInputName-和-GetOutputName"><a href="#GetInputName-和-GetOutputName" class="headerlink" title="GetInputName 和 GetOutputName"></a><strong><code>GetInputName</code> 和 <code>GetOutputName</code></strong></h4><ul>
<li>返回指定输入或输出节点的名称。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="GetInputTypeInfo-和-GetOutputTypeInfo"><a href="#GetInputTypeInfo-和-GetOutputTypeInfo" class="headerlink" title="GetInputTypeInfo 和 GetOutputTypeInfo"></a><strong><code>GetInputTypeInfo</code> 和 <code>GetOutputTypeInfo</code></strong></h4><ul>
<li>返回输入或输出节点的数据类型和维度信息。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">auto</span> input_shape = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"><span class="keyword">auto</span> input_dims = input_shape.<span class="built_in">GetShape</span>(); <span class="comment">// 获取输入的维度</span></span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="2-推理操作"><a href="#2-推理操作" class="headerlink" title="2. 推理操作"></a><strong>2. 推理操作</strong></h3><h4 id="Run"><a href="#Run" class="headerlink" title="Run"></a><strong><code>Run</code></strong></h4><ul>
<li>使用输入数据执行推理并返回结果。</li>
<li><strong>原型</strong>：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;Ort::Value&gt; <span class="title">Run</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Ort::RunOptions&amp; run_options,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span>* input_names,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> Ort::Value* input_tensors,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> input_count,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span>* output_names,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> output_count</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li><strong>参数说明</strong>：<ul>
<li><code>run_options</code>：推理选项，通常可以传入默认值。</li>
<li><code>input_names</code>：输入节点名称数组。</li>
<li><code>input_tensors</code>：输入数据张量数组。</li>
<li><code>input_count</code>：输入节点数量。</li>
<li><code>output_names</code>：输出节点名称数组。</li>
<li><code>output_count</code>：输出节点数量。</li>
</ul>
</li>
<li><strong>返回值</strong>：<ul>
<li>推理结果的 <code>Ort::Value</code> 对象数组。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="综合示例：执行推理"><a href="#综合示例：执行推理" class="headerlink" title="综合示例：执行推理"></a><strong>综合示例：执行推理</strong></h3><p>以下代码展示了如何加载模型并执行推理：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXRuntimeExample&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置会话选项</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    session_options.<span class="built_in">SetIntraOpNumThreads</span>(<span class="number">1</span>);</span><br><span class="line">    session_options.<span class="built_in">SetGraphOptimizationLevel</span>(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询模型输入信息</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    <span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">auto</span> input_shape = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; input_tensor_values = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_dims = &#123;<span class="number">1</span>, <span class="number">4</span>&#125;; <span class="comment">// 假设输入是一个 1x4 的张量</span></span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        allocator, input_tensor_values.<span class="built_in">data</span>(), input_tensor_values.<span class="built_in">size</span>(),</span><br><span class="line">        input_dims.<span class="built_in">data</span>(), input_dims.<span class="built_in">size</span>()</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_tensors = session.<span class="built_in">Run</span>(</span><br><span class="line">        Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;,              <span class="comment">// 默认推理选项</span></span><br><span class="line">        &amp;input_name, &amp;input_tensor, <span class="number">1</span>,         <span class="comment">// 输入名和输入张量</span></span><br><span class="line">        &amp;output_name, <span class="number">1</span>                        <span class="comment">// 输出名</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析输出</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorMutableData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> output_shape = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : output_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Values: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetElementCount</span>(); ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; output_data[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>线程安全性</strong>：</p>
<ul>
<li><code>Ort::Session</code> 是线程安全的，可以在多线程环境中同时使用。</li>
</ul>
</li>
<li><p><strong>内存管理</strong>：</p>
<ul>
<li>使用 ONNX Runtime 的默认分配器 (<code>Ort::AllocatorWithDefaultOptions</code>) 来管理内存。</li>
<li>所有使用分配器分配的内存需要显式释放或交由分配器管理。</li>
</ul>
</li>
<li><p><strong>推理性能</strong>：</p>
<ul>
<li>使用 <code>Ort::SessionOptions</code> 配置优化选项（如线程数和图优化级别）可以显著提升推理性能。</li>
</ul>
</li>
<li><p><strong>模型输入&#x2F;输出验证</strong>：</p>
<ul>
<li>在执行推理之前，确保输入数据的维度和类型与模型的输入定义一致。</li>
</ul>
</li>
<li><p><strong>错误处理</strong>：</p>
<ul>
<li>如果模型文件路径错误或输入数据不匹配，ONNX Runtime 会抛出异常，需要进行捕获和处理。</li>
</ul>
</li>
</ol>
<hr>
<p>如果需要进一步深入了解 <code>Ort::Session</code> 或相关操作（如自定义提供程序、动态输入处理等），可以继续提问！</p>
<h2 id="Ort-AllocatorWithDefaultOptions-详解"><a href="#Ort-AllocatorWithDefaultOptions-详解" class="headerlink" title="Ort::AllocatorWithDefaultOptions 详解"></a>Ort::AllocatorWithDefaultOptions 详解</h2><p><code>Ort::AllocatorWithDefaultOptions</code> 是 ONNX Runtime C++ API 提供的一个简化工具类，用于分配和管理内存资源，尤其在操作模型输入输出信息时非常常用。</p>
<p>它封装了 ONNX Runtime 的默认分配器，简化了内存分配、释放的操作，是处理 <code>Ort::Session</code> 返回的字符串、张量等动态内存资源时的重要工具。</p>
<hr>
<h2 id="Ort-AllocatorWithDefaultOptions-的主要功能"><a href="#Ort-AllocatorWithDefaultOptions-的主要功能" class="headerlink" title="Ort::AllocatorWithDefaultOptions 的主要功能"></a><strong><code>Ort::AllocatorWithDefaultOptions</code> 的主要功能</strong></h2><ol>
<li><p><strong>动态分配内存</strong>：</p>
<ul>
<li>用于获取模型的输入输出名称、类型信息等，这些信息通常是动态分配的内存。</li>
</ul>
</li>
<li><p><strong>简化内存管理</strong>：</p>
<ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 自动处理分配的内存释放，避免手动管理内存的复杂性。</li>
</ul>
</li>
<li><p><strong>轻量级</strong>：</p>
<ul>
<li>这是一个无状态的 RAII（资源获取即初始化）类，创建和销毁的开销非常小。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="用法和接口"><a href="#用法和接口" class="headerlink" title="用法和接口"></a><strong>用法和接口</strong></h2><h3 id="1-构造函数"><a href="#1-构造函数" class="headerlink" title="1. 构造函数"></a><strong>1. 构造函数</strong></h3><p><code>Ort::AllocatorWithDefaultOptions</code> 无需任何参数即可默认构造：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-常用方法"><a href="#2-常用方法" class="headerlink" title="2. 常用方法"></a><strong>2. 常用方法</strong></h3><h4 id="operator-OrtAllocator"><a href="#operator-OrtAllocator" class="headerlink" title="operator OrtAllocator*"></a><strong><code>operator OrtAllocator*</code></strong></h4><ul>
<li>提供一个隐式转换操作符，将对象转换为底层的 <code>OrtAllocator*</code> 类型。</li>
<li>示例：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrtAllocator* raw_allocator = allocator;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="内存管理功能"><a href="#内存管理功能" class="headerlink" title="内存管理功能"></a><strong>内存管理功能</strong></h4><ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 自动调用 ONNX Runtime 的默认分配器接口，例如分配和释放内存：<ul>
<li><strong>分配内存</strong>：<code>allocator</code> 在调用 API（如 <code>GetInputName</code>）时会自动分配内存。</li>
<li><strong>释放内存</strong>：当 <code>allocator</code> 作用域结束时，分配的内存会被自动释放。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h2><h3 id="1-获取输入输出名称"><a href="#1-获取输入输出名称" class="headerlink" title="1. 获取输入输出名称"></a><strong>1. 获取输入输出名称</strong></h3><p>在获取模型输入和输出名称时，返回的名称是动态分配的内存，<code>Ort::AllocatorWithDefaultOptions</code> 自动管理这部分内存。</p>
<h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入名称</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出名称</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(<span class="number">0</span>, allocator);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output Name: &quot;</span> &lt;&lt; output_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-与动态分配相关的-API"><a href="#2-与动态分配相关的-API" class="headerlink" title="2. 与动态分配相关的 API"></a><strong>2. 与动态分配相关的 API</strong></h3><p>许多 ONNX Runtime 的 C++ API 使用动态内存分配，<code>Ort::AllocatorWithDefaultOptions</code> 是处理这些 API 的关键工具。例如：</p>
<h4 id="获取输入输出类型信息"><a href="#获取输入输出类型信息" class="headerlink" title="获取输入输出类型信息"></a>获取输入输出类型信息</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">auto</span> input_tensor_info = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取输入的维度信息</span></span><br><span class="line"><span class="keyword">auto</span> input_dims = input_tensor_info.<span class="built_in">GetShape</span>();</span><br></pre></td></tr></table></figure>

<h4 id="获取分配器指针"><a href="#获取分配器指针" class="headerlink" title="获取分配器指针"></a>获取分配器指针</h4><p>当需要与 ONNX Runtime 的低层 API 交互时，可以通过 <code>Ort::AllocatorWithDefaultOptions</code> 提供的 <code>OrtAllocator*</code> 指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrtAllocator* raw_allocator = allocator;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="内部工作原理"><a href="#内部工作原理" class="headerlink" title="内部工作原理"></a><strong>内部工作原理</strong></h2><p><code>Ort::AllocatorWithDefaultOptions</code> 基于 ONNX Runtime 的默认分配器实现，以下是它的工作机制：</p>
<ol>
<li><p><strong>初始化默认分配器</strong>：</p>
<ul>
<li>使用 ONNX Runtime 的 <code>OrtCreateAllocatorWithDefaultOptions</code> 方法创建分配器。</li>
<li>该分配器负责分配和释放内存。</li>
</ul>
</li>
<li><p><strong>自动管理内存</strong>：</p>
<ul>
<li>通过 RAII 模式，确保分配器的生命周期与 <code>Ort::AllocatorWithDefaultOptions</code> 实例绑定。</li>
<li>在分配器超出作用域时，自动释放所有相关资源。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>作用域管理</strong>：</p>
<ul>
<li>确保 <code>Ort::AllocatorWithDefaultOptions</code> 的生命周期覆盖所有需要使用其分配内存的地方。</li>
<li>在使用动态内存后，分配器超出作用域时会自动释放内存。</li>
</ul>
</li>
<li><p><strong>多线程使用</strong>：</p>
<ul>
<li>ONNX Runtime 的默认分配器是线程安全的，可以在多线程环境中使用。</li>
</ul>
</li>
<li><p><strong>避免手动释放</strong>：</p>
<ul>
<li>如果已经使用 <code>Ort::AllocatorWithDefaultOptions</code> 进行内存分配，无需手动释放内存，否则可能导致重复释放的错误。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="完整示例：从模型中提取输入输出信息"><a href="#完整示例：从模型中提取输入输出信息" class="headerlink" title="完整示例：从模型中提取输入输出信息"></a><strong>完整示例：从模型中提取输入输出信息</strong></h2><p>以下是一个综合示例，展示如何使用 <code>Ort::AllocatorWithDefaultOptions</code> 获取输入输出名称和类型信息：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入信息</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(i, allocator);</span><br><span class="line">        <span class="keyword">auto</span> input_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">        <span class="keyword">auto</span> input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;  Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出信息</span></span><br><span class="line">    <span class="type">size_t</span> num_outputs = session.<span class="built_in">GetOutputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of outputs: &quot;</span> &lt;&lt; num_outputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_outputs; ++i) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* output_name = session.<span class="built_in">GetOutputName</span>(i, allocator);</span><br><span class="line">        <span class="keyword">auto</span> output_info = session.<span class="built_in">GetOutputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = output_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">        <span class="keyword">auto</span> output_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Output &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; output_name &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;  Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : output_shape) std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::AllocatorWithDefaultOptions</code> 是处理动态分配内存的便捷工具类，尤其在获取输入输出名称等操作中必不可少。</li>
<li>它与 ONNX Runtime 的默认分配器紧密集成，简化了内存管理。</li>
<li>使用时只需关注业务逻辑，分配器的内存生命周期会自动管理。</li>
</ul>
<p>如果需要进一步探索其工作机制或更多使用场景，可以继续提问！</p>
<h2 id="Session-GetInputTypeInfo-详解"><a href="#Session-GetInputTypeInfo-详解" class="headerlink" title="Session::GetInputTypeInfo() 详解"></a>Session::GetInputTypeInfo() 详解</h2><p><code>Session::GetInputTypeInfo()</code> 是 ONNX Runtime C++ API 中的一个重要方法，用于获取模型某个输入节点的类型和形状信息。通过这个方法，你可以详细了解模型输入的张量数据类型和维度，从而确保输入数据格式符合模型的要求。</p>
<hr>
<h2 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Ort::TypeInfo <span class="title">GetInputTypeInfo</span><span class="params">(<span class="type">size_t</span> index)</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h3><ul>
<li><code>index</code>：输入节点的索引，从 <code>0</code> 开始。</li>
</ul>
<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>Ort::TypeInfo</code> 对象，包含输入节点的数据类型和形状信息。</li>
</ul>
<hr>
<h2 id="Ort-TypeInfo-的功能"><a href="#Ort-TypeInfo-的功能" class="headerlink" title="Ort::TypeInfo 的功能"></a><strong>Ort::TypeInfo 的功能</strong></h2><p><code>Ort::TypeInfo</code> 是 ONNX Runtime 用于描述节点数据类型和形状的类。通过 <code>TypeInfo</code> 对象，你可以进一步获取以下信息：</p>
<ol>
<li><strong>节点是否为张量</strong>。</li>
<li><strong>张量的数据类型</strong>。</li>
<li><strong>张量的形状</strong>。</li>
</ol>
<hr>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a><strong>使用方法</strong></h2><p>以下是一个完整的示例，展示如何使用 <code>GetInputTypeInfo()</code> 获取模型输入节点的类型和形状信息：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取模型的输入节点数量</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历输入节点，获取类型和形状信息</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="comment">// 获取输入节点名称</span></span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* input_name = session.<span class="built_in">GetInputName</span>(i, allocator);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input Name: &quot;</span> &lt;&lt; input_name &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入节点类型信息</span></span><br><span class="line">        Ort::TypeInfo input_type_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_type_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据类型</span></span><br><span class="line">        ONNXTensorElementDataType input_data_type = tensor_info.<span class="built_in">GetElementType</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Data Type: &quot;</span> &lt;&lt; input_data_type &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入形状</span></span><br><span class="line">        <span class="keyword">auto</span> input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) &#123;</span><br><span class="line">            std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Ort-TensorTypeAndShapeInfo-的功能"><a href="#Ort-TensorTypeAndShapeInfo-的功能" class="headerlink" title="Ort::TensorTypeAndShapeInfo 的功能"></a><strong>Ort::TensorTypeAndShapeInfo 的功能</strong></h2><p>通过 <code>TypeInfo</code> 对象的 <code>GetTensorTypeAndShapeInfo()</code> 方法，可以获取输入节点的详细张量信息，返回的是一个 <code>Ort::TensorTypeAndShapeInfo</code> 对象。</p>
<h3 id="1-获取张量数据类型"><a href="#1-获取张量数据类型" class="headerlink" title="1. 获取张量数据类型"></a><strong>1. 获取张量数据类型</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ONNXTensorElementDataType <span class="title">GetElementType</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回值是一个枚举值，表示张量的数据类型。例如：<ul>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code> 表示 <code>float</code> 类型。</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64</code> 表示 <code>int64_t</code> 类型。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-获取张量形状"><a href="#2-获取张量形状" class="headerlink" title="2. 获取张量形状"></a><strong>2. 获取张量形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">int64_t</span>&gt; <span class="title">GetShape</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回一个包含张量每个维度大小的 <code>std::vector&lt;int64_t&gt;</code>。</li>
<li>对于动态维度，返回的值为 <code>-1</code>。</li>
</ul>
<hr>
<h3 id="3-获取元素总数"><a href="#3-获取元素总数" class="headerlink" title="3. 获取元素总数"></a><strong>3. 获取元素总数</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">GetElementCount</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>返回张量中元素的总数量，等于所有维度大小的乘积。</li>
<li>如果张量包含动态维度，无法计算总数时会抛出异常。</li>
</ul>
<hr>
<h2 id="输出示例"><a href="#输出示例" class="headerlink" title="输出示例"></a><strong>输出示例</strong></h2><p>假设我们加载的模型有一个输入节点，名称为 <code>input_0</code>，数据类型为 <code>float</code>，形状为 <code>[1, 3, 224, 224]</code>。运行上述代码的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Number of inputs: 1</span><br><span class="line">Input Name: input_0</span><br><span class="line">Data Type: 1</span><br><span class="line">Shape: 1 3 224 224</span><br></pre></td></tr></table></figure>

<ul>
<li>数据类型 <code>1</code> 对应 <code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>。</li>
<li>形状 <code>[1, 3, 224, 224]</code> 表示张量的维度。</li>
</ul>
<hr>
<h2 id="注意事项-4"><a href="#注意事项-4" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h2><ol>
<li><p><strong>动态维度</strong>：</p>
<ul>
<li>如果模型的某个维度是动态的，返回的值将是 <code>-1</code>。在这种情况下，需要根据具体推理场景动态确定实际的维度。</li>
</ul>
</li>
<li><p><strong>节点索引越界</strong>：</p>
<ul>
<li>如果输入 <code>index</code> 超出范围（大于或等于输入节点数量），会抛出异常，需要确保索引有效。</li>
</ul>
</li>
<li><p><strong>数据类型匹配</strong>：</p>
<ul>
<li>确保推理时提供的输入张量数据类型与模型的输入要求一致，否则会导致运行时错误。</li>
</ul>
</li>
<li><p><strong>多输入模型</strong>：</p>
<ul>
<li>如果模型有多个输入，需要分别对每个输入调用 <code>GetInputTypeInfo()</code> 获取信息。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="综合小结"><a href="#综合小结" class="headerlink" title="综合小结"></a><strong>综合小结</strong></h2><ul>
<li><code>Session::GetInputTypeInfo()</code> 是了解模型输入节点属性的核心方法。</li>
<li>它返回一个 <code>Ort::TypeInfo</code> 对象，进一步提供张量的数据类型和形状信息。</li>
<li>使用时需要注意动态维度的处理和数据类型的匹配。</li>
</ul>
<p>如需进一步了解 <code>Ort::TypeInfo</code> 或动态输入处理，可以继续提问！</p>
<h2 id="TensorTypeAndShapeInfo-GetShape-详解"><a href="#TensorTypeAndShapeInfo-GetShape-详解" class="headerlink" title="TensorTypeAndShapeInfo::GetShape() 详解"></a>TensorTypeAndShapeInfo::GetShape() 详解</h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 是 ONNX Runtime C++ API 中的一个方法，主要用于获取 ONNX 模型的张量输入或输出的形状信息。它返回一个包含张量维度大小的 <code>std::vector&lt;int64_t&gt;</code>，这些维度可以是静态或动态的。</p>
<hr>
<h2 id="函数原型-1"><a href="#函数原型-1" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">int64_t</span>&gt; <span class="title">GetShape</span><span class="params">()</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a><strong>参数</strong></h3><ul>
<li>此方法无参数，它直接作用于 <code>TensorTypeAndShapeInfo</code> 对象。</li>
</ul>
<h3 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>std::vector&lt;int64_t&gt;</code>，其中每个元素表示张量在该维度上的大小。如果维度是动态的，该维度的大小为 <code>-1</code>。</li>
</ul>
<hr>
<h2 id="功能和用途"><a href="#功能和用途" class="headerlink" title="功能和用途"></a><strong>功能和用途</strong></h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 方法的主要作用是返回张量的形状信息，这些信息对于输入数据的格式化和验证至关重要。它允许你获取张量在各个维度上的大小，通常用于确定输入数据或输出数据是否符合模型的要求。</p>
<ul>
<li><strong>静态维度</strong>：对于静态维度（在模型加载时确定的维度），返回的是实际的维度大小。</li>
<li><strong>动态维度</strong>：如果张量的某些维度是动态的（例如在模型训练时，模型的输入维度没有固定大小），返回的维度值为 <code>-1</code>，表示该维度的大小是动态变化的，通常需要根据输入数据在推理时动态确定。</li>
</ul>
<hr>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a><strong>使用示例</strong></h2><p>假设你已经加载了一个 ONNX 模型并且调用了 <code>GetInputTypeInfo()</code> 或 <code>GetOutputTypeInfo()</code> 来获取模型输入或输出的张量类型信息。接下来，你可以使用 <code>GetShape()</code> 获取张量的形状。</p>
<h3 id="示例代码：获取输入形状"><a href="#示例代码：获取输入形状" class="headerlink" title="示例代码：获取输入形状"></a><strong>示例代码：获取输入形状</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;Example&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">session</span><span class="params">(env, model_path, session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输入数量</span></span><br><span class="line">    <span class="type">size_t</span> num_inputs = session.<span class="built_in">GetInputCount</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of inputs: &quot;</span> &lt;&lt; num_inputs &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历输入节点，获取形状</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        <span class="comment">// 获取输入类型信息</span></span><br><span class="line">        Ort::TypeInfo input_type_info = session.<span class="built_in">GetInputTypeInfo</span>(i);</span><br><span class="line">        <span class="keyword">auto</span> tensor_info = input_type_info.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取张量形状</span></span><br><span class="line">        std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Input &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; shape: &quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> dim : input_shape) &#123;</span><br><span class="line">            std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="输出示例-1"><a href="#输出示例-1" class="headerlink" title="输出示例"></a><strong>输出示例</strong></h3><p>假设模型的输入是一个 4D 张量，形状为 <code>[1, 3, 224, 224]</code>，那么输出将类似于：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Number of inputs: 1</span><br><span class="line">Input 0 shape: 1 3 224 224</span><br></pre></td></tr></table></figure>

<p>如果输入的某个维度是动态的，例如批量大小（batch size）是动态的，那么输出可能是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Input 0 shape: -1 3 224 224</span><br></pre></td></tr></table></figure>

<p><code>-1</code> 表示批量大小是动态的，推理时由输入数据的批量大小来确定。</p>
<hr>
<h2 id="GetShape-的返回值解析"><a href="#GetShape-的返回值解析" class="headerlink" title="GetShape() 的返回值解析"></a><strong><code>GetShape()</code> 的返回值解析</strong></h2><p><code>GetShape()</code> 返回的 <code>std::vector&lt;int64_t&gt;</code> 每个元素表示一个维度的大小。下面是关于维度和返回值的一些常见情况：</p>
<h3 id="1-静态维度"><a href="#1-静态维度" class="headerlink" title="1. 静态维度"></a><strong>1. 静态维度</strong></h3><p>如果模型的输入或输出具有固定的维度，则 <code>GetShape()</code> 返回的是该张量的具体形状。例如：</p>
<ul>
<li><p>对于一个 <code>1x3x224x224</code> 的图像张量（通常用于图像分类任务），<code>GetShape()</code> 返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于一个二维张量（例如一个大小为 <code>100x50</code> 的矩阵），<code>GetShape()</code> 返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">100</span>, <span class="number">50</span>&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-动态维度"><a href="#2-动态维度" class="headerlink" title="2. 动态维度"></a><strong>2. 动态维度</strong></h3><p>如果模型的某些维度是动态的，则返回值中的该维度将是 <code>-1</code>，表示该维度在推理时会根据实际输入数据动态确定。常见的动态维度包括批量大小（batch size）。</p>
<ul>
<li><p>假设模型的输入形状是 <code>[batch_size, 3, 224, 224]</code>，其中 <code>batch_size</code> 是动态的。<code>GetShape()</code> 可能返回：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">-1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>-1</code> 表示 <code>batch_size</code> 是动态的，具体的值将在实际推理时由输入数据的大小确定。</p>
</li>
<li><p>如果模型具有 <code>sequence_length</code> 等动态维度，类似地，返回的形状可能为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">10</span>, <span class="number">-1</span>&#125;  <span class="comment">// 10 是固定维度，-1 表示第二维是动态的</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-无效维度"><a href="#3-无效维度" class="headerlink" title="3. 无效维度"></a><strong>3. 无效维度</strong></h3><p>如果张量没有形状（例如是一个标量），<code>GetShape()</code> 返回的向量为空：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#125;  <span class="comment">// 无形状的张量</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="常见问题和注意事项"><a href="#常见问题和注意事项" class="headerlink" title="常见问题和注意事项"></a><strong>常见问题和注意事项</strong></h2><ol>
<li><p><strong>动态维度的处理</strong>：</p>
<ul>
<li>对于动态维度，返回的形状中相应位置的值为 <code>-1</code>，你无法直接知道该维度的大小。通常，实际的维度会在推理时根据输入的大小动态确定。</li>
</ul>
</li>
<li><p><strong>如何获取元素数量</strong>：</p>
<ul>
<li>如果模型的输入具有静态形状，可以通过 <code>GetShape()</code> 返回的维度大小计算张量的元素数量。</li>
<li>例如，假设返回的形状是 <code>[1, 3, 224, 224]</code>，那么元素总数为 <code>1 * 3 * 224 * 224 = 150528</code>。</li>
</ul>
</li>
<li><p><strong>性能优化</strong>：</p>
<ul>
<li>当处理动态维度时，你可能需要动态调整输入的大小。通过 <code>GetShape()</code> 获取输入形状后，你可以根据实际情况调整输入数据的维度。</li>
</ul>
</li>
<li><p><strong>错误处理</strong>：</p>
<ul>
<li>如果在尝试获取形状时遇到问题（如 <code>session.GetInputTypeInfo()</code> 返回错误），确保模型输入的维度与预期一致。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><p><code>TensorTypeAndShapeInfo::GetShape()</code> 是 ONNX Runtime 中获取张量形状信息的关键方法。它返回张量的维度信息，帮助你在执行推理之前确认输入和输出数据的结构。了解如何处理静态和动态维度，对于正确地为模型准备输入数据至关重要。</p>
<p>如果你对 <code>GetShape()</code> 或其他 ONNX Runtime 方法有进一步的疑问，欢迎继续提问！</p>
<h2 id="1-3-224-224-表示的什么"><a href="#1-3-224-224-表示的什么" class="headerlink" title="[1, 3, 224, 224] 表示的什么"></a>[1, 3, 224, 224] 表示的什么</h2><p><code>[1, 3, 224, 224]</code> 是一个形状（Shape）表示，通常用于描述图像张量的维度。在计算机视觉和深度学习任务中，这种表示方法是非常常见的。让我们逐步分析它：</p>
<h3 id="1-3-224-224-解释"><a href="#1-3-224-224-解释" class="headerlink" title="[1, 3, 224, 224] 解释"></a><strong><code>[1, 3, 224, 224]</code> 解释</strong></h3><ul>
<li><p><strong>1</strong>：批量大小（Batch Size）</p>
<ul>
<li>这个数字表示图像数据批次中包含的图像数量。在这个例子中，批量大小为 <code>1</code>，意味着这只是一个单独的图像。批量大小是深度学习模型训练和推理过程中常用的概念。</li>
</ul>
</li>
<li><p><strong>3</strong>：颜色通道数（Channels）</p>
<ul>
<li>这个数字表示图像的颜色通道数。在这种情况下，<code>3</code> 代表图像是 RGB 彩色图像，意味着图像包含三个颜色通道：红色（Red）、绿色（Green）和蓝色（Blue）。每个通道包含一个灰度值，表示颜色的强度。</li>
</ul>
</li>
<li><p><strong>224</strong>：图像的高度（Height）</p>
<ul>
<li>这个数字表示图像的垂直尺寸，单位是像素。在这个例子中，图像的高度为 <code>224</code> 像素，表示图像的垂直方向有 224 个像素点。</li>
</ul>
</li>
<li><p><strong>224</strong>：图像的宽度（Width）</p>
<ul>
<li>这个数字表示图像的水平尺寸，单位也是像素。在这个例子中，图像的宽度为 <code>224</code> 像素，表示图像的水平方向有 224 个像素点。</li>
</ul>
</li>
</ul>
<h3 id="综合解释"><a href="#综合解释" class="headerlink" title="综合解释"></a><strong>综合解释</strong></h3><ul>
<li><code>1</code>：批量大小为 <code>1</code>，意味着只有一个图像。</li>
<li><code>3</code>：该图像为彩色图像，采用 RGB 色彩空间，因此有 3 个颜色通道。</li>
<li><code>224</code> 和 <code>224</code>：该图像的尺寸为 224x224 像素，表示图像的高度和宽度都是 224 像素。</li>
</ul>
<p>因此，<code>[1, 3, 224, 224]</code> 表示一个单独的 RGB 彩色图像，尺寸为 224x224 像素，具有 3 个颜色通道（红、绿、蓝）。这种形状常见于深度学习任务，特别是图像分类或卷积神经网络（CNN）的输入。</p>
<hr>
<h3 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a><strong>使用场景</strong></h3><p>这个张量形状通常用于深度学习框架，如 PyTorch、ONNX 或 TensorFlow，在图像数据的训练和推理过程中经常出现。例如，如果你使用一个深度学习模型来分类图像，模型通常期望输入的形状是 <code>[B, C, H, W]</code>，其中：</p>
<ul>
<li><code>B</code> 是批量大小。</li>
<li><code>C</code> 是颜色通道数。</li>
<li><code>H</code> 是图像的高度。</li>
<li><code>W</code> 是图像的宽度。</li>
</ul>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>[1, 3, 224, 224]</code> 表示一个具有 224x224 像素尺寸的 RGB 彩色图像，批量大小为 1。</li>
</ul>
<h2 id="什么是图像张量"><a href="#什么是图像张量" class="headerlink" title="什么是图像张量"></a>什么是图像张量</h2><p>图像张量（Image Tensor）是一个用于表示图像数据的多维数组（张量），通常用于深度学习、计算机视觉和其他机器学习任务。张量是一个数学概念，广泛用于表示高维数据。图像张量是张量在计算机视觉领域的具体应用，它通过数值数组的形式表示图像。</p>
<p>在深度学习和计算机视觉任务中，图像通常作为输入数据，通过神经网络进行处理和分析。为了能够处理图像，图像数据需要以一种数值形式表示，而这正是张量的作用所在。</p>
<hr>
<h2 id="图像张量的基本结构"><a href="#图像张量的基本结构" class="headerlink" title="图像张量的基本结构"></a><strong>图像张量的基本结构</strong></h2><p>图像张量的维度（Shape）通常与图像的表示方式、颜色通道数、分辨率和数据类型等因素相关。常见的图像张量表示方式包括以下几种：</p>
<h3 id="1-彩色图像张量"><a href="#1-彩色图像张量" class="headerlink" title="1. 彩色图像张量"></a><strong>1. 彩色图像张量</strong></h3><p>对于彩色图像，通常采用<strong>RGB</strong>（红绿蓝）颜色空间来表示。每个颜色通道的值通常是一个整数（比如 <code>0</code> 到 <code>255</code> 之间）或浮动的值（例如标准化到 <code>[0, 1]</code> 的浮动值）。</p>
<ul>
<li><strong>形状</strong>：<code>[C, H, W]</code><ul>
<li><strong>C</strong>：颜色通道数（通常是 <code>3</code>，表示 RGB 三个颜色通道）。</li>
<li><strong>H</strong>：图像的高度（即图像的行数，表示图像的垂直尺寸）。</li>
<li><strong>W</strong>：图像的宽度（即图像的列数，表示图像的水平尺寸）。</li>
</ul>
</li>
</ul>
<p>例如，一个大小为 <code>224x224</code> 像素的 RGB 彩色图像可以用一个形状为 <code>[3, 224, 224]</code> 的张量表示。</p>
<h3 id="2-灰度图像张量"><a href="#2-灰度图像张量" class="headerlink" title="2. 灰度图像张量"></a><strong>2. 灰度图像张量</strong></h3><p>对于灰度图像，通常只有一个颜色通道，即每个像素只有一个灰度值，表示图像的亮度。</p>
<ul>
<li><strong>形状</strong>：<code>[1, H, W]</code><ul>
<li><strong>1</strong>：表示灰度图像只有一个颜色通道。</li>
<li><strong>H</strong>：图像的高度。</li>
<li><strong>W</strong>：图像的宽度。</li>
</ul>
</li>
</ul>
<p>例如，一个 <code>28x28</code> 像素的灰度图像可以用一个形状为 <code>[1, 28, 28]</code> 的张量表示。</p>
<h3 id="3-批量图像张量"><a href="#3-批量图像张量" class="headerlink" title="3. 批量图像张量"></a><strong>3. 批量图像张量</strong></h3><p>在深度学习中，通常一次性处理多个图像。因此，图像张量通常以批量（batch）的形式输入到神经网络中。</p>
<ul>
<li><strong>形状</strong>：<code>[B, C, H, W]</code><ul>
<li><strong>B</strong>：批量大小（batch size），表示一次处理多少图像。</li>
<li><strong>C</strong>：颜色通道数（通常是 <code>3</code> 表示 RGB，或 <code>1</code> 表示灰度）。</li>
<li><strong>H</strong>：图像的高度。</li>
<li><strong>W</strong>：图像的宽度。</li>
</ul>
</li>
</ul>
<p>例如，一个包含 32 张 <code>224x224</code> 的 RGB 彩色图像的批量张量，可以表示为形状为 <code>[32, 3, 224, 224]</code> 的张量。</p>
<hr>
<h2 id="图像张量的存储格式"><a href="#图像张量的存储格式" class="headerlink" title="图像张量的存储格式"></a><strong>图像张量的存储格式</strong></h2><p>图像张量的存储格式依赖于数据的顺序和布局。最常见的两种存储格式是：</p>
<ol>
<li><p><strong>CHW（Channel, Height, Width）格式</strong>：</p>
<ul>
<li>这是深度学习框架如 PyTorch、ONNX、Caffe 中常见的图像张量格式。颜色通道在前，接着是高度和宽度。</li>
<li>示例：<code>[3, 224, 224]</code> 表示一个 RGB 彩色图像，具有 3 个通道，尺寸为 <code>224x224</code>。</li>
</ul>
</li>
<li><p><strong>HWC（Height, Width, Channel）格式</strong>：</p>
<ul>
<li>这是图像处理库如 OpenCV 和 TensorFlow 中常见的图像表示格式。图像的高度和宽度在前，颜色通道在后。</li>
<li>示例：<code>[224, 224, 3]</code> 表示一个 RGB 彩色图像，具有 3 个通道，尺寸为 <code>224x224</code>。</li>
</ul>
</li>
</ol>
<p>不同框架和工具可能采用不同的存储格式，但它们都可以通过相应的工具进行转换。</p>
<hr>
<h2 id="图像张量的用途"><a href="#图像张量的用途" class="headerlink" title="图像张量的用途"></a><strong>图像张量的用途</strong></h2><p>图像张量在计算机视觉和深度学习中的用途非常广泛，主要包括：</p>
<ol>
<li><p><strong>输入神经网络</strong>：</p>
<ul>
<li>图像张量是深度学习模型，特别是卷积神经网络（CNN）的输入数据。通过神经网络的前向传播，模型会根据图像的特征进行分类、检测、分割等任务。</li>
</ul>
</li>
<li><p><strong>图像预处理</strong>：</p>
<ul>
<li>在将图像输入神经网络之前，通常会进行一些预处理操作，如调整图像的大小、标准化颜色值、数据增强（例如旋转、裁剪、翻转等）。这些操作通常会改变图像张量的形状或数值。</li>
</ul>
</li>
<li><p><strong>图像生成</strong>：</p>
<ul>
<li>在生成模型（如生成对抗网络 GAN）中，图像张量不仅作为输入，还作为模型的输出，生成新的图像。</li>
</ul>
</li>
<li><p><strong>图像变换</strong>：</p>
<ul>
<li>图像张量在图像处理任务中，涉及图像的各种变换操作，比如图像的滤波、边缘检测、图像增强等。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="图像张量的示例"><a href="#图像张量的示例" class="headerlink" title="图像张量的示例"></a><strong>图像张量的示例</strong></h2><p>以下是一个示例，展示如何使用 Python 和 NumPy 创建图像张量（以 NumPy 为例，虽然 Python 是常见的深度学习语言，但 ONNX Runtime 或 PyTorch 等框架也使用类似的张量表示方法）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 224x224 的 RGB 图像张量（3 个颜色通道）</span></span><br><span class="line">image_tensor = np.random.rand(<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 随机生成一个图像张量</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)  <span class="comment"># 输出张量的形状，应该是 (3, 224, 224)</span></span><br></pre></td></tr></table></figure>

<p>如果是批量图像（例如批量大小为 32）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个包含 32 张 224x224 RGB 图像的张量</span></span><br><span class="line">batch_image_tensor = np.random.rand(<span class="number">32</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(batch_image_tensor.shape)  <span class="comment"># 输出张量的形状，应该是 (32, 3, 224, 224)</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h2><ul>
<li>图像张量是一个多维数组，表示图像的数值数据，通常用于深度学习中的输入数据。</li>
<li>图像张量的常见形状包括 <code>[C, H, W]</code>（单张图像）和 <code>[B, C, H, W]</code>（批量图像）。</li>
<li>图像张量可以是彩色图像（RGB）或灰度图像，并通过颜色通道、图像的高度和宽度来表示。</li>
<li>在深度学习和计算机视觉任务中，图像张量是神经网络输入和输出的重要形式。</li>
</ul>
<p>图像张量为机器学习模型提供了可以处理的格式，是许多计算机视觉任务的基础。</p>
<h2 id="C-onnxruntime-Ort-Value-详解"><a href="#C-onnxruntime-Ort-Value-详解" class="headerlink" title="C++ onnxruntime Ort::Value 详解"></a>C++ onnxruntime Ort::Value 详解</h2><p>在 ONNX Runtime 中，<code>Ort::Value</code> 是一个非常重要的类，用于表示张量（tensor）数据。它封装了模型的输入和输出数据，并提供了一种与 ONNX Runtime 进行交互的方式。</p>
<h3 id="Ort-Value-介绍"><a href="#Ort-Value-介绍" class="headerlink" title="Ort::Value 介绍"></a><strong>Ort::Value 介绍</strong></h3><p><code>Ort::Value</code> 是 ONNX Runtime API 中用于表示数据的一个类，它可以用于存储模型的输入和输出数据，并为数据提供访问接口。你可以将数据包装到 <code>Ort::Value</code> 中，从而通过 ONNX Runtime 执行推理操作。它提供了对张量的访问、内存管理以及各种用于操作张量数据的功能。</p>
<h3 id="主要功能-1"><a href="#主要功能-1" class="headerlink" title="主要功能"></a><strong>主要功能</strong></h3><ol>
<li><p><strong>张量表示</strong>：<code>Ort::Value</code> 用于表示和存储模型的输入、输出数据，这些数据通常是多维数组，表示为张量（Tensor）。</p>
</li>
<li><p><strong>数据类型支持</strong>：<code>Ort::Value</code> 支持各种数据类型，如浮动类型（<code>float</code>、<code>double</code>）、整数类型（<code>int</code>、<code>long</code>）、布尔类型等。它也支持不同的张量格式，例如：<code>float32</code>、<code>int64</code> 等。</p>
</li>
<li><p><strong>内存管理</strong>：<code>Ort::Value</code> 内部管理内存，并负责在不再需要时释放内存。ONNX Runtime 会通过 RAII（资源获取即初始化）管理内存的生命周期。</p>
</li>
</ol>
<h3 id="创建-Ort-Value"><a href="#创建-Ort-Value" class="headerlink" title="创建 Ort::Value"></a><strong>创建 Ort::Value</strong></h3><p><code>Ort::Value</code> 可以通过几种不同的方式创建，以下是一些常见的方法：</p>
<ol>
<li><p><strong>从原始数据创建</strong>：<br>使用 <code>Ort::Value::CreateTensor</code> 或类似函数，可以从原始内存或数组创建一个张量。常见的场景是创建张量并作为输入传递给模型。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">std::vector&lt;<span class="type">float</span>&gt; input_data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>&#125;;</span><br><span class="line">std::array&lt;<span class="type">int64_t</span>, 1&gt; input_shape = &#123;<span class="number">3</span>&#125;;</span><br><span class="line">Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(allocator, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>从现有的内存（例如：std::vector）创建</strong>：<br>可以直接使用 <code>Ort::Value</code> 来包装现有的数据。可以通过 <code>Ort::Value::CreateTensor</code> 方法指定数据类型和形状。</p>
</li>
<li><p><strong>从 C++ 标准容器（如 <code>std::vector</code> 或 <code>std::array</code>）创建张量</strong>：<br><code>Ort::Value</code> 可以直接从容器对象创建张量，方便快速构建推理的输入数据。</p>
</li>
</ol>
<h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a><strong>常用方法</strong></h3><h4 id="1-获取数据类型"><a href="#1-获取数据类型" class="headerlink" title="1. 获取数据类型"></a>1. <strong>获取数据类型</strong></h4><p><code>Ort::Value</code> 提供了方法来获取存储的数据类型。例如，获取张量的数据类型（如 <code>float32</code>，<code>int64</code> 等）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ONNXTensorElementDataType type = input_tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetElementType</span>();</span><br></pre></td></tr></table></figure>

<h4 id="2-获取张量的形状"><a href="#2-获取张量的形状" class="headerlink" title="2. 获取张量的形状"></a>2. <strong>获取张量的形状</strong></h4><p>通过 <code>Ort::Value</code> 的 <code>GetTensorTypeAndShapeInfo()</code> 方法，你可以获取张量的形状（即它的维度）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ort::TensorTypeAndShapeInfo tensor_info = input_tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">std::vector&lt;<span class="type">int64_t</span>&gt; shape = tensor_info.<span class="built_in">GetShape</span>();</span><br></pre></td></tr></table></figure>

<h4 id="3-获取数据指针"><a href="#3-获取数据指针" class="headerlink" title="3. 获取数据指针"></a>3. <strong>获取数据指针</strong></h4><p><code>Ort::Value</code> 也允许你访问底层数据。可以使用 <code>GetTensorData</code> 来获取存储张量数据的指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* float_data = input_tensor.<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br></pre></td></tr></table></figure>

<h4 id="4-设置张量的值"><a href="#4-设置张量的值" class="headerlink" title="4. 设置张量的值"></a>4. <strong>设置张量的值</strong></h4><p>你可以通过 <code>Ort::Value</code> 设置张量的数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_tensor.<span class="built_in">SetTensorData</span>&lt;<span class="type">float</span>&gt;(input_data.<span class="built_in">data</span>());</span><br></pre></td></tr></table></figure>

<h4 id="5-转换为其他类型"><a href="#5-转换为其他类型" class="headerlink" title="5. 转换为其他类型"></a>5. <strong>转换为其他类型</strong></h4><p>通过 ONNX Runtime 提供的 API，你可以将 <code>Ort::Value</code> 转换为其他类型（例如：从张量转为 NumPy 数组，或通过其他方式访问数据）。</p>
<h3 id="示例代码：推理输入和输出"><a href="#示例代码：推理输入和输出" class="headerlink" title="示例代码：推理输入和输出"></a><strong>示例代码：推理输入和输出</strong></h3><p>下面是一个基本示例，展示了如何使用 <code>Ort::Value</code> 来执行 ONNX 模型推理：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_ptr.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/ort_value.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个 SessionOptions 对象</span></span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载 ONNX 模型</span></span><br><span class="line">    std::string model_path = <span class="string">&quot;model.onnx&quot;</span>;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">onnx_session</span><span class="params">(env, model_path.c_str(), session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 准备输入数据 (例如: 3x3 数字)</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; input_data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>, <span class="number">5.0f</span>, <span class="number">6.0f</span>, <span class="number">7.0f</span>, <span class="number">8.0f</span>, <span class="number">9.0f</span>&#125;;</span><br><span class="line">    std::array&lt;<span class="type">int64_t</span>, 2&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">9</span>&#125;;  <span class="comment">// 假设这是一个 1x9 的输入</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Ort::Value 对象作为输入张量</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line">    Ort::Value input_tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(allocator, input_data.<span class="built_in">data</span>(), input_data.<span class="built_in">size</span>(), input_shape.<span class="built_in">data</span>(), input_shape.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 Ort::Value 对象作为输出张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_tensors;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    onnx_session.<span class="built_in">Run</span>(Ort::RunOptions&#123;<span class="literal">nullptr</span>&#125;, </span><br><span class="line">                     &amp;input_name, &amp;input_tensor, <span class="number">1</span>, <span class="comment">// 输入</span></span><br><span class="line">                     &amp;output_name, &amp;output_tensors, <span class="number">1</span>); <span class="comment">// 输出</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    <span class="type">float</span>* output_data = output_tensors[<span class="number">0</span>].<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference result: &quot;</span> &lt;&lt; output_data[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::Value</code> 是 ONNX Runtime 用于表示和操作张量的核心类之一。</li>
<li>它提供了一种接口来管理输入和输出数据，支持多种数据类型和形状。</li>
<li>你可以通过 <code>Ort::Value</code> 来包装原始数据（如 <code>std::vector</code>），并将其传递给模型进行推理。</li>
<li>通过 <code>Ort::Value</code> 提供的接口，你可以方便地访问张量的数据、形状以及数据类型。</li>
</ul>
<p><code>Ort::Value</code> 的使用使得与 ONNX Runtime 进行交互变得简便而高效，能够帮助开发者轻松处理模型推理所需的各种数据。</p>
<h2 id="Ort-Value-CreateTensor-详解"><a href="#Ort-Value-CreateTensor-详解" class="headerlink" title="Ort::Value::CreateTensor 详解"></a>Ort::Value::CreateTensor 详解</h2><p><code>Ort::Value::CreateTensor</code> 是 ONNX Runtime 中用于创建张量（Tensor）数据的一个静态函数。张量是 ONNX Runtime 中数据的核心表示形式，通常用于表示模型的输入和输出。通过 <code>CreateTensor</code>，你可以将数据封装成 <code>Ort::Value</code> 对象，供模型推理使用。</p>
<h3 id="函数原型-2"><a href="#函数原型-2" class="headerlink" title="函数原型"></a><strong>函数原型</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">static</span> Ort::Value <span class="title">CreateTensor</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Ort::Allocator&amp; allocator, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">void</span>* data, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> size, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int64_t</span>* shape, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> shape_len, </span></span></span><br><span class="line"><span class="params"><span class="function">    ONNXTensorElementDataType type)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a><strong>参数详解</strong></h3><ol>
<li><p>**<code>allocator</code>**（类型：<code>Ort::Allocator&amp;</code>）：</p>
<ul>
<li>用于分配内存的分配器，ONNX Runtime 会使用它来管理张量的内存。</li>
<li>推荐使用 <code>Ort::AllocatorWithDefaultOptions</code> 来创建一个默认的内存分配器。</li>
</ul>
</li>
<li><p>**<code>data</code>**（类型：<code>void*</code>）：</p>
<ul>
<li>一个指向存储数据的内存块的指针。数据将被存储在该指针指向的内存位置。</li>
<li>你需要将数据放入这块内存中，数据的类型和结构应该符合所需的张量类型和形状。</li>
</ul>
</li>
<li><p>**<code>size</code>**（类型：<code>size_t</code>）：</p>
<ul>
<li>数据的总大小，以字节为单位。</li>
<li><code>size</code> 应该等于张量的元素数量乘以每个元素的字节大小。例如，如果每个元素是 <code>float</code>（4 字节），而张量包含 100 个元素，那么 <code>size</code> 应该是 <code>100 * 4</code>。</li>
</ul>
</li>
<li><p>**<code>shape</code>**（类型：<code>const int64_t*</code>）：</p>
<ul>
<li>张量的形状，即每个维度的大小。形状是一个整型数组，表示张量的多维尺寸。</li>
<li>例如，对于一个 2D 张量，形状可能是 <code>&#123;3, 4&#125;</code>，表示该张量有 3 行 4 列。</li>
</ul>
</li>
<li><p>**<code>shape_len</code>**（类型：<code>size_t</code>）：</p>
<ul>
<li>张量形状的维度数量，即 <code>shape</code> 数组的长度。</li>
<li>例如，如果张量是 2D，<code>shape_len</code> 应该是 2；如果是 3D，<code>shape_len</code> 应该是 3，依此类推。</li>
</ul>
</li>
<li><p>**<code>type</code>**（类型：<code>ONNXTensorElementDataType</code>）：</p>
<ul>
<li>张量的数据类型，指定张量中元素的类型。ONNX 允许多种数据类型，包括：<ul>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>（<code>float</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32</code>（<code>int32</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64</code>（<code>int64</code> 类型）</li>
<li><code>ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8</code>（<code>uint8</code> 类型）</li>
<li>等等。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="返回值-2"><a href="#返回值-2" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li>返回一个 <code>Ort::Value</code> 对象，表示创建的张量。</li>
<li><code>Ort::Value</code> 对象封装了张量的数据和形状，并可以在后续的推理过程中作为输入或输出使用。</li>
</ul>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h3><p>以下是一个创建张量并将其用于模型推理的完整示例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/ort_value.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_ptr.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/common.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/onnxruntime_typeinfo.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个默认分配器</span></span><br><span class="line">    Ort::AllocatorWithDefaultOptions allocator;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个简单的 2D 张量数据（例如：2x3 的矩阵）</span></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; data = &#123;<span class="number">1.0f</span>, <span class="number">2.0f</span>, <span class="number">3.0f</span>, <span class="number">4.0f</span>, <span class="number">5.0f</span>, <span class="number">6.0f</span>&#125;;</span><br><span class="line">    std::array&lt;<span class="type">int64_t</span>, 2&gt; shape = &#123;<span class="number">2</span>, <span class="number">3</span>&#125;;  <span class="comment">// 张量的形状是 2x3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建张量对象</span></span><br><span class="line">    Ort::Value tensor = Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(</span><br><span class="line">        allocator, </span><br><span class="line">        data.<span class="built_in">data</span>(),            <span class="comment">// 数据指针</span></span><br><span class="line">        data.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>),  <span class="comment">// 数据大小，单位是字节</span></span><br><span class="line">        shape.<span class="built_in">data</span>(),           <span class="comment">// 形状</span></span><br><span class="line">        shape.<span class="built_in">size</span>(),           <span class="comment">// 形状的长度</span></span><br><span class="line">        ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT  <span class="comment">// 数据类型</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取张量的类型和形状信息</span></span><br><span class="line">    Ort::TensorTypeAndShapeInfo tensor_info = tensor.<span class="built_in">GetTensorTypeAndShapeInfo</span>();</span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; tensor_shape = tensor_info.<span class="built_in">GetShape</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印张量形状</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Tensor Shape: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> dim : tensor_shape) &#123;</span><br><span class="line">        std::cout &lt;&lt; dim &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 访问张量数据</span></span><br><span class="line">    <span class="type">float</span>* tensor_data = tensor.<span class="built_in">GetTensorData</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Tensor Data: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; data.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; tensor_data[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="详细解释"><a href="#详细解释" class="headerlink" title="详细解释"></a><strong>详细解释</strong></h3><ol>
<li><p><strong>内存分配</strong>：</p>
<ul>
<li>使用 <code>Ort::AllocatorWithDefaultOptions</code> 创建一个分配器对象，它将在创建张量时分配内存。</li>
</ul>
</li>
<li><p><strong>数据和形状</strong>：</p>
<ul>
<li>数据是一个 <code>std::vector&lt;float&gt;</code>，代表了一个 2x3 张量的数据。</li>
<li>形状 <code>shape</code> 是一个 <code>std::array&lt;int64_t, 2&gt;</code>，表示张量是 2 行 3 列的。</li>
</ul>
</li>
<li><p><strong>创建张量</strong>：</p>
<ul>
<li><code>Ort::Value::CreateTensor&lt;float&gt;</code> 方法用来创建一个 <code>float</code> 类型的张量。我们传入数据指针、数据大小、形状以及数据类型。</li>
<li>该方法将返回一个 <code>Ort::Value</code> 对象，封装了张量的所有信息。</li>
</ul>
</li>
<li><p><strong>获取张量信息</strong>：</p>
<ul>
<li>使用 <code>tensor.GetTensorTypeAndShapeInfo()</code> 获取张量的类型和形状信息。</li>
<li>使用 <code>tensor.GetTensorData&lt;float&gt;()</code> 获取存储在张量中的数据指针。</li>
</ul>
</li>
<li><p><strong>打印结果</strong>：</p>
<ul>
<li>打印张量的形状和数据内容。</li>
</ul>
</li>
</ol>
<h3 id="注意事项-5"><a href="#注意事项-5" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ul>
<li><strong>内存管理</strong>：<code>Ort::Value</code> 对象会自动管理张量的内存。因此，用户不需要手动释放张量的内存，只需保证它的生命周期在需要时有效。</li>
<li><strong>数据类型</strong>：确保传入的 <code>data</code> 的类型与指定的 <code>ONNXTensorElementDataType</code> 类型匹配。例如，如果选择 <code>ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT</code>，则 <code>data</code> 必须是 <code>float</code> 类型的数据。</li>
</ul>
<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><code>Ort::Value::CreateTensor</code> 是一个用于创建张量的静态方法，可以将数据、形状和数据类型封装成 <code>Ort::Value</code> 对象。</li>
<li>它为 ONNX Runtime 模型推理提供了输入和输出的数据结构。</li>
<li>使用时需要提供内存分配器、数据、数据大小、形状以及数据类型等信息。</li>
</ul>
<h2 id="Session-Run-函数-详解"><a href="#Session-Run-函数-详解" class="headerlink" title="Session::Run() 函数 详解"></a>Session::Run() 函数 详解</h2><p><code>Session::Run()</code> 是 ONNX Runtime 中的一个关键函数，负责执行模型推理（即前向传播）。它的主要作用是根据输入数据进行推理，并返回推理结果。你通过它将输入数据传递给 ONNX 模型，并得到相应的输出。</p>
<h3 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a><strong>函数定义</strong></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Run</span><span class="params">(<span class="type">const</span> Session::RunOptions&amp; run_options,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;std::string&gt;&amp; input_names,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;<span class="type">const</span> Ort::Value*&gt;&amp; input_values,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> std::vector&lt;std::string&gt;&amp; output_names,</span></span></span><br><span class="line"><span class="params"><span class="function">           std::vector&lt;Ort::Value&gt;&amp; output_values)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a><strong>参数说明</strong></h3><ol>
<li><p>**<code>run_options</code> (Session::RunOptions)**：</p>
<ul>
<li><strong>类型</strong>：<code>const Session::RunOptions&amp;</code></li>
<li><strong>描述</strong>：设置推理过程中的一些选项，比如是否使用优化等。可以为空，表示使用默认选项。</li>
</ul>
</li>
<li><p>**<code>input_names</code> (std::vector<a href="std::string">std::string</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;std::string&gt;&amp;</code></li>
<li><strong>描述</strong>：输入张量的名称列表。对于每个输入张量，你需要提供对应的名称，这些名称必须与模型中定义的输入名称一致。</li>
</ul>
</li>
<li><p>**<code>input_values</code> (std::vector&lt;const Ort::Value*&gt;)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;const Ort::Value*&gt;&amp;</code></li>
<li><strong>描述</strong>：输入数据的值列表。每个输入对应一个 <code>Ort::Value</code> 对象，这些对象包含了输入数据。数据的类型和形状应与模型要求的输入一致。</li>
</ul>
</li>
<li><p>**<code>output_names</code> (std::vector<a href="std::string">std::string</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>const std::vector&lt;std::string&gt;&amp;</code></li>
<li><strong>描述</strong>：输出张量的名称列表。指定你希望从模型中获取的输出名称。如果模型有多个输出，你需要提供所有输出的名称。</li>
</ul>
</li>
<li><p>**<code>output_values</code> (std::vector<a href="Ort::Value">Ort::Value</a>)**：</p>
<ul>
<li><strong>类型</strong>：<code>std::vector&lt;Ort::Value&gt;&amp;</code></li>
<li><strong>描述</strong>：模型推理的输出结果。执行完推理后，结果会被填充到这个 <code>Ort::Value</code> 的 vector 中。</li>
</ul>
</li>
</ol>
<h3 id="返回值-3"><a href="#返回值-3" class="headerlink" title="返回值"></a><strong>返回值</strong></h3><ul>
<li><strong>类型</strong>：<code>Status</code></li>
<li><strong>描述</strong>：函数返回一个 <code>Status</code> 对象，表示推理过程的状态。通常在成功时返回 <code>Status::OK()</code>，如果失败，则返回相应的错误信息。</li>
</ul>
<h3 id="函数工作流程"><a href="#函数工作流程" class="headerlink" title="函数工作流程"></a><strong>函数工作流程</strong></h3><p><code>Session::Run()</code> 函数在 ONNX Runtime 中负责以下几个步骤：</p>
<ol>
<li><p><strong>输入检查</strong>：</p>
<ul>
<li>检查提供的输入数据是否与模型的输入要求（如名称、形状、数据类型）一致。</li>
</ul>
</li>
<li><p><strong>推理执行</strong>：</p>
<ul>
<li>使用提供的输入数据执行推理。ONNX Runtime 会根据模型的计算图进行推理操作。</li>
</ul>
</li>
<li><p><strong>输出生成</strong>：</p>
<ul>
<li>根据指定的输出名称，返回计算结果。输出的数据会被填充到 <code>output_values</code> 中，通常这些数据是 <code>Ort::Value</code> 对象，包含了推理结果（如分类的概率、图像的预测框等）。</li>
</ul>
</li>
</ol>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a><strong>示例代码</strong></h3><p>以下是一个简单的使用 <code>Session::Run()</code> 函数的示例，它展示了如何加载一个 ONNX 模型并运行推理。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/cpu/cpu_provider_factory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/shared_library/provider_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/tensor/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/utils.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/common.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;onnxruntime/core/providers/session.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 ONNX Runtime 环境</span></span><br><span class="line">    <span class="function">Ort::Env <span class="title">env</span><span class="params">(ORT_LOGGING_LEVEL_WARNING, <span class="string">&quot;ONNXModel&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载模型</span></span><br><span class="line">    <span class="type">const</span> std::string model_path = <span class="string">&quot;path_to_your_model.onnx&quot;</span>;</span><br><span class="line">    Ort::SessionOptions session_options;</span><br><span class="line">    <span class="function">Ort::Session <span class="title">onnx_session</span><span class="params">(env, model_path.c_str(), session_options)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">const</span> <span class="type">char</span>*&gt; input_names = &#123;<span class="string">&quot;input_tensor_name&quot;</span>&#125;;  <span class="comment">// 假设模型有一个输入张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; input_values;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建输入 tensor（假设模型要求输入大小为 1x3x224x224）</span></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">input_data</span><span class="params">(<span class="number">1</span> * <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">1.0f</span>)</span></span>;  <span class="comment">// 示例输入数据</span></span><br><span class="line">    std::vector&lt;<span class="type">int64_t</span>&gt; input_shape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;  <span class="comment">// 输入张量的形状</span></span><br><span class="line">    input_values.<span class="built_in">push_back</span>(Ort::Value::<span class="built_in">CreateTensor</span>&lt;<span class="type">float</span>&gt;(input_data.<span class="built_in">data</span>(), input_shape));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置输出数据</span></span><br><span class="line">    std::vector&lt;<span class="type">const</span> <span class="type">char</span>*&gt; output_names = &#123;<span class="string">&quot;output_tensor_name&quot;</span>&#125;;  <span class="comment">// 假设模型有一个输出张量</span></span><br><span class="line">    std::vector&lt;Ort::Value&gt; output_values;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行推理</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        onnx_session.<span class="built_in">Run</span>(Ort::<span class="built_in">RunOptions</span>(), input_names, input_values.<span class="built_in">data</span>(), input_names.<span class="built_in">size</span>(), output_names, output_values);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理推理结果</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; output : output_values) &#123;</span><br><span class="line">            <span class="comment">// 处理每个输出的结果</span></span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Output tensor: &quot;</span> &lt;&lt; output.<span class="built_in">GetTensorTypeAndShapeInfo</span>().<span class="built_in">GetShape</span>()[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception&amp; e) &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Error during inference: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a><strong>解释</strong></h3><ol>
<li><strong>初始化环境</strong>：首先创建一个 <code>Ort::Env</code> 对象来初始化 ONNX Runtime 环境。</li>
<li><strong>加载模型</strong>：通过 <code>Ort::Session</code> 加载一个 ONNX 模型，并使用 <code>SessionOptions</code> 进行配置。</li>
<li><strong>准备输入数据</strong>：创建输入数据（如一张图像），并使用 <code>Ort::Value::CreateTensor</code> 方法将其转换为 <code>Ort::Value</code> 类型。</li>
<li><strong>执行推理</strong>：使用 <code>Run</code> 方法执行推理，传入输入数据和请求的输出数据名称。</li>
<li><strong>输出结果</strong>：推理结果被存储在 <code>output_values</code> 中，之后可以进一步处理或打印输出。</li>
</ol>
<h3 id="注意事项-6"><a href="#注意事项-6" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><ol>
<li><p><strong>输入和输出数据的类型一致性</strong>：</p>
<ul>
<li>需要确保输入数据的类型（如 <code>float</code>）与模型要求的数据类型一致。</li>
<li>输入数据的形状应与模型的输入形状匹配，否则推理会失败。</li>
</ul>
</li>
<li><p><strong>异常处理</strong>：</p>
<ul>
<li><code>Session::Run()</code> 可能会抛出异常（如数据类型不匹配、形状不匹配等），因此通常需要在调用时使用 <code>try-catch</code> 来捕获并处理异常。</li>
</ul>
</li>
<li><p><strong>性能优化</strong>：</p>
<ul>
<li><code>Session::Run()</code> 是一个同步操作，因此如果需要优化推理性能，可以考虑异步执行（如通过 <code>RunOptions</code> 或使用多线程）或使用硬件加速（如使用 GPU）。</li>
</ul>
</li>
</ol>
<h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>Session::Run()</code> 是 ONNX Runtime 推理流程中的核心函数，它负责将输入数据传递给模型并返回结果。理解和正确使用该函数对于模型推理至关重要。通过合适的参数配置（如输入输出的名称和数据类型），可以实现高效、正确的模型推理。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/34/">34</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhang junyi</p>
  <div class="site-description" itemprop="description">工作学习笔记</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">672</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">98</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/junyiha" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;junyiha" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/zhangjunyiha" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;zhangjunyiha" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhang junyi</span>
</div>

<!--
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>
-->

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
